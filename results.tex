\chapter{Evaluation}
\label{cha:evaluation}

In this chapter we discuss the results of our information retrieval model. For our evaluation, we generated a dataset with $821$ scientific articles. Each document in the dataset was stored with its logical structure (title, headings, chapters, sections, subsections, subsubsections). Additionally, sections contain information about their IMRaD-Types. Usually a section has only one IMRaD-Type, but some of them have more (e.g., Results and Discussion are assigned the IMRaD-Types Result and Discussion). Furthermore, the articles are linked according their citations. For more information about the creation of our dataset see ~\Cref{sec:dataset}.

In our experiments we compare $5$ common ranking algorithms:
\begin{enumerate}
  \item First, \textit{Term Frequency} is the simplest approach to rank generated lists. We used \textit{Raw Frequency} of the term frequency variants. There the occurrences of each query term are counted within the document. To read more about different variants of term frequency and inverse document frequency see \Cref{sec:tfidf}.
  \item Second, \textit{TF-IDF} to take inverse document frequency into account. The inverse document frequency represents the importance of a term with respect to the entire document collection. Therefore, \textit{Inverse Frequency} is applied in combination with \textit{Raw Frequency}.
  \item Third, \textit{Okapi BM$25$} is a baseline ranking algorithm. We use the params as suggested by Robertson et. al ~\cite{RobertsonWJHG94}. Therefore, we set $b=0.75$ and $K_1 = 1$ (cf. \Cref{sec:okapi_bm25}).
  \item Fourth, \textit{Divergence from Randomness} has $2$ assumptions that are formalized with equations. In our experiments we used \Cref{dvr_part1_1} and \Cref{dvr_part2_1} for our practical implementation of the assumptions. Furthermore, \Cref{dfv_avg_doclen_1} was applied for document length normalization (cf. \Cref{sec:divergence_from_randomness}). 
  \item Fifth, \textit{Ranked Boolean Retrieval} is a ranking method that combines zone scores with boolean expressions. The score is applied to the ranking if terms occur in the zone. The configuration of the algorithm depends on the experiment (cf. \Cref{sec:ranking_strategies_in_xml_retrieval}).
\end{enumerate}

\section{Evaluation of Ranking Algorithms}
\label{sec:evaluation_of_ranking_algorithms}

The effectivity of an information retrieval system (IRS) is lean on the underlying ranking function. Therefore, it is important to measure the performance of these ranking functions to make them comparable.

\myfig{precision_recall}
      {width=0.50\textwidth}
      {\textbf{Document collection split according their relevance.} During the evaluation of a ranking algorithm the document collection is split into $4$ subsets. First, the \textit{true positive} documents retrieved as relevant for the user, which are actually relevant. Second, the \textit{false positive} documents retrieved as relevant, but are not relevant for the user. Third, the \textit{true negatives} documents were correctly received as not relevant. Forth, the \textit{false negatives} documents were incorrectly received as not relevant. The \textit{true positive} set, and \textit{true negative} set should be as large as possible as it is directly related to the effectivity of the ranking algorithm.}
      {Document collection split according their relevance.}
      {fig:precision_recall}

Evaluation of a IRS is based on the set of relevant documents provided by the system. To evaluate a generated set without any ranking (unordered) $2$ information retrieval basic measures known as \textit{Precision} and \textit{Recall} are used. \textit{Precision} is a measurement of retrieved documents (see \Cref{fig:precision_recall} for sets in the document collection split according their relevance) that are relevant for the user:
\begin{align}
  \label{precision}
  \text{Precision} = & \frac{\text{\# relevant items retrieved}}{\text{\# retrieved items}} \nonumber \\
    = & P(\text{relevant} | \text{retrieved}) \nonumber \\
    = & \frac{\text{true positives}}{\text{true positives} + \text{false positives}}.
\end{align}
\textit{Recall} represents the fraction of relevant documents that are received:
\begin{align}
  \text{Recall} = & \frac{\text{\# relevant items retrieved}}{\text{\# relevant items}} \nonumber \\
    = & P(\text{retrieved} | \text{relevant}) \nonumber \\
    = & \frac{\text{true positives}}{\text{true positives} + \text{false negatives}}.
\end{align}
There is a trade-off between the two measures. Therefore, when having a \textit{Recall} of $1$ it is possible to have a low \textit{Precision}. This happens as \textit{Recall} always increasing until all relevant documents are retrieved, but new received documents can be \textit{false positives}. Hence, the \textit{Precision} decreases, and \textit{Recall} stays the same.

There exist many different metrics to evaluate generated sets with ranking (ordered). Mean Average Precision is one of the most commonly used evaluation techniques. It is defined as the average of all precision values after a new relevant document is observed:
\begin{align}
  \label{map_of_a_single_query}
  \mathit{MAP}_i = \frac{1}{|R_i|}\sum_{k = 1}^{|R_i|} P(R_i[k]),
\end{align}
where $R_i$ is the set of relevant documents with respect to query $q_i$. $R_i[k]$ represents the reference of the $k$th document in $R_i$, and $P(R_i[k])$ is the \textit{Precision} of the document (see \Cref{precision}). If $R_i[k]$ belongs to a \textit{false positive} document $P(R_i[k]) = 0$. Furthermore, the Mean Average Precision over a set of queries is defined as:
\begin{align}
  \mathit{MAP} = \frac{1}{N_q}\sum_{i = 1}^{|N_q|} \mathit{MAP}_i,
\end{align}
where $N_q$ is the total number of queries.

\myfig{map}
      {width=1.00\textwidth}
      {\textbf{Example for Mean Average Precision of a single query.} The Precision values are calculated according the retrieved set of relevant documents. Documents with a green hook are \textit{true positives} (TP), and documents with a red cross are \textit{false positives} (FP). The precision values of FP documents are ignored for these documents, as they are not counted. In the example $R_1 =$ \{TP, FP, TP, TP, FP, TP\}, and therefore $|R_1| = 6$. Inserting these values into the Mean Average Precision formula of a single query (see \Cref{map_of_a_single_query}) results in $\mathit{MAP}_1 = 1/6 \times (1 + (2/3) + (3/4) + (4/6)) = 0.513\overline{8}$.}
      {Example for Mean Average Precision of a single query}
      {fig:map}

 Mean Average Precision is widely used as it is simple, easy to implement, versatile, and stable. Therefore, we apply it to compare the generated ranked lists of our proposed ranking algorithms (for an example see \Cref{fig:map}).

\section{Leveraging IMRaD Structure Features}

In our first experiments we compare unstructured text retrieval with structured text retrieval. For structured text retrieval, we focus on the underlying IMRaD structure. Therefore, each document of the dataset contains a set of all index terms. In addition, they contain $5$ sets for each IMRaD-type. The union of the $5$ sets differs from the set with all index terms as there exists areas that cannot be assigned to an IMRaD-type (e.g., Abstract). 

First, we evaluate explicit search where query terms have to be formulated explicitly. For example, given a query $q_1=$"\textit{network} IN \texttt{Methods}" the system leverages different sets to search for the term. For structured text retrieval the Methods-set is used as it was specified in the query. In our system IN-statements of queries are ignored for unstructured text retrieval as all terms are searched in the set with all index terms.

Second, we discuss implicit search using entire scientific articles to search for other articles. For structured text retrieval we split the input paper according to its IMRaD-structure. We generate a query that is similar to the query of the explicit variant with the extracted terms. For unstructured text retrieval the same process is used to create the query, but IN-statements are ignored here as well.

\subsection{Explicit Search using N-Grams}

To search with information provided by the user is called explicit search. For our information retrieval model this is done by formalizing user queries. These queries can be seen as a sequence of keywords.

Our generated dataset consists of scientific articles, and links between them. We extracted citations in order to get queries that are used for testing. Our assumption was that citations describe the content of referenced articles. Therefore, a referenced article gets represented by the citation.

We store each citation as a set of terms. An important addition was that the order of the terms within the set was not lost. Additionally, we assume that a typical user searches by stringing together keywords. Therefore, we removed stopwords from the set, but terms was not stemmed. For each citation set we stored information about the refereed article, and the IMRaD-Types of the section. For example, "\textit{The authors of [$1$] present a comprehensive system for the structure extraction of PDF books, which is used within a commercial e-book software.}" is in the \textit{Introduction} of scientific article $A$. Furthermore, "\textit{[$1$]}" is the reference to the refereed scientific article $B$. The resulting citation set looks as follows: $cs_{A1} =$ \{"\textit{authors}", "\textit{present}", "\textit{comprehensive}", "\textit{system}", "\textit{structure}", "\textit{extraction}", "\textit{PDF}", "\textit{books}", "\textit{commercial}", "\textit{e-book}", "\textit{software}"\}, and additionally the reference to article $B$, and the IMRaD-Type \textit{Introduction} is stored for $cs_{A1}$.

Queries are produced as $N$-Grams with the citation sets. This means the citation sets are split into subsets of size $N$. Furthermore, the order of the terms is also important for these subsets. For example, $cs_{A1}$ is split into $N$-Grams and $N = 5$. The $7$ resulting subsets are:
\begin{align*}
  NG_{cs_{A1}, 1} &= \{"\textit{authors}", "\textit{present}", "\textit{comprehensive}", "\textit{system}", "\textit{structure}"\} \nonumber \\
  NG_{cs_{A1}, 2} &= \{"\textit{present}", "\textit{comprehensive}", "\textit{system}", "\textit{structure}", "\textit{extraction}"\} \nonumber \\
  NG_{cs_{A1}, 3} &= \{"\textit{comprehensive}", "\textit{system}", "\textit{structure}", "\textit{extraction}", "\textit{PDF}"\} \nonumber \\
  NG_{cs_{A1}, 4} &= \{"\textit{system}", "\textit{structure}", "\textit{extraction}", "\textit{PDF}", "\textit{books}"\} \nonumber \\
  NG_{cs_{A1}, 5} &= \{"\textit{structure}", "\textit{extraction}", "\textit{PDF}", "\textit{books}", "\textit{commercial}",\} \nonumber \\
  NG_{cs_{A1}, 6} &= \{"\textit{extraction}", "\textit{PDF}", "\textit{books}", "\textit{commercial}", "\textit{e-book}"\} \nonumber \\
  NG_{cs_{A1}, 7} &= \{"\textit{PDF}", "\textit{books}", "\textit{commercial}", "\textit{e-book}", "\textit{software}"\} \nonumber \\
\end{align*}
In general, the number of subsets that can be generated from a citation set $cs$ is defined as:
\begin{align}
  l = len(cs) - N + 1,
\end{align}
where $len(cs)$ is the number of terms in $cs$. 

The last step is to bring the query set together with the IMRaD-type in our query structure. For example, the resulting query compiled by $NG_{cs_{A1}, 1}$ looks as follows: $q_1=$"\textit{authors}, \textit{present}, \textit{comprehensive}, \textit{system}, \textit{structure} IN \texttt{Introduction}". In our system IN-statements of queries are ignored for unstructured text retrieval.

Mean Average Precision (see \Cref{sec:evaluation_of_ranking_algorithms}) is used to evaluate our proposed ranking algorithms. Therefore, we passed the query into our information retrieval model. The Average Precision is determined with respect to the position of the refereed article in the generated ranked list.

In \Cref{tbl:ranking_result_explicit} the performance results of our ranking algorithms are proposed. We evaluated different query lengths, where the length is defined by the number of terms in the query. Our query length varying in the range between $2$ to $14$. Furthermore, we compared the algorithms with respect to the usage of IMRaD chapter features, where disabled features denote unstructured text retrieval and enabled features represent structured text retrieval. The best results with enabled features are highlighted in violet and with disabled features are highlighted in blue for every algorithm.

All $5$ algorithms obtain higher results without IMRaD chapter features. The best performing algorithm is \textit{TF-IDF}. It archives a MAP of $0.2199$ with disabled features, where the query consists of $11$ terms. In addition, the MAP is $0.1642$ with enabled features, and a query length of $12$. Therefore, the comparison of the highest accuracies is $5.57$ percent better for unstructured text retrieval. 

The second best algorithm is \textit{TF}. It has an accuracy of $0.1966$ with disabled features, and a query length of $11$. Furthermore the accuracy is $0.1293$ with enabled features, and a query length of $12$. In comparison with \textit{TF-IDF} the archived accuracy is $2.33$ percent worse for disabled features, and $3.49$ percent worse for enabled features. The best performing query lengths are the same as for \textit{TF-IDF}. The highest accuracy is $6.73$ better for unstructured text retrieval than for structured text retrieval.

The third best algorithm is \textit{Ranked Boolean Retrieval}. Zone scores $zs$ have to be defined to configure the algorithm (cf. \Cref{sec:ranking_strategies_in_xml_retrieval}). For the experiments only zones that contain text areas are set to a value greater zero. This was done as all citations was extracted from text areas. In our model these are text areas of sections, subsections, and subsubsections. Therefore, the constant zone scores are $zs_{\text{Section}} = 0.34$, $zs_{\text{Subsection}} = 0.33$, and $zs_{\text{Subsubsection}} = 0.33$.

\begin{table}[h]
  \begin{adjustwidth}{-2cm}{}
    \begin{tabular}{ C{1cm} C{1cm} C{2.1cm} C{2.1cm} C{2.1cm} C{2.1cm} C{2.1cm} C{2.1cm} }
      \toprule
      \textbf{\# terms in query} & \textbf{\# queries} & \textbf{Using IMRaD Chapter Features} & \textbf{Term Frequency} & \textbf{TF-IDF} & \textbf{Ranked Boolean Retrieval} & \textbf{Okapi BM$25$} & \textbf{Divergence from Randomness} \\ \midrule
      \multirow{2}{*}{$2$} & \multirow{2}{*}{$8770$} & No  & $0.0882$ & $0.1128$ & $0.1035$ & $0.0442$ & \color{blue}$\mathbf{0.0498}$  \\
                                                    && Yes & $0.0696$ & $0.0897$ & $0.0638$ & $0.045$  & \color{Plum}$\mathbf{0.0379}$  \\ \midrule
      \multirow{2}{*}{$3$} & \multirow{2}{*}{$7070$} & No  & $0.1038$ & $0.1382$ & $0.1198$ & $0.064$  & $0.046$   \\
                                                    && Yes & $0.0785$ & $0.1042$ & $0.0713$ & $0.0628$ & $0.0323$  \\ \midrule
      \multirow{2}{*}{$4$} & \multirow{2}{*}{$5589$} & No  & $0.1197$ & $0.1547$ & $0.1336$ & $0.0739$ & $0.0448$  \\
                                                    && Yes & $0.0894$ & $0.1167$ & $0.0787$ & $0.0734$ & $0.031$   \\ \midrule
      \multirow{2}{*}{$5$} & \multirow{2}{*}{$4347$} & No  & $0.1317$ & $0.1689$ & $0.1479$ & $0.0794$ & $0.0416$  \\
                                                    && Yes & $0.0993$ & $0.1262$ & $0.0844$ & $0.0791$ & $0.0317$  \\ \midrule
      \multirow{2}{*}{$6$} & \multirow{2}{*}{$3336$} & No  & $0.1469$ & $0.1766$ & $0.1603$ & $0.0804$ & $0.0396$  \\
                                                    && Yes & $0.1042$ & $0.1319$ & $0.0918$ & $0.0819$ & $0.0311$  \\ \midrule
      \multirow{2}{*}{$7$} & \multirow{2}{*}{$2550$} & No  & $0.1588$ & $0.1857$ & $0.167$  & $0.0817$ & $0.0404$  \\
                                                    && Yes & $0.1085$ & $0.1375$ & $0.0961$ & $0.0842$ & $0.0312$  \\ \midrule
      \multirow{2}{*}{$8$} & \multirow{2}{*}{$1911$} & No  & $0.1712$ & $0.1957$ & $0.1718$ & $0.0818$ & $0.0449$  \\
                                                    && Yes & $0.1158$ & $0.1441$ & $0.0988$ & $0.0916$ & $0.0303$  \\ \midrule
      \multirow{2}{*}{$9$} & \multirow{2}{*}{$1402$} & No  & $0.1804$ & $0.2074$ & $0.1757$ & $0.0879$ & $0.0456$  \\
                                                    && Yes & $0.1213$ & $0.1496$ & \color{Plum}$\mathbf{0.1015}$ & $0.0969$ & $0.0301$  \\ \midrule
      \multirow{2}{*}{$10$} & \multirow{2}{*}{$1051$} & No & $0.1847$ & $0.2153$ & $0.1851$ & $0.0952$ & $0.0466$  \\
                                                    && Yes & $0.1235$ & $0.1555$ & $0.1005$ & $0.099$  & $0.0304$  \\ \midrule
      \multirow{2}{*}{$11$} & \multirow{2}{*}{$787$} & No  & \color{blue}$\mathbf{0.1966}$ & \color{blue}$\mathbf{0.2199}$ & \color{blue}$\mathbf{0.1921}$ & $0.1075$ & $0.0439$  \\
                                                    && Yes & $0.1217$ & $0.1589$ & $0.0978$ & $0.1034$ & $0.0296$  \\ \midrule
      \multirow{2}{*}{$12$} & \multirow{2}{*}{$606$} & No  & $0.1923$ & $0.2159$ & $0.1851$ & $0.1102$ & $0.0397$  \\
                                                    && Yes & \color{Plum}$\mathbf{0.1293}$ & \color{Plum}$\mathbf{0.1642}$ & $0.0974$ & $0.1043$ & $0.0296$  \\ \midrule
      \multirow{2}{*}{$13$} & \multirow{2}{*}{$470$} & No  & $0.1846$ & $0.205$ & $0.1712$ & $0.1192$ & $0.0319$   \\
                                                    && Yes & $0.1247$ & $0.1637$ & $0.0958$ & \color{Plum}$\mathbf{0.1058}$ & $0.0295$  \\ \midrule
      \multirow{2}{*}{$14$} & \multirow{2}{*}{$362$} & No  & $0.1634$ & $0.1919$ & $0.1558$ & \color{blue}$\mathbf{0.1207}$ & $0.0221$  \\
                                                    && Yes & $0.1183$ & $0.1589$ & $0.0964$ & $0.1008$ & $0.0311$  \\
      \bottomrule
    \end{tabular}
  \caption[Ranking results with explicit search]{\textbf{Ranking results of our proposed ranking algorithms using explicit search.} We compared our proposed ranking algorithms with respect to the usage of IMRaD chapter features. Without IMRaD chapter features the entire document is used to search for query terms (unstructured). When IMRaD chapter features are enabled query terms are searched only specified sections.}
  \label{tbl:ranking_result_explicit}
  \end{adjustwidth}
\end{table}
\clearpage

\textit{Ranked Boolean Retrieval} archives an accuracy of $0.1921$ with disabled features, and a query length of $11$. Therefore, in the context of unstructured text retrieval the best performing query length is the same as for \textit{TF-IDF} and \textit{TF}. Additionally, the accuracy is $0.1015$ with enabled features, and a query length of $9$. In comparison with \textit{TF-IDF} the archived accuracy is $2.78$ percent worse for disabled features, and $6.27$ percent worse for enabled features. Furthermore, when comparing with \textit{TF} the MAP is $0.45$ percent worse for disabled features, and $2.78$ percent worse for enabled features. The highest accuracy is $9.06$ better for unstructured text retrieval than for structured text retrieval.

The forth best algorithm is \textit{Okapi BM$25$}.

\subsection{Implicit Search using Scientific Articles}

Number of scientific articles: $821$

... more like this. describe evaluation, ranked boolean setup

\begin{table}[b!]
    \centering
    \begin{tabular}{ C{2.1cm} C{2.1cm} C{2.1cm} C{2.1cm} C{2.1cm} C{2.1cm} }
      \toprule
      \textbf{Using IMRaD Chapter Features} & \textbf{Term Frequency} & \textbf{TF-IDF} & \textbf{Ranked Boolean Retrieval} & \textbf{Okapi BM25} & \textbf{Divergence from Randomness} \\ \midrule
      No  & $0.1186$ & $0.1163$ & $0.0466$ & $0.0554$ & $0.0137$ \\
      Yes & $0.1463$ & $0.1613$ & $0.0506$ & $0.0882$ & $0.0137$ \\
      \bottomrule
    \end{tabular}
  \caption[Ranking results using scientific articles]{Ranking results of the used weighting schemes using scientific articles}
  \label{tbl:ranking_result_full}
\end{table}


\section{Chapter Based Search}

Number of scientific articles: $821$

describe evaluation, ranked boolean setup

\begin{table}[b!]
\vrule\pgfplotstabletypeset[%
     color cells={min=0,max=0.15,textcolor=black},
     /pgfplots/colormap={blackwhite}{rgb255=(255,170,0) color=(white) rgb255=(255,170,0)},
    /pgf/number format/fixed,
    /pgf/number format/precision=4,
    col sep=comma,
    columns/Section/.style={reset styles,string type}%
]{
Section, Introduction, Background, Methods, Results, Discussion
Introduction, 0.1242, 0.1226, 0.1095, 0.1092, 0.1049
Background,   0.1454, 0.1249, 0.1331, 0.1255, 0.1106 
Methods,      0.0947, 0.0857, 0.1017, 0.0897, 0.0668
Results,      0.0877, 0.0783, 0.0815, 0.0783, 0.0631
Discussion,   0.1188, 0.1078, 0.0957, 0.0914, 0.084
}\vrule
  \caption[Chapter based Search using Tf-idf]{Chapter based Search using Tf-idf}
  \label{tbl:ranking_result_full}
\end{table}

\begin{table}[b!]
\vrule\pgfplotstabletypeset[%
     color cells={min=0,max=0.1,textcolor=black},
     /pgfplots/colormap={blackwhite}{rgb255=(255,170,0) color=(white) rgb255=(255,170,0)},
    /pgf/number format/fixed,
    /pgf/number format/precision=4,
    col sep=comma,
    columns/Section/.style={reset styles,string type}%
]{
Section, Introduction, Background, Methods, Results, Discussion
Introduction, 0.0884, 0.0686, 0.0631, 0.061,  0.0708
Background,   0.0909, 0.0715, 0.076,  0.0618, 0.0751
Methods,      0.0565, 0.0417, 0.0593, 0.0379, 0.0403
Results,      0.0438, 0.0426, 0.0433, 0.0461, 0.0443
Discussion,   0.0799, 0.0682, 0.0587, 0.0595, 0.0616
}\vrule
  \caption[Chapter based Search using Okapi BM$25$]{Chapter based Search using Okapi BM$25$}
  \label{tbl:ranking_result_full}
\end{table}
