\chapter{Results and Discussion}
\label{cha:results_discussion}

In this chapter we discuss the results of our information retrieval model. For our evaluation, we generated a dataset with $821$ scientific articles. Each document in the dataset was stored with its logical structure (title, headings, chapters, sections, subsections, subsubsections). Additionally, sections contain information about their IMRaD-Types. Usually a section has only one IMRaD-Type, but some of them have more (e.g., Results and Discussion are assigned the IMRaD-Types Result and Discussion). Furthermore, the articles are linked according their citations. For more information about the creation of our dataset see ~\Cref{sec:dataset}.

In our experiments we compare $5$ common ranking algorithms:
\begin{enumerate}
  \item First, \textit{Term Frequency} is the simplest approach to rank generated lists. We used \textit{Raw Frequency} of the term frequency variants. There the occurrences of each query term are counted within the document. To read more about different variants of term frequency and inverse document frequency see \Cref{sec:tfidf}.
  \item Second, \textit{TF-IDF} to take inverse document frequency into account. The inverse document frequency represents the importance of a term with respect to the entire document collection. Therefore, \textit{Inverse Frequency} is applied in combination with \textit{Raw Frequency}.
  \item Third, \textit{Okapi BM$25$} is a baseline ranking algorithm. We use the params as suggested by Robertson et. al ~\cite{RobertsonWJHG94}. Therefore, we set $b=0.75$ and $K_1 = 1$ (cf. \Cref{sec:okapi_bm25}).
  \item Fourth, \textit{Divergence from Randomness} has $2$ assumptions that are formalized with equations. In our experiments we used \Cref{dvr_part1_1} and \Cref{dvr_part2_1} for our practical implementation of the assumptions. Furthermore, \Cref{dfv_avg_doclen_1} was applied for document length normalization (cf. \Cref{sec:divergence_from_randomness}). 
  \item Fifth, \textit{Ranked Boolean Retrieval} is a ranking method that combines zone scores with boolean expressions. The score is applied to the ranking if terms occur in the zone. The configuration of the algorithm depends on the experiment (cf. \Cref{sec:ranking_strategies_in_xml_retrieval}).
\end{enumerate}


\section{Evaluation of Ranking Algorithms}
\label{sec:evaluation_of_ranking_algorithms}

The effectivity of an information retrieval system (IRS) is lean on the underlying ranking function. Therefore, it is important to measure the performance of these ranking functions to make them comparable.

\myfig{precision_recall}
      {width=0.50\textwidth}
      {\textbf{Document collection split according their relevance.} During the evaluation of a ranking algorithm the document collection is split into $4$ subsets. First, the \textit{true positive} documents retrieved as relevant for the user, which are actually relevant. Second, the \textit{false positive} documents retrieved as relevant, but are not relevant for the user. Third, the \textit{true negatives} documents were correctly received as not relevant. Forth, the \textit{false negatives} documents were incorrectly received as not relevant. The \textit{true positive} set, and \textit{true negative} set should be as large as possible as it is directly related to the effectivity of the ranking algorithm.}
      {Document collection split according their relevance.}
      {fig:precision_recall}

Evaluation of a IRS is based on the set of relevant documents provided by the system. To evaluate a generated set $2$ information retrieval basic measures known as \textit{Precision} and \textit{Recall} are used. Both measures do not take any ranking of the relevant documents into account. \textit{Precision} is a measurement of retrieved documents (see \Cref{fig:precision_recall} for sets in the document collection split according their relevance) that are relevant for the user:
\begin{align}
  \label{precision}
  \text{Precision} = & \frac{\text{\# relevant items retrieved}}{\text{\# retrieved items}} \nonumber \\
    = & P(\text{relevant} | \text{retrieved}) \nonumber \\
    = & \frac{\text{true positives}}{\text{true positives} + \text{false positives}}.
\end{align}
\textit{Recall} represents the fraction of relevant documents that are received:
\begin{align}
  \text{Recall} = & \frac{\text{\# relevant items retrieved}}{\text{\# relevant items}} \nonumber \\
    = & P(\text{retrieved} | \text{relevant}) \nonumber \\
    = & \frac{\text{true positives}}{\text{true positives} + \text{false negatives}}.
\end{align}
There is a trade-off between the two measures. Therefore, when having a \textit{Recall} of $1$ it is possible to have a low \textit{Precision}. This happens as \textit{Recall} always increasing until all relevant documents are retrieved, but new received documents can be \textit{false positives}. Hence, the \textit{Precision} decreases, and \textit{Recall} stays the same.

There exist many different metrics to evaluate generated sets with ranking (ordered). Average Precision is one of the most commonly used evaluation techniques. It is defined as the average of all precision values after a new relevant document is observed:
\begin{align}
  \label{map_of_a_single_query}
  \mathit{AP}_i = \frac{1}{|R_i|}\sum_{k = 1}^{|R_i|} P(R_i[k]),
\end{align}
where $R_i$ is the set of relevant documents with respect to query $q_i$. $R_i[k]$ represents the reference of the $k$th document in $R_i$, and $P(R_i[k])$ is the \textit{Precision} of the document (see \Cref{precision}). If $R_i[k]$ belongs to a \textit{false positive} document $P(R_i[k]) = 0$. Furthermore, the Mean Average Precision over a set of queries is defined as:
\begin{align}
  \mathit{MAP} = \frac{1}{N_q}\sum_{i = 1}^{|N_q|} \mathit{AP}_i,
\end{align}
where $N_q$ is the total number of queries.

\myfig{map}
      {width=1.00\textwidth}
      {\textbf{Example for Mean Average Precision of a single query.} The Precision values are calculated according the retrieved set of relevant documents. Documents with a green hook are \textit{true positives} (TP), and documents with a red cross are \textit{false positives} (FP). The precision values of FP documents are ignored for these documents, as they are not counted. In the example $R_1 =$ \{TP, FP, TP, TP, FP, TP\}, and therefore $|R_1| = 6$. Inserting these values into the Mean Average Precision formula of a single query (see \Cref{map_of_a_single_query}) results in $\mathit{MAP}_1 = 1/6 \times (1 + (2/3) + (3/4) + (4/6)) = 0.513\overline{8}$.}
      {Example for Mean Average Precision of a single query}
      {fig:map}

 Mean Average Precision is widely used as it is simple, easy to implement, versatile, and stable. Therefore, we apply it to compare the generated ranked lists of our proposed ranking algorithms (for an example see \Cref{fig:map}).


\section{Leveraging IMRaD Structure Features}
\label{sec:leveraging_imrad_structure_features}

In our first experiments we compare unstructured text retrieval with structured text retrieval. Therefore, each document in the document collection contains multiple bag of words. In a bag of words each index term is represented as a pair of term and term frequency (see \Cref{sec:model} for the definition of the document). For structured text retrieval, we focus on the underlying IMRaD structure.

First, we evaluate explicit search for which query terms have to be formulated explicitly. For example, given a query $q_1=$"\textit{network} IN \texttt{Methods}" the system leverages different bag of words to search for the term. For structured text retrieval the Methods-bag of words is used as it was specified in the query. In our system IN-statements of queries are ignored for unstructured text retrieval, as all terms are searched in the bag of words containing all index terms of a document.

Second, we discuss implicit search were we use entire scientific articles to search for other articles. For structured text retrieval, we split the input paper according to its IMRaD-structure. We generate a query that is similar to the query of the explicit variant with the extracted terms. For unstructured text retrieval the same process is used to create the query, however IN-statements are ignored here as well.


\subsection{Explicit Search using Word N-Grams}
\label{sec:explicit_search_ngrams}

Searching with information provided by the user is denoted as explicit search. For our information retrieval model this is done by formalizing user queries. These queries can be seen as a sequence of keywords.

Our generated dataset consists of scientific articles and links between them. We extracted citations in order to get queries that are used for testing. We base our model on the assumption that citations describe the content of referenced articles. Therefore, a referenced article is represented by the citation.

We store each citation as a set of terms. An important addition is that the order of terms within the set is not lost. Additionally, we assume that a typical user searches by combining keywords. Therefore, we removed stopwords from the set, but terms were not stemmed. For each citation set we store information about the cited article and the IMRaD-Types of the section. For example, "\textit{The authors of [$1$] present a comprehensive system for the structure extraction of PDF books, which is used within a commercial e-book software.}" is in the \textit{Introduction} of scientific article $A$. Furthermore, "\textit{[$1$]}" is the reference to the cited scientific article $B$. The resulting citation set looks as follows: $cs_{A1} =$ \{"\textit{authors}", "\textit{present}", "\textit{comprehensive}", "\textit{system}", "\textit{structure}", "\textit{extraction}", "\textit{PDF}", "\textit{books}", "\textit{commercial}", "\textit{e-book}", "\textit{software}"\}, and additionally the reference to article $B$, and the IMRaD-Type \textit{Introduction} is stored for $cs_{A1}$.

Queries are produced as $N$-Grams using the citation sets. This means the citation sets are split into subsets of size $N$. Furthermore, the order of the terms is also important for these subsets. For example, $cs_{A1}$ is split into $5$-Grams. The $7$ resulting subsets are:
\begin{align*}
  NG_{cs_{A1}, 1} &= \{"\textit{authors}", "\textit{present}", "\textit{comprehensive}", "\textit{system}", "\textit{structure}"\} \nonumber \\
  NG_{cs_{A1}, 2} &= \{"\textit{present}", "\textit{comprehensive}", "\textit{system}", "\textit{structure}", "\textit{extraction}"\} \nonumber \\
  NG_{cs_{A1}, 3} &= \{"\textit{comprehensive}", "\textit{system}", "\textit{structure}", "\textit{extraction}", "\textit{PDF}"\} \nonumber \\
  NG_{cs_{A1}, 4} &= \{"\textit{system}", "\textit{structure}", "\textit{extraction}", "\textit{PDF}", "\textit{books}"\} \nonumber \\
  NG_{cs_{A1}, 5} &= \{"\textit{structure}", "\textit{extraction}", "\textit{PDF}", "\textit{books}", "\textit{commercial}",\} \nonumber \\
  NG_{cs_{A1}, 6} &= \{"\textit{extraction}", "\textit{PDF}", "\textit{books}", "\textit{commercial}", "\textit{e-book}"\} \nonumber \\
  NG_{cs_{A1}, 7} &= \{"\textit{PDF}", "\textit{books}", "\textit{commercial}", "\textit{e-book}", "\textit{software}"\} \nonumber \\
\end{align*}
In general, the number of subsets that can be generated from a citation set $cs$ is defined as:
\begin{align}
  l = len(cs) - N + 1,
\end{align}
where $len(cs)$ is the number of terms in $cs$. 

The last step of our query generation process is to transform query sets and IMRaD-type into our used query structure. For example, the resulting query compiled by $NG_{cs_{A1}, 1}$ looks as follows: $q_1=$"\textit{authors}, \textit{present}, \textit{comprehensive}, \textit{system}, \textit{structure} IN \texttt{Introduction}". In our system IN-statements of queries are ignored if IMRaD chapter features are disabled.

Mean Average Precision (see \Cref{sec:evaluation_of_ranking_algorithms}) is used to evaluate our proposed ranking algorithms. Therefore, we pass the query into our information retrieval system. The Average Precision depends on the position of the cited article in the generated ranked list.

\Cref{tbl:ranking_result_explicit} highlights the performance of our proposed ranking algorithms. We evaluate different query lengths (the number of terms in the query), where query lengths vary in the range between $2$ to $14$. Furthermore, we compare the algorithms with respect to the usage of IMRaD chapter features, where disabled features denote unstructured text retrieval and enabled features represent structured text retrieval. We highlight the best results with enabled features in violet and with disabled features in blue for every algorithm.

All $5$ algorithms achieve better performance results without IMRaD chapter features. Therefore, the ranking is generated with respect to their achieved accuracy when IMRaD chapter features are disabled. The best performing algorithm is \textit{TF-IDF}. It achieves a MAP of $0.2199$ with disabled features, where the query consists of $11$ terms. In contrast, the same algorithm achieves a MAP of $0.1642$ with enabled features and a query length of $12$. Therefore, unstructured text retrieval is $5.57 \%$ better than structured text retrieval for this configuration.

The second best algorithm is \textit{TF}. It achieves an accuracy of $0.1966$ with disabled features and a query length of $11$. In contrast, the same algorithm achieves a MAP of $0.1293$ with enabled features and a query length of $12$. Therefore, unstructured text retrieval is $6.73 \%$ better than structured text retrieval for this configuration. In comparison to \textit{TF-IDF} the achieved accuracy is $2.33 \%$ lower with disabled features, and $3.49 \%$ lower with enabled features. The best performing query lengths are the same as for \textit{TF-IDF}.

The third best algorithm is \textit{Ranked Boolean Retrieval}. Zone scores $zs$ have to be defined to configure the algorithm (cf. \Cref{sec:ranking_strategies_in_xml_retrieval}). For the experiments only zones that contain text areas are set to a value greater zero. This is done as all citations were extracted from text areas. 
% split paragraph
\begin{table}[h]
  \begin{adjustwidth}{-2cm}{}
    \begin{tabular}{ C{1cm} C{1cm} C{2.1cm}|C{2.1cm} C{2.1cm} C{2.1cm} C{2.1cm} C{2.1cm} }
      \toprule
      \textbf{\# Terms in Query} & \textbf{\# Queries} & \textbf{Using IMRaD Chapter Features} & \textbf{Term Frequency} & \textbf{TF-IDF} & \textbf{Ranked Boolean Retrieval} & \textbf{Okapi BM$25$} & \textbf{Divergence from Randomness} \\ \midrule
      \multirow{2}{*}{$2$} & \multirow{2}{*}{$8770$} & No  & $0.0882$ & $0.1128$ & $0.1035$ & $0.0442$ & \color{blue}$\mathbf{0.0498}$  \\
                                                    && Yes & $0.0696$ & $0.0897$ & $0.0638$ & $0.045$  & \color{Plum}$\mathbf{0.0379}$  \\ \midrule
      \multirow{2}{*}{$3$} & \multirow{2}{*}{$7070$} & No  & $0.1038$ & $0.1382$ & $0.1198$ & $0.064$  & $0.046$   \\
                                                    && Yes & $0.0785$ & $0.1042$ & $0.0713$ & $0.0628$ & $0.0323$  \\ \midrule
      \multirow{2}{*}{$4$} & \multirow{2}{*}{$5589$} & No  & $0.1197$ & $0.1547$ & $0.1336$ & $0.0739$ & $0.0448$  \\
                                                    && Yes & $0.0894$ & $0.1167$ & $0.0787$ & $0.0734$ & $0.031$   \\ \midrule
      \multirow{2}{*}{$5$} & \multirow{2}{*}{$4347$} & No  & $0.1317$ & $0.1689$ & $0.1479$ & $0.0794$ & $0.0416$  \\
                                                    && Yes & $0.0993$ & $0.1262$ & $0.0844$ & $0.0791$ & $0.0317$  \\ \midrule
      \multirow{2}{*}{$6$} & \multirow{2}{*}{$3336$} & No  & $0.1469$ & $0.1766$ & $0.1603$ & $0.0804$ & $0.0396$  \\
                                                    && Yes & $0.1042$ & $0.1319$ & $0.0918$ & $0.0819$ & $0.0311$  \\ \midrule
      \multirow{2}{*}{$7$} & \multirow{2}{*}{$2550$} & No  & $0.1588$ & $0.1857$ & $0.167$  & $0.0817$ & $0.0404$  \\
                                                    && Yes & $0.1085$ & $0.1375$ & $0.0961$ & $0.0842$ & $0.0312$  \\ \midrule
      \multirow{2}{*}{$8$} & \multirow{2}{*}{$1911$} & No  & $0.1712$ & $0.1957$ & $0.1718$ & $0.0818$ & $0.0449$  \\
                                                    && Yes & $0.1158$ & $0.1441$ & $0.0988$ & $0.0916$ & $0.0303$  \\ \midrule
      \multirow{2}{*}{$9$} & \multirow{2}{*}{$1402$} & No  & $0.1804$ & $0.2074$ & $0.1757$ & $0.0879$ & $0.0456$  \\
                                                    && Yes & $0.1213$ & $0.1496$ & \color{Plum}$\mathbf{0.1015}$ & $0.0969$ & $0.0301$  \\ \midrule
      \multirow{2}{*}{$10$} & \multirow{2}{*}{$1051$} & No & $0.1847$ & $0.2153$ & $0.1851$ & $0.0952$ & $0.0466$  \\
                                                    && Yes & $0.1235$ & $0.1555$ & $0.1005$ & $0.099$  & $0.0304$  \\ \midrule
      \multirow{2}{*}{$11$} & \multirow{2}{*}{$787$} & No  & \color{blue}$\mathbf{0.1966}$ & \color{blue}$\mathbf{0.2199}$ & \color{blue}$\mathbf{0.1921}$ & $0.1075$ & $0.0439$  \\
                                                    && Yes & $0.1217$ & $0.1589$ & $0.0978$ & $0.1034$ & $0.0296$  \\ \midrule
      \multirow{2}{*}{$12$} & \multirow{2}{*}{$606$} & No  & $0.1923$ & $0.2159$ & $0.1851$ & $0.1102$ & $0.0397$  \\
                                                    && Yes & \color{Plum}$\mathbf{0.1293}$ & \color{Plum}$\mathbf{0.1642}$ & $0.0974$ & $0.1043$ & $0.0296$  \\ \midrule
      \multirow{2}{*}{$13$} & \multirow{2}{*}{$470$} & No  & $0.1846$ & $0.205$ & $0.1712$ & $0.1192$ & $0.0319$   \\
                                                    && Yes & $0.1247$ & $0.1637$ & $0.0958$ & \color{Plum}$\mathbf{0.1058}$ & $0.0295$  \\ \midrule
      \multirow{2}{*}{$14$} & \multirow{2}{*}{$362$} & No  & $0.1634$ & $0.1919$ & $0.1558$ & \color{blue}$\mathbf{0.1207}$ & $0.0221$  \\
                                                    && Yes & $0.1183$ & $0.1589$ & $0.0964$ & $0.1008$ & $0.0311$  \\
      \bottomrule
    \end{tabular}
  \caption[Ranking results with explicit search]{\textbf{Ranking results of our proposed ranking algorithms using explicit search.} Mean Average Precision is used to evaluate our proposed ranking algorithms. Furthermore, we compare them with and without IMRaD chapter features. Without IMRaD chapter features the entire document is used to search for query terms (unstructured). When IMRaD chapter features are enabled query terms are searched only in specified sections. We evaluate query lengths (the number of terms in the query) in the range between $2$ to $14$.}
  \label{tbl:ranking_result_explicit}
  \end{adjustwidth}
  \thisfloatpagestyle{empty}
\end{table}
\clearpage

In our model, these are text areas of sections, subsections, and subsubsections. Therefore, the constant zone scores are $zs_{\text{Section}} = 0.34$, $zs_{\text{Subsection}} = 0.33$, and $zs_{\text{Subsubsection}} = 0.33$.

\textit{Ranked Boolean Retrieval} achieves an accuracy of $0.1921$ with disabled features and a query length of $11$. Hence, in the context of unstructured text retrieval the best performing query length is the same as for \textit{TF-IDF} and \textit{TF}. In contrast, the same algorithm achieves a MAP of $0.1015$ with enabled features and a query length of $9$. Therefore, unstructured text retrieval is $9.06 \%$ better than structured text retrieval for this configuration. In comparison to \textit{TF-IDF} the achieved accuracy is $2.78 \%$ lower with disabled features and $6.27 \%$ lower with enabled features. Furthermore, in comparison to \textit{TF} the accuracy is $0.45 \%$ lower with disabled features, and $2.78 \%$ lower with enabled features.

The forth best algorithm is \textit{Okapi BM$25$}. It achieves an accuracy of $0.1207$ with disabled features. The associated best performing query length is $14$, which was the upper bound of the query lengths for our experiments. The upper bound comes from the number of queries that can be generated from our dataset. In contrast, the same algorithm achieves a MAP of $0.1058$ with enabled features and a query length of $13$. Therefore, unstructured text retrieval is $1.49 \%$ better than structured text retrieval for this configuration. In comparison to \textit{TF-IDF} the achieved accuracy is $9.92 \%$ lower with disabled features and $5.84 \%$ lower with enabled features. Furthermore, in comparison to \textit{Ranked Boolean Retrieval} the accuracy is $7.14 \%$ lower with disabled features, and $0.43 \%$ better with enabled features.

The fifth best algorithm is \textit{Divergence from Randomness}. It achieves an accuracy of $0.0498$ with disabled features, and an accuracy of $0.0379$ with enabled features. Therefore, unstructured text retrieval is $1.19 \%$ better than structured text retrieval for this configuration. The best performing query length is $2$ with disabled and enabled features. In comparison to \textit{TF-IDF} the achieved accuracy is $17.01 \%$ lower with disabled features, and $12.63 \%$ lower with enabled features. Furthermore, in comparison to \textit{Okapi BM$25$} the accuracy is $7.09 \%$ lower with disabled features, and $6.79 \%$ lower with enabled features.

An interpretation of the results captures that it is not necessary to have the overhead of IMRaD chapter features when only a few keywords are used to search for scientific articles. This happens as the keywords define content that should occur anywhere in the articles. Additional constraints that restrict where terms can appear are rather obstructive as they tend to prevent the retrieval of relevant articles.

Furthermore, the results of each ranking algorithm were interpreted. \textit{TF-IDF} outperforms the other algorithms, which was unsurprisingly as good results are often observed for this algorithm. During our research we found \textit{Okapi BM$25$} with similar good results, but only when an exhaustive parameter search was done. We used \textit{Okapi BM$25$} with recommended parameters, which resulted with lower results. In comparison to the other ranking algorithms \textit{Divergence from Randomness} was always outperformed by all other algorithms. This bad performance is probably related to our dataset. The good performance of \textit{Ranked Boolean Retrieval} was surprising. We can probably attribute this to the zone scores, as they are hiding irrelevant areas, and mark important article areas.


\subsection{Implicit Search using Scientific Articles}
\label{sec:implicit_search_results}

In traditional information retrieval models information is available that was not explicitly provided by the user. Leveraging this type of information in addition is denoted implicit search. In our model, we use explicit and implicit information of scientific articles to search for other articles. This approach is referred to as \textit{more like this}.

For our experiment we transformed scientific articles into user queries. Therefore, queries contain information about index terms and the IMRaD sections they occur. For example, $q_1=$"\textit{structure}, \textit{present} IN \texttt{Introduction} AND \textit{system}, \textit{structure} IN \texttt{Methods}" is a query that can be executed by our system. IN-statements are used to define where the terms occur, and AND-statements are used to combine subqueries. In our system IN-statements of queries are ignored if IMRaD chapter features are disabled.

We use a generated dataset with $821$ scientific articles to evaluate our proposed ranking algorithms. Therefore, we assume that the content of a scientific article has to be similar to its cited articles. As a result, the Mean Average Precision (see \Cref{sec:evaluation_of_ranking_algorithms}) is calculated with respect to the position of the cited articles in the generated ranked list.

\begin{table}[b!]
    \centering
    \begin{tabular}{ C{2.1cm} C{2.1cm} C{2.1cm} C{2.1cm} C{2.1cm} C{2.1cm} }
      \toprule
      \textbf{Using IMRaD Chapter Features} & \textbf{Term Frequency} & \textbf{TF-IDF} & \textbf{Ranked Boolean Retrieval} & \textbf{Okapi BM$25$} & \textbf{Divergence from Randomness} \\ \midrule
      No  & $0.1186$ & $0.1163$ & $0.0466$ & $0.0554$ & $0.0137$ \\
      Yes & $0.1463$ & $0.1613$ & $0.0506$ & $0.0882$ & $0.0137$ \\
      \bottomrule
    \end{tabular}
  \caption[Ranking results using scientific articles]{\textbf{Ranking results of the used weighting schemes using scientific articles.} We compare our proposed ranking algorithms with respect to the underlying structure. Therefore, we use scientific articles unstructured and structured to search for other scientific articles. For structured articles, we focus on the underlying IMRaD structure. Mean average precision was used to evaluate the results.}
  \label{tbl:ranking_result_full}
\end{table}

\Cref{tbl:ranking_result_full} highlights the performance of our proposed ranking algorithms. \textit{TF}, \textit{TF-IDF}, \textit{Ranked Boolean Retrieval}, and \textit{Okapi BM$25$} obtain higher results with the usage of IMRaD chapter features. We find no difference in performance of \textit{Divergence from Randomness} when leveraging IMRaD chapter features.

The ranking of the proposed ranking algorithms is generated with respect to their achieved accuracy when IMRaD chapter features are enabled. The best performing algorithm is \textit{TF-IDF}. It achieves an accuracy of $0.1613$ with enabled features, and $0.1163$ with disabled features. Therefore, structured text retrieval is $4.5 \%$ better than unstructured text retrieval for this configuration.

The second best performing algorithm is \textit{TF}. It achieves an accuracy of $0.1463$ with enabled features, and $0.1186$ with disabled features. Therefore, structured text retrieval is $2.77 \%$ better than unstructured text retrieval for this configuration. In comparison to \textit{TF-IDF} the achieved accuracy is $1.5 \%$ lower for enabled features, and $0.2 \%$ better for disabled features.

The third best performing algorithm is \textit{Okapi BM$25$}. It achieves an accuracy of $0.0882$ with enabled features, and $0.0554$ with disabled features. Therefore, structured text retrieval is $3.28 \%$ better than unstructured text retrieval for this configuration. In comparison to \textit{TF-IDF} the achieved accuracy is $7.31 \%$ lower for enabled features, and $6.09 \%$ lower for disabled features. Furthermore, in comparison to \textit{TF} the accuracy is $5.81 \%$ lower for enabled features, and $6.32 \%$ lower for disabled features. 

The forth best performing algorithm is \textit{Ranked Boolean Retrieval}. Zone scores $zs$ have to be defined to configure the algorithm. For our experiments we determine the best configuration using parameter search. The best performing configuration is as follows:
\begin{align*}
  zs_{\text{Title}} & = 0.2                \nonumber \\
  zs_{\text{Section Title}} & = 0.3        \nonumber \\
  zs_{\text{Section Text}} & = 0.2         \nonumber \\
  zs_{\text{Subsection Title}} & = 0.18    \nonumber \\
  zs_{\text{Subsection Text}} & = 0.05     \nonumber \\
  zs_{\text{Subsubsection Title}} & =0.05  \nonumber \\
  zs_{\text{Subsubsection Text}} & = 0.02, \nonumber
\end{align*}
where "Title" defines the header of the document component, and "Text" the text area of the of the document component. Fur further information about structured text retrieval, document components, and \textit{Ranked Boolean Retrieval} see \Cref{sec:structured_text_Retrieval}. 

\textit{Ranked Boolean Retrieval} achieves an accuracy of $0.0506$ with enabled features, and $0.0466$ with disabled features. Therefore, structured text retrieval is $0.4 \%$ better than unstructured text retrieval for this configuration. In comparison to \textit{TF-IDF} the achieved accuracy is $11.07 \%$ lower for enabled features, and $6.97 \%$ lower for disabled features. Furthermore, in comparison to \textit{Okapi BM$25$} the accuracy is $3.76 \%$ lower for enabled features, and $0.88 \%$ lower for disabled features.

The fifth best performing algorithm is \textit{Divergence from Randomness}. It achieves an accuracy of $0.0137$ with enabled and disabled features. In comparison to \textit{TF-IDF} the achieved accuracy is $14.76 \%$ lower for enabled features, and $10.26 \%$ lower for disabled features. Furthermore, in comparison to \textit{Ranked Boolean Retrieval} the accuracy is $3.69 \%$ lower for enabled features, and $3.29 \%$ lower for disabled features.

When articles are used to search for other articles they can be seen as large and precise queries. IMRaD chapter features are leveraged to define these queries. These are helpful as they introduce constraints that describe the expected content of articles.

As expected, we find that TF-IDF achieves the best accuracy. We used \textit{Okapi BM$25$} only with recommended parameters. An exhaustive parameter search would be necessary to fit our generated dataset. In comparison to the other ranking algorithms \textit{Divergence from Randomness} was always outperformed by all other algorithms. This bad performance is probably related to our dataset. In this experiments it can be seen that the zone scores of \textit{Ranked Boolean Retrieval} are not enough to fit the complexity of large queries. 


\section{Chapter Based Search}

\begin{table}[b!]
\vrule\pgfplotstabletypeset[%
     color cells={min=0,max=0.15,textcolor=black},
     /pgfplots/colormap={blackwhite}{rgb255=(255,170,0) color=(white) rgb255=(255,170,0)},
    /pgf/number format/fixed,
    /pgf/number format/precision=4,
    col sep=comma,
    columns/Section/.style={reset styles,string type}%
]{
Section, Introduction, Background, Methods, Results, Discussion
Introduction, 0.1242, 0.1226, 0.1095, 0.1092, 0.1049
Background,   0.1454, 0.1249, 0.1331, 0.1255, 0.1106 
Methods,      0.0947, 0.0857, 0.1017, 0.0897, 0.0668
Results,      0.0877, 0.0783, 0.0815, 0.0783, 0.0631
Discussion,   0.1188, 0.1078, 0.0957, 0.0914, 0.084
}\vrule
  \caption[Chapter based Search using TF-IDF]{\textbf{Chapter based Search using TF-IDF.} Keywords of a single chapter are used to search in individual chapters of other articles. These input chapters are represented as rows, and the search chapters are represented as columns. Mean average precision was used to evaluate the results of the TF-IDF ranking algorithm.}
  \label{tbl:chapter_based_tfidf}
\end{table}

When searching with entire scientific articles (see \Cref{sec:implicit_search_results}) we obtain better results with the usage of IMRaD chapter features compared to the usage without IMRaD chapter features. Therefore, we focus on structured text retrieval and the influence of single chapters on the search result. The idea is that keywords of a chapter can be used to search in other chapters. For example, the Methods section of one paper can be referenced in the Related Work of another paper.

We use the same dataset and evaluation process as for our implicit search evaluation (see \Cref{sec:implicit_search_results}). In our dataset the background chapter is available in addition to the IMRaD chapters. The evaluation was done for \textit{TF-IDF} and \textit{Okapi BM$25$} as they are state-of-the-art ranking algorithms.

The chapter of the article that is used to search for other articles is defined as input chapter. Furthermore, the chapter that is searched in the articles of the collection is defined as search chapter. For example, the introduction of an article is used to search in the discussion of articles in the collection. In this example the introduction is the input chapter and the discussion is the search chapter.

\begin{table}[b!]
\vrule\pgfplotstabletypeset[%
     color cells={min=0.0,max=0.1,textcolor=black},
     /pgfplots/colormap={blackwhite}{rgb255=(255,170,0) color=(white) rgb255=(255,170,0)},
    /pgf/number format/fixed,
    /pgf/number format/precision=4,
    col sep=comma,
    columns/Section/.style={reset styles,string type}%
]{
Section, Introduction, Background, Methods, Results, Discussion
Introduction, 0.0884, 0.0686, 0.0631, 0.061,  0.0708
Background,   0.0909, 0.0715, 0.076,  0.0618, 0.0751
Methods,      0.0565, 0.0417, 0.0593, 0.0379, 0.0403
Results,      0.0438, 0.0426, 0.0433, 0.0461, 0.0443
Discussion,   0.0799, 0.0682, 0.0587, 0.0595, 0.0616
}\vrule
  \caption[Chapter based Search using Okapi BM$25$]{\textbf{Chapter based Search using Okapi BM$25$.} Keywords of a single chapter are used to search in individual chapters of other articles. These input chapters are represented as rows, and the search chapters are represented as columns. Mean average precision was used to evaluate the results of the Okapi BM$25$ ranking algorithm.}
  \label{tbl:chapter_based_okapi}
\end{table}

\Cref{tbl:chapter_based_tfidf} highlights the performance results of \textit{TF-IDF}. When \textit{Introduction}, \textit{Background}, \textit{Results}, or \textit{Discussion} is the input chapter the best results are obtained when \textit{Introduction} is the search chapter. Furthermore, when \textit{Methods} is the input chapter then the best performance is given if \textit{Methods} is also the search chapter. The highest accuracy is $0.1454$, where \textit{Background} is the input chapter, and \textit{Introduction} the search chapter. When summing up accuracies \textit{Background} is the best performing input chapter, and \textit{Introduction} the best performing search chapter.

\Cref{tbl:chapter_based_okapi} highlights the performance results of \textit{Okapi BM$25$}. When \textit{Introduction}, \textit{Background}, or \textit{Discussion} is the input chapter the best results are obtained when \textit{Introduction} is the search chapter. For \textit{Methods} and \textit{Results} the best results are obtained when they are used as input chapter as well as search chapter. The highest accuracy is $0.0909$, where \textit{Background} is the input chapter, and \textit{Introduction} the search chapter. When summing up accuracies \textit{Background} is the best performing input chapter, and \textit{Introduction} the best performing search chapter.

\textit{TF-IDF} achieves better results than \textit{Okapi BM$25$} when comparing the performance results. Both ranking algorithms have \textit{Background} as their best performing input chapter, and \textit{Introduction} as their best performing search chapter. When comparing their highest accuracy \textit{TF-IDF} is $5.45 \%$ better than \textit{Okapi BM$25$}. This reflects also approximately the performance of the other input chapters and search chapters.

The accuracy of implicit search with enabled IMRaD chapter Features is $1.59 \%$ better than the highest accuracy of search with single chapters. One interesting point is that queries for single chapters are one fifth of queries of implicit search, but have almost the same performance. We used \textit{Okapi BM$25$} only with recommended parameters. A more precised parameter search would be necessary to suite our generated dataset.