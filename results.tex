\chapter{Evaluation}
\label{cha:evaluation}

In this chapter we discuss the results of our information retrieval model. For our evaluation we took a self generated dataset with $821$ scientific articles. Each document in the dataset was stored with its logical structure (title, headings, chapters, sections, subsections, subsubsections). Additionally, sections contains information about their IMRaD-Types. Normally a section has only one IMRaD-Type, but some of them have more (e.g. Results and Discussion has IMRaD-Type Result and IMRaD-Type Discussion). Furthermore, the articles was linked according their citations. For more information about the creation of our used dataset see ~\Cref{sec:dataset}.

In our experiments we compare $5$ common ranking algorithms:
\begin{enumerate}
  \item First, \textit{Term Frequency} was the simplest approach to rank generated lists. We used \textit{Raw Frequency} of the term frequency variants (for different variants of term frequency and inverse document frequency see \Cref{sec:tfidf}). There the occurrences of each query term are counted within the document.
  \item Second, \textit{TF-IDF} to take inverse document frequency into account. The inverse document frequency represents the importance of a term regarding the entire document collection. Therefore, \textit{Inverse Frequency} was applied in combination with \textit{Raw Frequency}.
  \item Third, \textit{Okapi BM$25$} is a baseline to compare ranking algorithms. We used the params as suggested by Robertson et. al ~\cite{RobertsonWJHG94}. Therefore, we set $b=0.75$ and $K_1 = 1$ (cf. \Cref{sec:okapi_bm25}).
  \item Fourth, \textit{Divergence from Randomness} has $2$ assumptions that are formalized with equations. In our experiments we used \Cref{dvr_part1_1} and \Cref{dvr_part2_1} for our practical implementation of the assumptions. Furthermore, \Cref{dfv_avg_doclen_1} was applied for document length normalization (cf. \Cref{sec:divergence_from_randomness}). 
  \item Fifth, \textit{Ranked Boolean Retrieval} is a ranking method that combines zone scores with boolean expressions. The score is applied to the ranking if terms occurs in the zone. The configuration of the algorithm depend on the experiment (cf. \Cref{sec:ranking_strategies_in_xml_retrieval}).
\end{enumerate}

\section{Evaluation of Ranking Algorithms}
\label{sec:evaluation_of_ranking_algorithms}

The effectivity of an information retrieval system (IRS) is lean on the underlying ranking function. Therefore, it is important to measure the performance of these ranking functions to make them comparable.

\myfig{precision_recall}
      {width=0.50\textwidth}
      {\textbf{Document collection split according their relevance.} During the evaluation of an ranking algorithm the document collection is split into $4$ subsets. First, the \textit{true positive} documents are retrieved as relevant for the user, and are relevant. Second, the \textit{false positive} documents are retrieved as relevant, but are not relevant. Third, the \textit{true negatives} documents were correctly received as not relevant. Forth, the \textit{false negatives} documents were wrongly received as not relevant. The \textit{true positive} set, and \textit{true negative} set should be as large as possible as it is directly related to the effectivity of the ranking algorithm.}
      {Document collection split according their relevance.}
      {fig:precision_recall}

Evaluation of a IRS is based on the set of relevant documents provided by the system. To evaluate a generated set without any ranking (unordered) $2$ information retrieval basic measures known as \textit{Precision} and \textit{Recall} are used. \textit{Precision} is a measurement of retrieved documents (see \Cref{fig:precision_recall} for sets in the document collection split according their relevance) that are relevant for the user:
\begin{align}
  \label{precision}
  \text{Precision} = & \frac{\text{\# relevant items retrieved}}{\text{\# retrieved items}} \nonumber \\
    = & P(\text{relevant} | \text{retrieved}) \nonumber \\
    = & \frac{\text{true positives}}{\text{true positives} + \text{false positives}}.
\end{align}
\textit{Recall} represents the fraction of relevant documents that are received:
\begin{align}
  \text{Recall} = & \frac{\text{\# relevant items retrieved}}{\text{\# relevant items}} \nonumber \\
    = & P(\text{retrieved} | \text{relevant}) \nonumber \\
    = & \frac{\text{true positives}}{\text{true positives} + \text{false negatives}}.
\end{align}
There is a trade-off between the two measures. Therefore, when having a \textit{Recall} of $1$ it is possible to have a low \textit{Precision}. This happens as \textit{Recall} always increasing until all relevant documents are retrieved, but new received documents can be \textit{false positives}. Hence, the \textit{Precision} decreases, and \textit{Recall} stays the same.

There exist many different metrics to evaluate generated sets with ranking (ordered). Mean Average Precision is one of the most commonly used evaluation techniques. It is defined as the average of all precision values after a new relevant document is observed:
\begin{align}
  \label{map_of_a_single_query}
  \mathit{MAP}_i = \frac{1}{|R_i|}\sum_{k = 1}^{|R_i|} P(R_i[k]),
\end{align}
where $R_i$ is the set of relevant documents with respect to query $q_i$. $R_i[k]$ represents the reference of the $k$th document in $R_i$, and $P(R_i[k])$ is the \textit{Precision} of the document (see \Cref{precision}). If $R_i[k]$ belongs to a \textit{false positive} document $P(R_i[k]) = 0$. Furthermore, the Mean Average Precision over a set of queries is defined as:
\begin{align}
  \mathit{MAP} = \frac{1}{N_q}\sum_{i = 1}^{|N_q|} \mathit{MAP}_i,
\end{align}
where $N_q$ is the total number of queries.

\myfig{map}
      {width=1.00\textwidth}
      {\textbf{Example for Mean Average Precision of a single query.} The Precision values are calculated according the retrieved set of relevant documents. Documents with a green hook are \textit{true positives} (TP), and documents with a red cross are \textit{false positives} (FP). The precision values down the FP are ignored, as they are not counted. In the example $R_1 =$ \{TP, FP, TP, TP, FP, TP\}, and therefore $|R_1| = 6$. Inserting these values into the Mean Average Precision formula of a single query (see \Cref{map_of_a_single_query}) results with $\mathit{MAP}_1 = 1/6 \times (1 + (2/3) + (3/4) + (4/6)) = 0,513\overline{8}$.}
      {Example for Mean Average Precision of a single query}
      {fig:map}

 Mean Average Precision is widely used as it is simple, easy to implement, versatile, and stable. Therefore, we applied it to compare the generated ranked lists of our ranking algorithms (for an example see \Cref{fig:map}).

\section{Leveraging IMRaD Structure Features}

In our first experiments we investigate on the comparison of unstructured text retrieval against structured text retrieval. The focus for structured text retrieval is on the underlying IMRaD structure. Therefore, each document of the dataset contains a set with all index terms. In addition they contain $5$ sets for each IMRaD-type. The union of the $5$ sets is unequal the set with all index terms as there are areas that cannot be assigned to an IMRaD-type (e.g. Abstract). 

First, we evaluate explicit search where query terms have to be formulated explicitly. For example, given a query $q_1=$"\textit{network} IN \texttt{Methods}" the system takes different sets to search for the term. For structured text retrieval the Methods-set is used as it was specified in the query. In our system IN-statements of queries are ignored for unstructured text retrieval as all terms are searched in the set with all index terms.

Second, we discuss implicit search using whole scientific articles to search for other articles. For structured text retrieval we split the input paper according its IMRaD-structure. With the extracted terms we generate a query that is similar to the query of the explicit variant. For unstructured text retrieval the same process us used to create the query, but one more time the IN-statements are ignored.

\subsection{Explicit Search using N-Grams}

experimental setup (extract citations), describe results afterwards, ranked boolean setup

\begin{table}[b]
  \begin{adjustwidth}{-2cm}{}
    \begin{tabular}{ c C{1.5cm} C{2.1cm} C{2.1cm} C{2.1cm} C{2.1cm} C{2.1cm} C{2.1cm} }
      \toprule
      \textbf{n} & \textbf{\# queries} & \textbf{Using IMRaD Chapter Features} & \textbf{Term Frequency} & \textbf{TF-IDF} & \textbf{Ranked Boolean Retrieval} & \textbf{Okapi BM25} & \textbf{Divergence from Randomness} \\ \midrule
      \multirow{2}{*}{$2$} & \multirow{2}{*}{$8770$} & No  & $0.0882$ & $0.1128$ & $0.1035$ & $0.0442$ & $0.0498$  \\
                                                    && Yes & $0.0696$ & $0.0897$ & $0.0638$ & $0.045$  & $0.0379$  \\ \midrule
      \multirow{2}{*}{$3$} & \multirow{2}{*}{$7070$} & No  & $0.1038$ & $0.1382$ & $0.1198$ & $0.064$  & $0.046$   \\
                                                    && Yes & $0.0785$ & $0.1042$ & $0.0713$ & $0.0628$ & $0.0323$  \\ \midrule
      \multirow{2}{*}{$4$} & \multirow{2}{*}{$5589$} & No  & $0.1197$ & $0.1547$ & $0.1336$ & $0.0739$ & $0.0448$  \\
                                                    && Yes & $0.0894$ & $0.1167$ & $0.0787$ & $0.0734$ & $0.031$   \\ \midrule
      \multirow{2}{*}{$5$} & \multirow{2}{*}{$4347$} & No  & $0.1317$ & $0.1689$ & $0.1479$ & $0.0794$ & $0.0416$  \\
                                                    && Yes & $0.0993$ & $0.1262$ & $0.0844$ & $0.0791$ & $0.0317$  \\ \midrule
      \multirow{2}{*}{$6$} & \multirow{2}{*}{$3336$} & No  & $0.1469$ & $0.1766$ & $0.1603$ & $0.0804$ & $0.0396$  \\
                                                    && Yes & $0.1042$ & $0.1319$ & $0.0918$ & $0.0819$ & $0.0311$  \\ \midrule
      \multirow{2}{*}{$7$} & \multirow{2}{*}{$2550$} & No  & $0.1588$ & $0.1857$ & $0.167$  & $0.0817$ & $0.0404$  \\
                                                    && Yes & $0.1085$ & $0.1375$ & $0.0961$ & $0.0842$ & $0.0312$  \\ \midrule
      \multirow{2}{*}{$8$} & \multirow{2}{*}{$1911$} & No  & $0.1712$ & $0.1957$ & $0.1718$ & $0.0818$ & $0.0449$  \\
                                                    && Yes & $0.1158$ & $0.1441$ & $0.0988$ & $0.0916$ & $0.0303$  \\ \midrule
      \multirow{2}{*}{$9$} & \multirow{2}{*}{$1402$} & No  & $0.1804$ & $0.2074$ & $0.1757$ & $0.0879$ & $0.0456$  \\
                                                    && Yes & $0.1213$ & $0.1496$ & $0.1015$ & $0.0969$ & $0.0301$  \\ \midrule
      \multirow{2}{*}{$10$} & \multirow{2}{*}{$1051$} & No & $0.1847$ & $0.2153$ & $0.1851$ & $0.0952$ & $0.0466$  \\
                                                    && Yes & $0.1235$ & $0.1555$ & $0.1005$ & $0.099$  & $0.0304$  \\ \midrule
      \multirow{2}{*}{$11$} & \multirow{2}{*}{$787$} & No  & $0.1966$ & $0.2199$ & $0.1921$ & $0.1075$ & $0.0439$  \\
                                                    && Yes & $0.1217$ & $0.1589$ & $0.0978$ & $0.1034$ & $0.0296$  \\ \midrule
      \multirow{2}{*}{$12$} & \multirow{2}{*}{$606$} & No  & $0.1923$ & $0.2159$ & $0.1851$ & $0.1102$ & $0.0397$  \\
                                                    && Yes & $0.1293$ & $0.1642$ & $0.0974$ & $0.1043$ & $0.0296$  \\ \midrule
      \multirow{2}{*}{$13$} & \multirow{2}{*}{$470$} & No  & $0.1846$ & $0.205$ & $0.1712$ & $0.1192$ & $0.0319$   \\
                                                    && Yes & $0.1247$ & $0.1637$ & $0.0958$ & $0.1058$ & $0.0295$  \\ \midrule
      \multirow{2}{*}{$14$} & \multirow{2}{*}{$362$} & No  & $0.1634$ & $0.1919$ & $0.1558$ & $0.1207$ & $0.0221$  \\
                                                    && Yes & $0.1183$ & $0.1589$ & $0.0964$ & $0.1008$ & $0.0311$  \\
      \bottomrule
    \end{tabular}
  \caption[Ranking results with explicit search]{Ranking results of the used weighting schemes using explicit search}
  \label{tbl:ranking_result_explicit}
  \end{adjustwidth}
\end{table}

\subsection{Implicit Search using Scientific Articles}

Number of scientific articles: $821$

... more like this. describe evaluation, ranked boolean setup

\begin{table}[b]
    \centering
    \begin{tabular}{ C{2.1cm} C{2.1cm} C{2.1cm} C{2.1cm} C{2.1cm} C{2.1cm} }
      \toprule
      \textbf{Using IMRaD Chapter Features} & \textbf{Term Frequency} & \textbf{TF-IDF} & \textbf{Ranked Boolean Retrieval} & \textbf{Okapi BM25} & \textbf{Divergence from Randomness} \\ \midrule
      No  & $0.1186$ & $0.1163$ & $0.0466$ & $0.0554$ & $0.0137$ \\
      Yes & $0.1463$ & $0.1613$ & $0.0506$ & $0.0882$ & $0.0137$ \\
      \bottomrule
    \end{tabular}
  \caption[Ranking results using scientific articles]{Ranking results of the used weighting schemes using scientific articles}
  \label{tbl:ranking_result_full}
\end{table}


\section{Chapter Based Search}

Number of scientific articles: $821$

describe evaluation, ranked boolean setup

\begin{table}
\vrule\pgfplotstabletypeset[%
     color cells={min=0,max=0.15,textcolor=black},
     /pgfplots/colormap={blackwhite}{rgb255=(255,170,0) color=(white) rgb255=(255,170,0)},
    /pgf/number format/fixed,
    /pgf/number format/precision=4,
    col sep=comma,
    columns/Section/.style={reset styles,string type}%
]{
Section, Introduction, Background, Methods, Results, Discussion
Introduction, 0.1242, 0.1226, 0.1095, 0.1092, 0.1049
Background,   0.1454, 0.1249, 0.1331, 0.1255, 0.1106 
Methods,      0.0947, 0.0857, 0.1017, 0.0897, 0.0668
Results,      0.0877, 0.0783, 0.0815, 0.0783, 0.0631
Discussion,   0.1188, 0.1078, 0.0957, 0.0914, 0.084
}\vrule
  \caption[Chapter based Search using Tf-idf]{Chapter based Search using Tf-idf}
  \label{tbl:ranking_result_full}
\end{table}

\begin{table}
\vrule\pgfplotstabletypeset[%
     color cells={min=0,max=0.1,textcolor=black},
     /pgfplots/colormap={blackwhite}{rgb255=(255,170,0) color=(white) rgb255=(255,170,0)},
    /pgf/number format/fixed,
    /pgf/number format/precision=4,
    col sep=comma,
    columns/Section/.style={reset styles,string type}%
]{
Section, Introduction, Background, Methods, Results, Discussion
Introduction, 0.0884, 0.0686, 0.0631, 0.061,  0.0708
Background,   0.0909, 0.0715, 0.076,  0.0618, 0.0751
Methods,      0.0565, 0.0417, 0.0593, 0.0379, 0.0403
Results,      0.0438, 0.0426, 0.0433, 0.0461, 0.0443
Discussion,   0.0799, 0.0682, 0.0587, 0.0595, 0.0616
}\vrule
  \caption[Chapter based Search using Okapi BM$25$]{Chapter based Search using Okapi BM$25$}
  \label{tbl:ranking_result_full}
\end{table}
