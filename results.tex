\chapter{Results and Discussion}
\label{cha:results_discussion}

In this chapter we discuss the results of our information retrieval model. For our evaluation, we generated a dataset with $821$ scientific articles. Each document in the dataset was stored with its logical structure (title, headings, chapters, sections, subsections, subsubsections). Additionally, sections contain information about their IMRaD-Types. Usually a section has only one IMRaD-Type, but some of them have more (e.g., Results and Discussion are assigned the IMRaD-Types Result and Discussion). Furthermore, the articles are linked according their citations. For more information about the creation of our dataset see ~\Cref{sec:dataset}.

In our experiments we compare $5$ common ranking algorithms:
\begin{enumerate}
  \item First, \textit{Term Frequency} is the simplest approach to rank generated lists. We used \textit{Raw Frequency} of the term frequency variants. There the occurrences of each query term are counted within the document. To read more about different variants of term frequency and inverse document frequency see \Cref{sec:tfidf}.
  \item Second, \textit{TF-IDF} to take inverse document frequency into account. The inverse document frequency represents the importance of a term with respect to the entire document collection. Therefore, \textit{Inverse Frequency} is applied in combination with \textit{Raw Frequency}.
  \item Third, \textit{Okapi BM$25$} is a baseline ranking algorithm. We use the params as suggested by Robertson et. al ~\cite{RobertsonWJHG94}. Therefore, we set $b=0.75$ and $K_1 = 1$ (cf. \Cref{sec:okapi_bm25}).
  \item Fourth, \textit{Divergence from Randomness} has $2$ assumptions that are formalized with equations. In our experiments we used \Cref{dvr_part1_1} and \Cref{dvr_part2_1} for our practical implementation of the assumptions. Furthermore, \Cref{dfv_avg_doclen_1} was applied for document length normalization (cf. \Cref{sec:divergence_from_randomness}). 
  \item Fifth, \textit{Ranked Boolean Retrieval} is a ranking method that combines zone scores with boolean expressions. The score is applied to the ranking if terms occur in the zone. The configuration of the algorithm depends on the experiment (cf. \Cref{sec:ranking_strategies_in_xml_retrieval}).
\end{enumerate}


\section{Evaluation of Ranking Algorithms}
\label{sec:evaluation_of_ranking_algorithms}

The effectivity of an information retrieval system (IRS) is lean on the underlying ranking function. Therefore, it is important to measure the performance of these ranking functions to make them comparable.

\myfig{precision_recall}
      {width=0.50\textwidth}
      {\textbf{Document collection split according their relevance.} During the evaluation of a ranking algorithm the document collection is split into $4$ subsets. First, the \textit{true positive} documents retrieved as relevant for the user, which are actually relevant. Second, the \textit{false positive} documents retrieved as relevant, but are not relevant for the user. Third, the \textit{true negatives} documents were correctly received as not relevant. Forth, the \textit{false negatives} documents were incorrectly received as not relevant. The \textit{true positive} set, and \textit{true negative} set should be as large as possible as it is directly related to the effectivity of the ranking algorithm.}
      {Document collection split according their relevance.}
      {fig:precision_recall}

Evaluation of a IRS is based on the set of relevant documents provided by the system. To evaluate a generated set without any ranking (unordered) $2$ information retrieval basic measures known as \textit{Precision} and \textit{Recall} are used. \textit{Precision} is a measurement of retrieved documents (see \Cref{fig:precision_recall} for sets in the document collection split according their relevance) that are relevant for the user:
\begin{align}
  \label{precision}
  \text{Precision} = & \frac{\text{\# relevant items retrieved}}{\text{\# retrieved items}} \nonumber \\
    = & P(\text{relevant} | \text{retrieved}) \nonumber \\
    = & \frac{\text{true positives}}{\text{true positives} + \text{false positives}}.
\end{align}
\textit{Recall} represents the fraction of relevant documents that are received:
\begin{align}
  \text{Recall} = & \frac{\text{\# relevant items retrieved}}{\text{\# relevant items}} \nonumber \\
    = & P(\text{retrieved} | \text{relevant}) \nonumber \\
    = & \frac{\text{true positives}}{\text{true positives} + \text{false negatives}}.
\end{align}
There is a trade-off between the two measures. Therefore, when having a \textit{Recall} of $1$ it is possible to have a low \textit{Precision}. This happens as \textit{Recall} always increasing until all relevant documents are retrieved, but new received documents can be \textit{false positives}. Hence, the \textit{Precision} decreases, and \textit{Recall} stays the same.

There exist many different metrics to evaluate generated sets with ranking (ordered). Mean Average Precision is one of the most commonly used evaluation techniques. It is defined as the average of all precision values after a new relevant document is observed:
\begin{align}
  \label{map_of_a_single_query}
  \mathit{MAP}_i = \frac{1}{|R_i|}\sum_{k = 1}^{|R_i|} P(R_i[k]),
\end{align}
where $R_i$ is the set of relevant documents with respect to query $q_i$. $R_i[k]$ represents the reference of the $k$th document in $R_i$, and $P(R_i[k])$ is the \textit{Precision} of the document (see \Cref{precision}). If $R_i[k]$ belongs to a \textit{false positive} document $P(R_i[k]) = 0$. Furthermore, the Mean Average Precision over a set of queries is defined as:
\begin{align}
  \mathit{MAP} = \frac{1}{N_q}\sum_{i = 1}^{|N_q|} \mathit{MAP}_i,
\end{align}
where $N_q$ is the total number of queries.

\myfig{map}
      {width=1.00\textwidth}
      {\textbf{Example for Mean Average Precision of a single query.} The Precision values are calculated according the retrieved set of relevant documents. Documents with a green hook are \textit{true positives} (TP), and documents with a red cross are \textit{false positives} (FP). The precision values of FP documents are ignored for these documents, as they are not counted. In the example $R_1 =$ \{TP, FP, TP, TP, FP, TP\}, and therefore $|R_1| = 6$. Inserting these values into the Mean Average Precision formula of a single query (see \Cref{map_of_a_single_query}) results in $\mathit{MAP}_1 = 1/6 \times (1 + (2/3) + (3/4) + (4/6)) = 0.513\overline{8}$.}
      {Example for Mean Average Precision of a single query}
      {fig:map}

 Mean Average Precision is widely used as it is simple, easy to implement, versatile, and stable. Therefore, we apply it to compare the generated ranked lists of our proposed ranking algorithms (for an example see \Cref{fig:map}).


\section{Leveraging IMRaD Structure Features}
\label{sec:leveraging_imrad_structure_features}

In our first experiments we compare unstructured text retrieval with structured text retrieval. For structured text retrieval, we focus on the underlying IMRaD structure. Therefore, each document of the dataset contains a set of all index terms. In addition, they contain $5$ sets for each IMRaD-type. The union of the $5$ sets differs from the set with all index terms as there exists areas that cannot be assigned to an IMRaD-type (e.g., Abstract). 

First, we evaluate explicit search where query terms have to be formulated explicitly. For example, given a query $q_1=$"\textit{network} IN \texttt{Methods}" the system leverages different sets to search for the term. For structured text retrieval the Methods-set is used as it was specified in the query. In our system IN-statements of queries are ignored for unstructured text retrieval as all terms are searched in the set with all index terms.

Second, we discuss implicit search using entire scientific articles to search for other articles. For structured text retrieval we split the input paper according to its IMRaD-structure. We generate a query that is similar to the query of the explicit variant with the extracted terms. For unstructured text retrieval the same process is used to create the query, but IN-statements are ignored here as well.


\subsection{Explicit Search using N-Grams}

To search with information provided by the user is called explicit search. For our information retrieval model this is done by formalizing user queries. These queries can be seen as a sequence of keywords.

Our generated dataset consists of scientific articles, and links between them. We extracted citations in order to get queries that are used for testing. Our assumption was that citations describe the content of referenced articles. Therefore, a referenced article gets represented by the citation.

We store each citation as a set of terms. An important addition was that the order of the terms within the set was not lost. Additionally, we assume that a typical user searches by stringing together keywords. Therefore, we removed stopwords from the set, but terms was not stemmed. For each citation set we stored information about the refereed article, and the IMRaD-Types of the section. For example, "\textit{The authors of [$1$] present a comprehensive system for the structure extraction of PDF books, which is used within a commercial e-book software.}" is in the \textit{Introduction} of scientific article $A$. Furthermore, "\textit{[$1$]}" is the reference to the refereed scientific article $B$. The resulting citation set looks as follows: $cs_{A1} =$ \{"\textit{authors}", "\textit{present}", "\textit{comprehensive}", "\textit{system}", "\textit{structure}", "\textit{extraction}", "\textit{PDF}", "\textit{books}", "\textit{commercial}", "\textit{e-book}", "\textit{software}"\}, and additionally the reference to article $B$, and the IMRaD-Type \textit{Introduction} is stored for $cs_{A1}$.

Queries are produced as $N$-Grams with the citation sets. This means the citation sets are split into subsets of size $N$. Furthermore, the order of the terms is also important for these subsets. For example, $cs_{A1}$ is split into $N$-Grams and $N = 5$. The $7$ resulting subsets are:
\begin{align*}
  NG_{cs_{A1}, 1} &= \{"\textit{authors}", "\textit{present}", "\textit{comprehensive}", "\textit{system}", "\textit{structure}"\} \nonumber \\
  NG_{cs_{A1}, 2} &= \{"\textit{present}", "\textit{comprehensive}", "\textit{system}", "\textit{structure}", "\textit{extraction}"\} \nonumber \\
  NG_{cs_{A1}, 3} &= \{"\textit{comprehensive}", "\textit{system}", "\textit{structure}", "\textit{extraction}", "\textit{PDF}"\} \nonumber \\
  NG_{cs_{A1}, 4} &= \{"\textit{system}", "\textit{structure}", "\textit{extraction}", "\textit{PDF}", "\textit{books}"\} \nonumber \\
  NG_{cs_{A1}, 5} &= \{"\textit{structure}", "\textit{extraction}", "\textit{PDF}", "\textit{books}", "\textit{commercial}",\} \nonumber \\
  NG_{cs_{A1}, 6} &= \{"\textit{extraction}", "\textit{PDF}", "\textit{books}", "\textit{commercial}", "\textit{e-book}"\} \nonumber \\
  NG_{cs_{A1}, 7} &= \{"\textit{PDF}", "\textit{books}", "\textit{commercial}", "\textit{e-book}", "\textit{software}"\} \nonumber \\
\end{align*}
In general, the number of subsets that can be generated from a citation set $cs$ is defined as:
\begin{align}
  l = len(cs) - N + 1,
\end{align}
where $len(cs)$ is the number of terms in $cs$. 

The last step is to bring the query set together with the IMRaD-type in our query structure. For example, the resulting query compiled by $NG_{cs_{A1}, 1}$ looks as follows: $q_1=$"\textit{authors}, \textit{present}, \textit{comprehensive}, \textit{system}, \textit{structure} IN \texttt{Introduction}". In our system IN-statements of queries are ignored if IMRaD chapter features are disabled.

Mean Average Precision (see \Cref{sec:evaluation_of_ranking_algorithms}) is used to evaluate our proposed ranking algorithms. Therefore, we passed the query into our information retrieval system. The Average Precision is determined with respect to the position of the refereed article in the generated ranked list.

In \Cref{tbl:ranking_result_explicit} the performance results of our ranking algorithms are proposed. We evaluated different query lengths, where the length is defined by the number of terms in the query. Our query length varying in the range between $2$ to $14$. Furthermore, we compared the algorithms with respect to the usage of IMRaD chapter features, where disabled features denote unstructured text retrieval and enabled features represent structured text retrieval. The best results with enabled features are highlighted in violet and with disabled features are highlighted in blue for every algorithm.

All $5$ algorithms obtain higher results without IMRaD chapter features. Therefore, the ranking was generated with respect to their achieved accuracy when IMRaD chapter features are disabled. The best performing algorithm is \textit{TF-IDF}. It archives a MAP of $0.2199$ with disabled features, where the query consists of $11$ terms. In addition, the MAP is $0.1642$ with enabled features, and a query length of $12$. Therefore, the comparison of the highest accuracies is $5.57$ percent better for unstructured text retrieval. 

The second best algorithm is \textit{TF}. It has an accuracy of $0.1966$ with disabled features, and a query length of $11$. Furthermore the accuracy is $0.1293$ with enabled features, and a query length of $12$. In comparison with \textit{TF-IDF} the archived accuracy is $2.33$ percent worse for disabled features, and $3.49$ percent worse for enabled features. The best performing query lengths are the same as for \textit{TF-IDF}. The highest accuracy is $6.73$ percent better for unstructured text retrieval than for structured text retrieval.

The third best algorithm is \textit{Ranked Boolean Retrieval}. Zone scores $zs$ have to be defined to configure the algorithm (cf. \Cref{sec:ranking_strategies_in_xml_retrieval}). For the experiments only zones that contain text areas are set to a value greater zero. This was done as all citations was extracted from text areas. In our model these are text areas of sections, subsections, and subsubsections. Therefore, the constant zone scores are $zs_{\text{Section}} = 0.34$, $zs_{\text{Subsection}} = 0.33$, and $zs_{\text{Subsubsection}} = 0.33$.

\begin{table}[h]
  \begin{adjustwidth}{-2cm}{}
    \begin{tabular}{ C{1cm} C{1cm} C{2.1cm} C{2.1cm} C{2.1cm} C{2.1cm} C{2.1cm} C{2.1cm} }
      \toprule
      \textbf{\# terms in query} & \textbf{\# queries} & \textbf{Using IMRaD Chapter Features} & \textbf{Term Frequency} & \textbf{TF-IDF} & \textbf{Ranked Boolean Retrieval} & \textbf{Okapi BM$25$} & \textbf{Divergence from Randomness} \\ \midrule
      \multirow{2}{*}{$2$} & \multirow{2}{*}{$8770$} & No  & $0.0882$ & $0.1128$ & $0.1035$ & $0.0442$ & \color{blue}$\mathbf{0.0498}$  \\
                                                    && Yes & $0.0696$ & $0.0897$ & $0.0638$ & $0.045$  & \color{Plum}$\mathbf{0.0379}$  \\ \midrule
      \multirow{2}{*}{$3$} & \multirow{2}{*}{$7070$} & No  & $0.1038$ & $0.1382$ & $0.1198$ & $0.064$  & $0.046$   \\
                                                    && Yes & $0.0785$ & $0.1042$ & $0.0713$ & $0.0628$ & $0.0323$  \\ \midrule
      \multirow{2}{*}{$4$} & \multirow{2}{*}{$5589$} & No  & $0.1197$ & $0.1547$ & $0.1336$ & $0.0739$ & $0.0448$  \\
                                                    && Yes & $0.0894$ & $0.1167$ & $0.0787$ & $0.0734$ & $0.031$   \\ \midrule
      \multirow{2}{*}{$5$} & \multirow{2}{*}{$4347$} & No  & $0.1317$ & $0.1689$ & $0.1479$ & $0.0794$ & $0.0416$  \\
                                                    && Yes & $0.0993$ & $0.1262$ & $0.0844$ & $0.0791$ & $0.0317$  \\ \midrule
      \multirow{2}{*}{$6$} & \multirow{2}{*}{$3336$} & No  & $0.1469$ & $0.1766$ & $0.1603$ & $0.0804$ & $0.0396$  \\
                                                    && Yes & $0.1042$ & $0.1319$ & $0.0918$ & $0.0819$ & $0.0311$  \\ \midrule
      \multirow{2}{*}{$7$} & \multirow{2}{*}{$2550$} & No  & $0.1588$ & $0.1857$ & $0.167$  & $0.0817$ & $0.0404$  \\
                                                    && Yes & $0.1085$ & $0.1375$ & $0.0961$ & $0.0842$ & $0.0312$  \\ \midrule
      \multirow{2}{*}{$8$} & \multirow{2}{*}{$1911$} & No  & $0.1712$ & $0.1957$ & $0.1718$ & $0.0818$ & $0.0449$  \\
                                                    && Yes & $0.1158$ & $0.1441$ & $0.0988$ & $0.0916$ & $0.0303$  \\ \midrule
      \multirow{2}{*}{$9$} & \multirow{2}{*}{$1402$} & No  & $0.1804$ & $0.2074$ & $0.1757$ & $0.0879$ & $0.0456$  \\
                                                    && Yes & $0.1213$ & $0.1496$ & \color{Plum}$\mathbf{0.1015}$ & $0.0969$ & $0.0301$  \\ \midrule
      \multirow{2}{*}{$10$} & \multirow{2}{*}{$1051$} & No & $0.1847$ & $0.2153$ & $0.1851$ & $0.0952$ & $0.0466$  \\
                                                    && Yes & $0.1235$ & $0.1555$ & $0.1005$ & $0.099$  & $0.0304$  \\ \midrule
      \multirow{2}{*}{$11$} & \multirow{2}{*}{$787$} & No  & \color{blue}$\mathbf{0.1966}$ & \color{blue}$\mathbf{0.2199}$ & \color{blue}$\mathbf{0.1921}$ & $0.1075$ & $0.0439$  \\
                                                    && Yes & $0.1217$ & $0.1589$ & $0.0978$ & $0.1034$ & $0.0296$  \\ \midrule
      \multirow{2}{*}{$12$} & \multirow{2}{*}{$606$} & No  & $0.1923$ & $0.2159$ & $0.1851$ & $0.1102$ & $0.0397$  \\
                                                    && Yes & \color{Plum}$\mathbf{0.1293}$ & \color{Plum}$\mathbf{0.1642}$ & $0.0974$ & $0.1043$ & $0.0296$  \\ \midrule
      \multirow{2}{*}{$13$} & \multirow{2}{*}{$470$} & No  & $0.1846$ & $0.205$ & $0.1712$ & $0.1192$ & $0.0319$   \\
                                                    && Yes & $0.1247$ & $0.1637$ & $0.0958$ & \color{Plum}$\mathbf{0.1058}$ & $0.0295$  \\ \midrule
      \multirow{2}{*}{$14$} & \multirow{2}{*}{$362$} & No  & $0.1634$ & $0.1919$ & $0.1558$ & \color{blue}$\mathbf{0.1207}$ & $0.0221$  \\
                                                    && Yes & $0.1183$ & $0.1589$ & $0.0964$ & $0.1008$ & $0.0311$  \\
      \bottomrule
    \end{tabular}
  \caption[Ranking results with explicit search]{\textbf{Ranking results of our proposed ranking algorithms using explicit search.} We compared our proposed ranking algorithms with respect to the usage of IMRaD chapter features. Without IMRaD chapter features the entire document is used to search for query terms (unstructured). When IMRaD chapter features are enabled query terms are searched only specified sections.}
  \label{tbl:ranking_result_explicit}
  \end{adjustwidth}
\end{table}
\clearpage

\textit{Ranked Boolean Retrieval} archives an accuracy of $0.1921$ with disabled features, and a query length of $11$. Therefore, in the context of unstructured text retrieval the best performing query length is the same as for \textit{TF-IDF} and \textit{TF}. Additionally, the accuracy is $0.1015$ with enabled features, and a query length of $9$. In comparison with \textit{TF-IDF} the archived accuracy is $2.78$ percent worse for disabled features, and $6.27$ percent worse for enabled features. Furthermore, when comparing with \textit{TF} the accuracy is $0.45$ percent worse for disabled features, and $2.78$ percent worse for enabled features. The highest accuracy is $9.06$ percent better for unstructured text retrieval than for structured text retrieval.

The forth best algorithm is \textit{Okapi BM$25$}. It has an accuracy of $0.1207$ with disabled features. The associated best performing query length is $14$, which was the upper bound of the query length. It was not possible to further increase the query length as the upper bound of the query length comes with the number of queries. Furthermore, the accuracy is $0.1058$ with enabled features, and a query length of $13$. In comparison with \textit{TF-IDF} the archived accuracy is $9.92$ percent worse for disabled features, and $5.84$ percent worse for enabled features. Furthermore, when comparing with \textit{Ranked Boolean Retrieval} the accuracy is $7.14$ percent worse for disabled features, and $0.43$ percent better for enabled features. The highest accuracy is $1.49$ percent better for unstructured text retrieval than for structured text retrieval.

The fifth best algorithm is \textit{Divergence from Randomness}. It has an accuracy of $0.0498$ with disabled features, and an accuracy of $0.0379$ with enabled features. Both configurations had a best performing query length of $2$. In comparison with \textit{TF-IDF} the archived accuracy is $17.01$ percent worse for disabled features, and $12.63$ percent worse for enabled features. Furthermore, when comparing with \textit{Okapi BM$25$} the accuracy is $7.09$ percent worse for disabled features, and $6.79$ percent worse for enabled features. The highest accuracy is $1.19$ percent better for unstructured text retrieval than for structured text retrieval.

When only a few keywords are used to search for scientific articles it is not necessary to have the overhead of IMRaD chapter features. This happens as the keywords define content that should occur anywhere in the articles. Additional constraints where terms should appear are rather obstructive as they tend to hinder the finding of relevant articles.

It was unsurprisingly that \textit{TF-IDF} has a high accuracy. We used \textit{Okapi BM$25$} only with recommended parameters. A more precised parameter search would be necessary to suite our generated dataset. In comparison with the other ranking algorithms \textit{Divergence from Randomness} always had a bad performance. This bad performance is probably related to the used dataset. The good performance of \textit{Ranked Boolean Retrieval} was surprising. This probably happens as the zone scores are leveraging to hide unnecessary article areas, and mark important article areas.


\subsection{Implicit Search using Scientific Articles}
\label{sec:implicit_search_results}

In common information retrieval models there is information available that was not explicitly provided by the user. Leveraging this type of information in addition is denoted implicit search. In our model we use explicit and implicit information of scientific articles to search for other articles. This approach is called \textit{more like this}.

In our experiment we transformed scientific articles into user queries. Therefore, queries contain information about index terms and the IMRaD sections they occurred. For example, $q_1=$"\textit{structure}, \textit{present} IN \texttt{Introduction} AND \textit{system}, \textit{structure} IN \texttt{Methods}" is a query that can be executed by our system. IN-statements are used to define where the terms occurred, and AND-statements are used to combine subqueries. In our system IN-statements of queries are ignored if IMRaD chapter features are disabled.

We used a generated dataset with $821$ scientific articles to evaluate our proposed ranking algorithms. Therefore, we assumed that the content of a scientific article has to be similar to its refereed articles. As a result, the Mean Average Precision (see \Cref{sec:evaluation_of_ranking_algorithms}) is calculated with respect to the position of the refereed articles in the generated ranked list.

\begin{table}[b!]
    \centering
    \begin{tabular}{ C{2.1cm} C{2.1cm} C{2.1cm} C{2.1cm} C{2.1cm} C{2.1cm} }
      \toprule
      \textbf{Using IMRaD Chapter Features} & \textbf{Term Frequency} & \textbf{TF-IDF} & \textbf{Ranked Boolean Retrieval} & \textbf{Okapi BM$25$} & \textbf{Divergence from Randomness} \\ \midrule
      No  & $0.1186$ & $0.1163$ & $0.0466$ & $0.0554$ & $0.0137$ \\
      Yes & $0.1463$ & $0.1613$ & $0.0506$ & $0.0882$ & $0.0137$ \\
      \bottomrule
    \end{tabular}
  \caption[Ranking results using scientific articles]{\textbf{Ranking results of the used weighting schemes using scientific articles.} We compared our proposed ranking algorithms with respect to the underlying structure. Therefore, we used scientific articles unstructured and structured to search for other scientific articles. For structured articles, we focus on the underlying IMRaD structure. Mean average precision was used to evaluate the results.}
  \label{tbl:ranking_result_full}
\end{table}

In table \Cref{tbl:ranking_result_full} the performance results of our ranking algorithms are proposed. \textit{TF}, \textit{TF-IDF}, \textit{Ranked Boolean Retrieval}, and \textit{Okapi BM$25$} obtain higher results with the usage of IMRaD chapter features. \textit{Divergence from Randomness} performs with features as well as without features.

The ranking of the proposed ranking algorithms was generated with respect to their achieved accuracy when IMRaD chapter features are enabled.  The best performing algorithm is \textit{TF-IDF} with an accuracy of $0.1613$ with enabled features, and $0.1163$ with disabled features. The highest accuracy is $4.5$ percent better for structured text retrieval than for unstructured text retrieval.

The second best performing algorithm is \textit{TF}. It obtained an accuracy of $0.1463$ with enabled features, and $0.1186$ with disabled features. In comparison with \textit{TF-IDF} the archived accuracy is $1.5$ percent worse for enabled features, and $0.2$ percent better for disabled features. The highest accuracy is $2.77$ percent better for structured text retrieval than for unstructured text retrieval.

The third best performing algorithm is \textit{Okapi BM$25$}. It has an accuracy of $0.0882$ with enabled features, and $0.0554$ with disabled features. In comparison with \textit{TF-IDF} the archived accuracy is $7.31$ percent worse for enabled features, and $6.09$ percent worse for disabled features. Furthermore, when comparing with \textit{TF} the accuracy is $5.81$ percent worse for enabled features, and $6.32$ percent worse for disabled features. The highest accuracy is $3.28$ percent better for structured text retrieval than for unstructured text retrieval.

The forth best performing algorithm is \textit{Ranked Boolean Retrieval}. Zone scores $zs$ have to be defined to configure the algorithm. For the experiments the zones scores are determined through a param search, and defined as follows:
\begin{align*}
  zs_{\text{Title}} & = 0.2                \nonumber \\
  zs_{\text{Section Title}} & = 0.3        \nonumber \\
  zs_{\text{Section Text}} & = 0.2         \nonumber \\
  zs_{\text{Subsection Title}} & = 0.18    \nonumber \\
  zs_{\text{Subsection Text}} & = 0.05     \nonumber \\
  zs_{\text{Subsubsection Title}} & =0.05  \nonumber \\
  zs_{\text{Subsubsection Text}} & = 0.02, \nonumber
\end{align*}
where "Title" defines the header of the document component, and "Text" the text area of the of the document component. Fur further information about structured text retrieval, document components, and \textit{Ranked Boolean Retrieval} see \Cref{sec:structured_text_Retrieval}. 

\textit{Ranked Boolean Retrieval} has an accuracy of $0.0506$ with enabled features, and $0.0466$ with disabled features. In comparison with \textit{TF-IDF} the archived accuracy is $11.07$ percent worse for enabled features, and $6.97$ percent worse for disabled features. Furthermore, when comparing with \textit{Okapi BM$25$} the accuracy is $3.76$ percent worse for enabled features, and $0.88$ percent worse for disabled features. The highest accuracy is $0.4$ percent better for structured text retrieval than for unstructured text retrieval.

The fifth best performing algorithm is \textit{Divergence from Randomness}. It has an accuracy of $0.0137$ with enabled and disabled features. In comparison with \textit{TF-IDF} the archived accuracy is $14.76$ percent worse for enabled features, and $10.26$ percent worse for disabled features. Furthermore, when comparing with \textit{Ranked Boolean Retrieval} the accuracy is $3.69$ percent worse for enabled features, and $3.29$ percent worse for disabled features. 

When articles are used to search for other articles they can be seen as large and precised queries. IMRaD chapter features are leveraging to define these queries. This happens as constraints where terms should appear are helpful to describe the expected content of articles.

It was unsurprisingly that \textit{TF-IDF} has a high accuracy. We used \textit{Okapi BM$25$} only with recommended parameters. A more precised parameter search would be necessary to suite our generated dataset. In comparison with the other ranking algorithms \textit{Divergence from Randomness} always had a bad performance. This bad performance is probably related to the used dataset. In this experiments it can be seen that the zone scores of \textit{Ranked Boolean Retrieval} are not enough to fit the complexity of large queries. 


\section{Chapter Based Search}

\begin{table}[b!]
\vrule\pgfplotstabletypeset[%
     color cells={min=0,max=0.15,textcolor=black},
     /pgfplots/colormap={blackwhite}{rgb255=(255,170,0) color=(white) rgb255=(255,170,0)},
    /pgf/number format/fixed,
    /pgf/number format/precision=4,
    col sep=comma,
    columns/Section/.style={reset styles,string type}%
]{
Section, Introduction, Background, Methods, Results, Discussion
Introduction, 0.1242, 0.1226, 0.1095, 0.1092, 0.1049
Background,   0.1454, 0.1249, 0.1331, 0.1255, 0.1106 
Methods,      0.0947, 0.0857, 0.1017, 0.0897, 0.0668
Results,      0.0877, 0.0783, 0.0815, 0.0783, 0.0631
Discussion,   0.1188, 0.1078, 0.0957, 0.0914, 0.084
}\vrule
  \caption[Chapter based Search using TF-IDF]{\textbf{Chapter based Search using TF-IDF.} Keywords of a single chapter are used to search in single chapters of other articles. This input chapters are represented as rows, and the search chapters are represented as columns. Mean average precision was used to evaluate the results of the TF-IDF ranking algorithm.}
  \label{tbl:chapter_based_tfidf}
\end{table}

When searching with entire scientific articles we obtain better results with the usage of IMRaD chapter features in our previous experiments. Therefore, we focus on structured text retrieval and the influence of single chapters on the search result. The idea is that keywords of a chapter can be used to search in in other chapters. For example, the Methods section of one paper would be referenced in the Related Work of another paper.

We used the same dataset and evaluation process as for our implicit search evaluation (see \Cref{sec:implicit_search_results}). In our used dataset the background chapter was available in addition to the IMRaD chapters. The evaluation was done for \textit{TF-IDF} and \textit{Okapi BM$25$} as they are foundational ranking algorithms.

The chapter of the article that is used to search for other articles is defined as input chapter. Furthermore, the chapter that is searched in the articles of the collection is defined as search chapter. For example, the introduction of an article is used to search in the discussion of articles in the collection. The introduction is called input chapter and the discussion is called search chapter.

In \Cref{tbl:chapter_based_tfidf} the performance results of the \textit{TF-IDF} ranking algorithm is proposed. When \textit{Introduction}, \textit{Background}, \textit{Results}, or \textit{Discussion} is the input chapter the best results are obtained when \textit{Introduction} is the search chapter. Furthermore, when \textit{Methods} is the input chapter then the best performance is given if \textit{Methods} is also the search chapter. The highest accuracy is $0.1454$, where \textit{Background} is the input chapter, and \textit{Introduction} the search chapter. When summing up accuracies \textit{Background} is the best performing input chapter, and \textit{Introduction} the best performing search chapter.

\begin{table}[b!]
\vrule\pgfplotstabletypeset[%
     color cells={min=0.0,max=0.1,textcolor=black},
     /pgfplots/colormap={blackwhite}{rgb255=(255,170,0) color=(white) rgb255=(255,170,0)},
    /pgf/number format/fixed,
    /pgf/number format/precision=4,
    col sep=comma,
    columns/Section/.style={reset styles,string type}%
]{
Section, Introduction, Background, Methods, Results, Discussion
Introduction, 0.0884, 0.0686, 0.0631, 0.061,  0.0708
Background,   0.0909, 0.0715, 0.076,  0.0618, 0.0751
Methods,      0.0565, 0.0417, 0.0593, 0.0379, 0.0403
Results,      0.0438, 0.0426, 0.0433, 0.0461, 0.0443
Discussion,   0.0799, 0.0682, 0.0587, 0.0595, 0.0616
}\vrule
  \caption[Chapter based Search using Okapi BM$25$]{\textbf{Chapter based Search using Okapi BM$25$.} Keywords of a single chapter are used to search in single chapters of other articles. This input chapters are represented as rows, and the search chapters are represented as columns. Mean average precision was used to evaluate the results of the Okapi BM$25$ ranking algorithm.}
  \label{tbl:chapter_based_okapi}
\end{table}

In \Cref{tbl:chapter_based_okapi} the performance results of the \textit{Okapi BM$25$} ranking algorithm is proposed. When \textit{Introduction}, \textit{Background}, or \textit{Discussion} is the input chapter the best results are obtained when \textit{Introduction} is the search chapter. For \textit{Methods} and \textit{Results} the best results are obtained when they are used as input chapter as well as search chapter. The highest accuracy is $0.0909$, where \textit{Background} is the input chapter, and \textit{Introduction} the search chapter. When summing up accuracies \textit{Background} is the best performing input chapter, and \textit{Introduction} the best performing search chapter.

\textit{TF-IDF} archives better results than \textit{Okapi BM$25$} when comparing the performance results. Both ranking algorithms have \textit{Background} as their best performing input chapter, and \textit{Introduction} as their best performing search chapter. When comparing their highest accuracy \textit{TF-IDF} is $5.45$ percent better than \textit{Okapi BM$25$}. This reflects also approximately the performance of the other input chapters and search chapters.

The accuracy of implicit search with enables IMRaD chapter Features is $1.59$ percent better that the highest accuracy of search with single chapters. One interesting point is that queries for single chapters are approximately a factor of $5$ smaller than queries of implicit search, but have almost the same performance. We used \textit{Okapi BM$25$} only with recommended parameters. A more precised parameter search would be necessary to suite our generated dataset.