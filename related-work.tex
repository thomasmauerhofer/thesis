\chapter{Related Work}
\label{cha:related_work}

\section{Information Retrieval Models}
\label{sec:information_retrieval_models}

Creating an information retrieval system is a complex process that needs to be considered carefully. To aim this goal models are used as a base, where the whole system is sketched. The generation of the model consists of two tasks. The first one is to design a framework which represents the documents and the user queries. The second one is to create a ranking function which generates a numeric rank for each document based on a query. Afterwards these ranks are used by the system to sort the documents.

One of the best known retrieval approaches is retrieval based on index terms. In this context an index term is a keyword which appears in the document collection of the framework. This approach can be implemented efficient since query words can be used easily as index terms with only limited transformations.

In general a information retrieval models can be spitted into $4$ parts. Baeza-Yates and Baeza-Yates ~\cite{ModernInvormationRetrieval1999} define them as a quadruple $[\textbf{D}, \textbf{Q}, \mathcal{F}, \mathcal{R}(q_i, d_j)]$ where:

\begin{enumerate}
  \item \textbf{D} is a set composed of logical views of documents in a collection.
  \item \textbf{Q} is a set composed of logical views of the user information needs. Such representations are called queries.
  \item $\mathcal{F}$ is a framework for modeling document representations, queries and their relationships.
  \item $\mathcal{R}(q_i, d_j)$ is a raking function that associates a real number with a query representation $q_i \in \textbf{Q}$ and a document representation $d_j \in \textbf{D}$. The ranking function generates an order over all documents \textbf{D} with respect to a query $q_i$.
\end{enumerate}

In relation to this definition it can be seen that the model is used to define the framework $\mathcal{F}$ and the ranking function $\mathcal{R}(q_i, d_j)$. To provide a better understanding how a framework looks like, an example based on text documents is given. The document representation is a set of all terms within the document. To keep the collection smaller without loosing any information stop words should be removed in a preprocessing step. According our document representation the query representation is a set of all terms within the query. There can also be an additional step in the query creation, where for example synonyms are added to the query set.

After the design of the framework, a ranking function can be created. It should be constructed in a way that it fits at its best to the requirements of the user. This means for a given query the ranking function supply a numeric rank to each document in the collection which represent the relevance for the user. As a basic example, the ranking function could count how often query terms appear in a document. This rate is called term frequency. To be able to compare the documents using the term frequency, the ranks must be normalized.

%% maybe document frequency more detailed.

\subsection{The Boolean Model}
\label{sec:the_boolean_model}

General about vector boolean model.

\subsubsection{Ranked Boolean Retrieval}
\label{sec:ranked_boolean_retrieval}

Described in ~\cite{manning2008} page 103

\begin{equation}
  \sum_{i = 1}^{l}g_i s_i
\end{equation}

\subsection{The Vector Space Model}
\label{sec:the_vector_space_model}

General about vector space model... The classical vector space model was introduced by Salton et al.~\cite{salton75vsm}

\subsubsection{Term Frequency-Inverse Document Frequency Model}
\label{sec:tfidf}

% also Tf-idf Model in the same paper defined ~\cite{salton75vsm}
% concept of inverse document frequency was introduced by jones72astatistical

Describe basics like term frequency, document frequency...

\begin{equation}
  \begin{split}
    \text{idf}_t & = log \frac{N}{df_t} \\
    \text{tf-idf}_{t, d} & = \text{tf}_{t, d} \cdot \text{idf}_t \\
    \text{Score}(q, d) & = \sum_{t \in q}\text{tf-idf}_{t, d}
  \end{split}
\end{equation}

\subsection{The Probabilistic Model}
\label{sec:the_probabilistic_model}

General about probabilistic model.

\subsubsection{Okapi BM25}
\label{sec:okapi_bm25}
as described in ~\cite{manning2008} page 214 and in ~\cite{ModernInvormationRetrieval1999} page 105
\begin{equation}
  \begin{split}
    \text{idf}_t & = log \frac{N - \text{df}_t + 0.5}{\text{df}_t + 0.5} \\
    \text{bm25}_{t, d} & = \text{idf}_t \cdot \frac{\text{tf}_{t, d} \cdot (k_1 + 1)}{\text{tf}_{t, d} + k_1 \cdot \bigl(1 - b \cdot \frac{|G|}{\text{avgdl}}\bigr)}  \\
    \text{Score}(q, d) & = \sum_{t \in q}\text{bm25}_{t, d}
  \end{split}
\end{equation}


\subsubsection{Divergence from Randomness}
\label{sec:divergence_from_randomness}

as described in ~\cite{ModernInvormationRetrieval1999} page 113
\begin{equation}
  w_{i, j} = (- \log P(k_i | C)) \cdot (1 - P(k_i | d_j))
\end{equation}
\begin{equation}
  \text{Score}(d_j, q) = \sum_{k_i \in q} f_{i, q} \cdot w_{i, j}
\end{equation}
\begin{equation}
  F_i = \sum_j f_{i, j}
\end{equation}
\begin{equation}
  P(k_i | C) = \binom{F_i}{f_{i, j}}p^{f_{i, j}} \cdot (1 - p)^{F_i - f_{i, j}}
\end{equation}
\begin{equation}
  \lambda_i = p \cdot F_i
\end{equation}
\begin{equation}
  P(k_i | C) = \frac{e^{-\lambda_i}\lambda_i^{f_{i, j}}}{f_{i, j}!}
\end{equation}
\begin{equation}
  \begin{split}
    - \log P(k_i | C) & = -\log\Biggl(\frac{e^{-\lambda_i}\lambda_i^{f_{i, j}}}{f_{i, j}!}\Biggr) \\
    & \approx -f_{i,j} \log \lambda_i + \lambda_i \log e + log(f_{i,j}!) \\
    & \approx f_{i, j} \log \Bigl( \frac{f_{i, j}}{\lambda_i} \Bigr) + \Bigl( \lambda_i + \frac{1}{12 f_{i, j} + 1} - f_{i, j}\Bigr) \log e + \frac{1}{2} \log(2 \pi f_{i, j})
  \end{split}
\end{equation}
\begin{equation}
  1 - P(k_i | d_j) = \frac{1}{f_{i, j} + 1}
\end{equation}
\begin{equation}
  f^{\prime}_{i, j} = f_{i, j} \cdot \frac{avgdl}{len(d_j)}
\end{equation}


\section{Document Preprocessing}
\label{sec:document_preprocessing}

~\cite{ModernInvormationRetrieval1999} page 223

\subsection{Extract Document Structure}
\label{subsec:extract_document_structure}

Write about the underlying framework. Created within ~\cite{KlampflGJK14}

\subsection{Stop Words}
\label{subsec:stop_words}

What are stopwords, and describe common techniques. In ~\cite{Vijayarani2015} and references

\subsection{Stemming}
\label{subsec:stemming}

Describe stemming and common stemming techniques. In ~\cite{Vijayarani2015} and references

\section{Text Similarities}
\label{sec:text_similarities}

Describe Text Similarities and common techniques. ~\cite{ModernInvormationRetrieval1999} page 222

\section{IMRaD Structure}
\label{sec:imrad_structure}

general about IMRaD in scientific writing: ~\cite{robert1989}, chapter distribution analysis: ~\cite{bertin2013} important: ~\cite{Sollaci-The-2004}

\section{Evaluation of Ranking Algorithms}
\label{sec:evaluation_of_ranking_algorithms}

as described in ~\cite{manning2008} page 147 and in ~\cite{ModernInvormationRetrieval1999} page 140

\myfig{precision_recall}
      {width=0.50\textwidth}
      {Precision and Recall}
      {Precision and Recall}
      {fig:precision_recall}

\begin{equation}
  P = \frac{\text{\# relevant items retrieved}}{\text{\# retrieved items}} = \frac{TP}{TP + FP} = P(\text{relevant} | \text{retrieved})
\end{equation}

\myfig{map}
      {width=1.00\textwidth}
      {Example for the precision of a search result}
      {Example for the precision of a search result}
      {fig:map}

\begin{equation}
  \text{MAP}(Q) = \frac{1}{|Q|}\sum_{j = 1}^{|Q|} \frac{1}{m_j}\sum_{k = 1}^{m_j}\text{Precision}(R_{jk})
\end{equation}
