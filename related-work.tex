\chapter{Related Work}
\label{cha:related_work}

\section{Information Retrieval Models}
\label{sec:information_retrieval_models}

Creating an information retrieval system is a complex process that has to be planned accordingly. To reach this goal models are used as a base, where the whole system is sketched. The model generation consists of two tasks. First, design a framework which represents the documents and the user queries. Second, create a ranking function, which generates a numeric rank for each document based on a query. Afterwards these ranks are used by the system to sort the documents.

One of the most common retrieval approaches is retrieval based on index terms. In this context an index term is a keyword, which appears in the document collection of the framework. This approach can be implemented efficiently as query words can be used as index terms with limited transformations. For example a user is interested in cooking, and searches for "Austrian dishes". The query words "Austrian", and "dishes" can directly used to search through the document collection since they do not need any transformation.

In general, information retrieval models consists of four parts. Baeza-Yates and Baeza-Yates ~\cite{ModernInvormationRetrieval1999} define them as a quadruple $[\textbf{D}, \textbf{Q}, \mathcal{F}, \mathcal{R}(q_i, d_j)]$, where:

\begin{enumerate}
  \item \textbf{D} is a set composed of logical views of documents in a collection.
  \item \textbf{Q} is a set composed of logical views of the user information needs. Such representations are called queries.
  \item $\mathcal{F}$ is a framework for modeling document representations, queries, and their relationships.
  \item $\mathcal{R}(q_i, d_j)$ is a raking function that associates a real number with a query representation $q_i \in \textbf{Q}$ and a document representation $d_j \in \textbf{D}$. The ranking function generates an order over all documents \textbf{D} with respect to a query $q_i$.
\end{enumerate}

Hence, the model is used to define the framework $\mathcal{F}$ and the ranking function $\mathcal{R}(q_i, d_j)$. For example, for textual documents the document representation is a set of all terms within the document. To keep the collection smaller without losing any information stop words should be removed in a preprocessing step. The set of index terms within a document collection is called vocabulary. According to our document representation the query representation is a set of all terms within the query. There can also be an additional preprocessing step in the query creation. An example for such an preprocessing step would be synonyms which are added to the query set.

After the design of the framework, a ranking function is created. It should be constructed in a way that it fits to the requirements of the user. This means for a given query, the ranking function determines a numeric rank to each document in the collection, which represents the relevance for the user. For example, the ranking function counts how many query terms appear in the term set of a document.

Another example is to use term frequency as ranking function. Term frequency itself denotes how often a term occurs in a document. To be able to use it, the document representation is adapted from a set with all terms to a bag of words. In a bag of words each term is represented as a pair of term and term frequency. The ranking function sums the frequencies over all query terms. To be able to compare the documents using the term frequency, the ranks are normalized.

Information retrieval is used in several fields where the underlying models have to fulfill different requirements. Therefore, they are separated into text-based models, link-based models, and multimedia objects-based models Furthermore, text-based models can be categorized in unstructured, and semi-structured text models. Unstructured text models are used for text documents where the content is represented as sequence of words. Semi-structured text models contain structure such as title, sections, paragraphs, in addition to unstructured text.

The web is rapidly growing, and as a consequence has a huge number of web pages (i.e. documents). Therefore, additional information has to be leveraged as well.
This means that the content of documents, and furthermore the links between those documents are take into account. Models which use those additional link information are called Link-based models where PageRank ~\cite{brin1998anatomy} and Hyperlink-Induced Topic Search ~\cite{kleinberg1999authoritative} are important parts of the models.

Information retrieval for multimedia objects differs completely from the first $2$ types. For example when thinking on a image it can be seen as a matrix of color values. Detecting similarities between images means compare colors and shapes of them. This consideration continues when thinking of the query representation. The user can use words, or use a image to define a query. The simplest form of multimedia-based retrieval is image retrieval. Audio and video retrieval are more complicated since there is also a time value which have to be taken into account.

\section{Unstructured Text Retrieval}
\label{sec:unstructured_text_Retrieval}

In unstructured text retrieval the used documents can be seen as sequence of words. The $3$ classical models are called Boolean, vector, and probabilistic model. For the Boolean model documents and queries are represented as sets. Terms are stitched together with Boolean operators to formulize user queries. For the vector model documents and queries are represented as a vector in a t-dimensional space. The size of t is defined by the vocabulary of the collection. For the probabilistic models documents and queries are represented based on probability theory. Gudivada et al. ~\cite{gudivada1997} advice in their work to call Boolean models set theoretic, vector models algebraic, and probabilistic models probabilistic.

\subsection{The Boolean Model}
\label{sec:the_boolean_model}

\myfig{boolean_model}
      {width=0.7\textwidth}
      {\textbf{Example query in the Boolean model with $3$ terms.} For the Boolean model documents in the collection are represented as sets of terms. Furthermore they can be separated according to the terms ($t_1$, $t_2$, $t_3$) they are containing. Given a query $q=t_1 \wedge (t_2 \vee \neg t_3)$ all documents which satisfy this query are marked as relevant for the user.}
      {Example query in the Boolean model with $3$ terms.}
      {fig:boolean_model}

The Boolean model is a well-known information retrieval model in the field of unstructured text retrieval. It was proposed as a paradigm for accessing large scale systems since the $1950$s ~\cite{Melucci2009}. The model uses Boolean operators and set theory to find relevant documents in the document collection.

The classical Boolean model can only decide if a document is relevant for the user, or not. It does not provide a rank which is used to sort the documents. Salton et al. introduce in their work ~\cite{Salton-Extended-1983} a extension where the documents are sorted according their relevance.

Index terms are combined with the $3$ Boolean operators NOT($\neg$), AND($\wedge$), OR($\vee$) to formulize user queries. The disjunctive normal form of the query shows which areas of the sets are relevant. When considering an example query with $3$ terms $q=t_1 \wedge (t_2 \vee \neg t_3)$, where $t_1$, $t_2$, $t_3$ are the query terms, the disjunctive normal form looks as follows:
\begin{equation}
q_{DNF} = (t_1 \wedge t_2 \wedge t_3) \vee (t_1 \wedge t_2 \wedge \neg t_3) \vee (t_1 \wedge \neg t_2 \wedge \neg t_3)
\end{equation}
\Cref{fig:boolean_model} displays the relevant documents according their areas in the Venn diagram. Their it can be seen... % noch genauer welche areas, welche nicht...

\subsection{The Vector Space Model}
\label{sec:the_vector_space_model}

General about vector space model... The classical vector space model was introduced by Salton et al.~\cite{salton75vsm}

\subsubsection{Term Frequency-Inverse Document Frequency Model}
\label{sec:tfidf}

% also Tf-idf Model in the same paper defined ~\cite{salton75vsm}
% concept of inverse document frequency was introduced by jones72astatistical

Describe basics like term frequency, document frequency...

\begin{equation}
  \begin{split}
    \text{idf}_t & = log \frac{N}{df_t} \\
    \text{tf-idf}_{t, d} & = \text{tf}_{t, d} \cdot \text{idf}_t \\
    \text{Score}(q, d) & = \sum_{t \in q}\text{tf-idf}_{t, d}
  \end{split}
\end{equation}

\subsection{The Probabilistic Model}
\label{sec:the_probabilistic_model}

General about probabilistic model.

\subsubsection{Okapi BM25}
\label{sec:okapi_bm25}
as described in ~\cite{manning2008} page 214 and in ~\cite{ModernInvormationRetrieval1999} page 105
\begin{equation}
  \begin{split}
    \text{idf}_t & = log \frac{N - \text{df}_t + 0.5}{\text{df}_t + 0.5} \\
    \text{bm25}_{t, d} & = \text{idf}_t \cdot \frac{\text{tf}_{t, d} \cdot (k_1 + 1)}{\text{tf}_{t, d} + k_1 \cdot \bigl(1 - b \cdot \frac{|G|}{\text{avgdl}}\bigr)}  \\
    \text{Score}(q, d) & = \sum_{t \in q}\text{bm25}_{t, d}
  \end{split}
\end{equation}


\subsubsection{Divergence from Randomness}
\label{sec:divergence_from_randomness}

as described in ~\cite{ModernInvormationRetrieval1999} page 113
\begin{equation}
  w_{i, j} = (- \log P(k_i | C)) \cdot (1 - P(k_i | d_j))
\end{equation}
\begin{equation}
  \text{Score}(d_j, q) = \sum_{k_i \in q} f_{i, q} \cdot w_{i, j}
\end{equation}
\begin{equation}
  F_i = \sum_j f_{i, j}
\end{equation}
\begin{equation}
  P(k_i | C) = \binom{F_i}{f_{i, j}}p^{f_{i, j}} \cdot (1 - p)^{F_i - f_{i, j}}
\end{equation}
\begin{equation}
  \lambda_i = p \cdot F_i
\end{equation}
\begin{equation}
  P(k_i | C) = \frac{e^{-\lambda_i}\lambda_i^{f_{i, j}}}{f_{i, j}!}
\end{equation}
\begin{equation}
  \begin{split}
    - \log P(k_i | C) & = -\log\Biggl(\frac{e^{-\lambda_i}\lambda_i^{f_{i, j}}}{f_{i, j}!}\Biggr) \\
    & \approx -f_{i,j} \log \lambda_i + \lambda_i \log e + log(f_{i,j}!) \\
    & \approx f_{i, j} \log \Bigl( \frac{f_{i, j}}{\lambda_i} \Bigr) + \Bigl( \lambda_i + \frac{1}{12 f_{i, j} + 1} - f_{i, j}\Bigr) \log e + \frac{1}{2} \log(2 \pi f_{i, j})
  \end{split}
\end{equation}
\begin{equation}
  1 - P(k_i | d_j) = \frac{1}{f_{i, j} + 1}
\end{equation}
\begin{equation}
  f^{\prime}_{i, j} = f_{i, j} \cdot \frac{avgdl}{len(d_j)}
\end{equation}

\section{Structured Text Retrieval}
\label{sec:structured_text_Retrieval}

Write about structuring, early text retrieval, xml retrieval

\subsubsection{Ranked Boolean Retrieval}
\label{sec:ranked_boolean_retrieval}

Described in ~\cite{manning2008} page 103

\begin{equation}
  \sum_{i = 1}^{l}g_i s_i
\end{equation}

\section{Document Preprocessing}
\label{sec:document_preprocessing}

~\cite{ModernInvormationRetrieval1999} page 223

\subsection{Extract Document Structure}
\label{subsec:extract_document_structure}

Write about the underlying framework. Created within ~\cite{KlampflGJK14}

\subsection{Stop Words}
\label{subsec:stop_words}

What are stopwords, and describe common techniques. In ~\cite{Vijayarani2015} and references

\subsection{Stemming}
\label{subsec:stemming}

Describe stemming and common stemming techniques. In ~\cite{Vijayarani2015} and references

\section{Text Similarities}
\label{sec:text_similarities}

Describe Text Similarities and common techniques. ~\cite{ModernInvormationRetrieval1999} page 222

\section{IMRaD Structure}
\label{sec:imrad_structure}

general about IMRaD in scientific writing: ~\cite{robert1989}, chapter distribution analysis: ~\cite{bertin2013} important: ~\cite{Sollaci-The-2004}

\section{Evaluation of Ranking Algorithms}
\label{sec:evaluation_of_ranking_algorithms}

as described in ~\cite{manning2008} page 147 and in ~\cite{ModernInvormationRetrieval1999} page 140

\myfig{precision_recall}
      {width=0.50\textwidth}
      {Precision and Recall}
      {Precision and Recall}
      {fig:precision_recall}

\begin{equation}
  P = \frac{\text{\# relevant items retrieved}}{\text{\# retrieved items}} = \frac{TP}{TP + FP} = P(\text{relevant} | \text{retrieved})
\end{equation}

\myfig{map}
      {width=1.00\textwidth}
      {Example for the precision of a search result}
      {Example for the precision of a search result}
      {fig:map}

\begin{equation}
  \text{MAP}(Q) = \frac{1}{|Q|}\sum_{j = 1}^{|Q|} \frac{1}{m_j}\sum_{k = 1}^{m_j}\text{Precision}(R_{jk})
\end{equation}
