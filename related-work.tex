\chapter{Related Work}
\label{cha:related_work}

\section{Information Retrieval Models}
\label{sec:information_retrieval_models}

Creating an information retrieval system is a complex process that has to be planned accordingly. To reach this goal models are used as a base, where the whole system is sketched. The model generation consists of two tasks. First, design a framework which represents the documents and the user queries. Second, create a ranking function, which generates a numeric rank for each document based on a query. Afterwards these ranks are used by the system to sort the documents.

One of the most common retrieval approaches is retrieval based on index terms. In this context an index term is a keyword, which appears in the document collection of the framework. This approach can be implemented efficiently as query words can be used as index terms with limited transformations. For example a user is interested in cooking, and searches for "Austrian dishes". The query words "Austrian", and "dishes" can directly used to search through the document collection since they do not need any transformation.

In general, information retrieval models consists of four parts. Ribeiro-Neto and Baeza-Yates ~\cite{ModernInvormationRetrieval1999} define them as a quadruple $[\textbf{D}, \textbf{Q}, \mathcal{F}, \mathcal{R}(q_i, d_j)]$, where:

\begin{enumerate}
  \item \textbf{D} is a set composed of logical views of documents in a collection.
  \item \textbf{Q} is a set composed of logical views of the user information needs. Such representations are called queries.
  \item $\mathcal{F}$ is a framework for modeling document representations, queries, and their relationships.
  \item $\mathcal{R}(q_i, d_j)$ is a raking function that associates a real number with a query representation $q_i \in \textbf{Q}$ and a document representation $d_j \in \textbf{D}$. The ranking function generates an order over all documents \textbf{D} with respect to a query $q_i$.
\end{enumerate}

Hence, the model is used to define the framework $\mathcal{F}$ and the ranking function $\mathcal{R}(q_i, d_j)$. For example, for textual documents the document representation is a set of all terms within the document. To keep the collection smaller without losing any information stop words should be removed in a preprocessing step. The set of index terms within a document collection is called vocabulary. According to our document representation the query representation is a set of all terms within the query. There can also be an additional preprocessing step in the query creation. An example for such an preprocessing step would be synonyms which are added to the query set.

After the design of the framework, a ranking function is created. It should be constructed in a way that it fits to the requirements of the user. This means for a given query, the ranking function determines a numeric rank to each document in the collection, which represents the relevance for the user. For example, the ranking function counts how many query terms appear in the term set of a document.

Another example is to use term frequency as ranking function. Term frequency itself denotes how often a term occurs in a document. To be able to use it, the document representation is adapted from a set with all terms to a bag of words. In a bag of words each term is represented as a pair of term and term frequency. The ranking function sums the frequencies over all query terms. To be able to compare the documents using the term frequency, the ranks are normalized.

Information retrieval is used in several fields where the underlying models have to fulfill different requirements. Therefore, they are separated into text-based models, link-based models, and multimedia objects-based models. Furthermore, text-based models can be categorized in unstructured, and semi-structured text models. Unstructured text models are used for text documents where the content is represented as sequence of words. Semi-structured text models contain structure such as title, sections, paragraphs, in addition to unstructured text.

The web is rapidly growing, and as a consequence has a huge number of web pages (i.e. documents). Therefore, additional information has to be leveraged as well. This means that the content of documents, and furthermore the links between those documents are take into account. Models which use those additional link information are called Link-based models where PageRank ~\cite{brin1998anatomy} and Hyperlink-Induced Topic Search ~\cite{kleinberg1999authoritative} are important parts of the models.

Information retrieval for multimedia objects differs according their underlying data from the first $2$ types. For example, when thinking on a image it can be seen as a matrix of color values. Detecting similarities between images requires the calculation of more complex features, such as shapes. The representation of the query has to be adapted as well. The user can use words, or use images to define a query. One of the simplest forms of multimedia-based retrieval is image retrieval. Audio and video retrieval are more complicated since there is also a time value which have to be taken into account.

\section{Unstructured Text Retrieval}
\label{sec:unstructured_text_Retrieval}

In unstructured text retrieval, documents can be seen as sequence of words. The $3$ classical models are boolean-, vector-, and probabilistic model. First, in the boolean model, documents and queries are represented as sets. Terms are stitched together with boolean operators to formulate user queries. Second, in the vector model, documents and queries are represented as a vector in a t-dimensional space. The size of t is defined by the number of words in the vocabulary of the collection. Third, in the probabilistic model, documents and queries are represented based on probability theory. Specifically by estimating the probability of a term appearing in a relevant document. Gudivada et al. ~\cite{gudivada1997} advice in their work to denote boolean models as set theoretic, vector models as algebraic, and probabilistic models as probabilistic.

\subsection{The Boolean Model}
\label{sec:the_boolean_model}

The boolean model is a well-known information retrieval model in the area of unstructured text retrieval. It was proposed as a paradigm for accessing large-scale systems since the $1950$s ~\cite{Melucci2009}. The model uses boolean operators and set theory to find relevant documents.

\myfig{boolean_model}
      {width=0.65\textwidth}
      {\textbf{Example query in the boolean model with $3$ terms.} For the boolean model documents in the collection are represented as sets of terms. In this example the vocabulary of the document collection is given by V = \{$t_1$, $t_2$, $t_3$\}. Furthermore, documents can be separated according to the terms they are containing. Given a query $q=t_1 \wedge (t_2 \vee \neg t_3)$ all documents which satisfy this query are marked with an green hook. This means that they are relevant for the user. All other documents which does not satisfy the query are marked with a red cross.}
      {Example query in the boolean model with $3$ terms.}
      {fig:boolean_model}

The classic boolean model can only decide if a document is relevant for the user, or not. It does not provide a rank, which is used to sort the documents. Salton et al. ~\cite{Salton-Extended-1983} introduce in their work an extension where documents are sorted according their relevance.

Index terms are combined with the $3$ boolean operators NOT($\neg$), AND($\wedge$), OR($\vee$) to formulate user queries. The disjunctive normal form of the query shows which areas of the sets are relevant. For example, for query $q=t_1 \wedge (t_2 \vee \neg t_3)$, and vocabulary V = \{$t_1$, $t_2$, $t_3$\}, $q_{DNF}$ is:
\begin{equation}
q_{DNF} = (t_1 \wedge t_2 \wedge t_3) \vee (t_1 \wedge t_2 \wedge \neg t_3) \vee (t_1 \wedge \neg t_2 \wedge \neg t_3)
\end{equation}
This representation of the query highlight that $3$ areas are relevant for the user. First, all $3$ query terms occur. Second, the first and the second term occur, but not the third. Finally, the first term occurs, but not the second and the third. \Cref{fig:boolean_model} displays the example query represented in a Venn diagram, where the $3$ areas can be seen in a graphical representation.

The boolean model works also if not all terms of the vocabulary are part of the user query. Considering a vocabulary V = \{$t_1$, $t_2$, $t_3$, $t_4$\}, and the previous example query, the disjunctive normal form is:
\begin{equation}
  \begin{aligned}
    q_{DNF} = &(t_1 \wedge t_2 \wedge t_3 \wedge \neg t_4) \vee (t_1 \wedge t_2 \wedge t_3 \wedge t_4) \vee \\
              &(t_1 \wedge t_2 \wedge \neg t_3 \wedge \neg t_4) \vee (t_1 \wedge t_2 \wedge \neg t_3 \wedge t_4) \vee \\
              &(t_1 \wedge \neg t_2 \wedge \neg t_3 \wedge \neg t_4) \vee (t_1 \wedge \neg t_2 \wedge \neg t_3 \wedge t_4)
  \end{aligned}
\end{equation}
The last term (i.e., $t_4$), which is not part of the query, is also considered in the disjunctive normal form. It is added once as present, and once as absent to the other $3$ terms.

The main advantages of the boolean model are the clean formalism, and its simplicity ~\cite{ModernInvormationRetrieval1999}. These advantages comes with the usage of binary operators, and binary index term weighing. One of the main disadvantages is the exact matching of documents. This means that a document can only be relevant, or not relevant to the user, without any ranking. As a result, users receive too few or too many documents.

\subsection{The Vector Space Model}
\label{sec:the_vector_space_model}

The vector space model was introduced by Salton et al.~\cite{salton75vsm}. In the model documents and queries are represented as vectors in an t-dimensional space, whereas t is the number of words in the vocabulary.

The vector space model is more advanced than the Boolean model, as it contains partial matching. Partial matching means, that a degree of similarity between user queries and documents in the system are calculated. To accomplish this, non-binary weights are used in combination with index terms.

The idea of non-binary weights is based on the assumption that some index terms are more important than others to describe the content of a document. The calculation of such term weights is a challenging task, as they have to reflect the subjective expectation of a user. As a result, terms that appear in a few documents have a higher weight than terms that occur in many documents. 

For example, a collection consists of $3$ documents, given by $D_1 =$ \{"cooking", "appetizer"\}, $D_2 =$ \{"cooking", "main", "dish"\}, and $D_3 =$ \{"cooking", "dessert"\}. A user is interested to prepare a dessert. Therefore he searches for "cooking dessert". The first query word "cooking" is part of each document. This means it is not very useful to define the users requirements. The second query word "dessert" is only part of one document. Therefore, it has more expressiveness than the first term. As a result, the weight of the first term will be smaller than the weight of the second term.

The combination of index terms and weights allows to calculate a numeric rank for each document. These ranks are used to sort the documents ranked by a user query. The resulting list contains the best results on top according to their relevance.

When having a closer look on index terms and their correlations it can be assumed that they are mutually independent. This means knowing $w_{i, j}$, where $w_{i, j}$ is the weight of an index term $t_i$ in a document $d_j$, tells nothing about the next weight $w_{i + 1, j}$. The assumption does not hold, as terms in a document are always related to each other. For example, the words \textit{mobile} and \textit{phone}, which often occur together. Therefore, when a document contains the word \textit{mobile} it is probable that the document also contains the word \textit{phone}. This correlation is reflected in their term weights. The term-term correlation matrix described by Ribeiro-Neto and Baeza-Yates ~\cite{ModernInvormationRetrieval1999} model such relations. It is defined as:
\begin{align}
  \textbf{C} = \textbf{M} \cdot \textbf{M}^T,
\end{align}
where \textbf{M} is a term-document matrix with t rows, and N columns. Each row contains a term of the vocabulary. As a result, the number of rows is equal to the size of the vocabulary. The columns represent the documents collection, where each column contains a single document. Each entry in the matrix \textbf{M} is given a weight $w_{i, j}$ associated with index term $t_i$ and document $d_j$. In the term-term correlation matrix \textbf{C} each element $c_{u, v} \in \textbf{C}$ describes the correlation between the terms $t_u$ and $t_v$, which is given by:
\begin{align}
  c_{u, v} = \sum_{d_j}w_{u, j} \times w_{v, j}.
\end{align}
Therefore the relation between any two terms $t_u$ and $t_v$ is in the matrix. It is based on the joint co-occurrence of the two terms within all documents of the collection. For example, when having a collection of $2$ documents, and the vocabulary of the collection is given by V = \{$t_1$, $t_2$, $t_3$\}. The term-term correlation matrix is calculated as follows:
\begin{samepage}
\[
  \begin{blockarray}{ccc}
    & d_1 & d_2 \\
    \begin{block}{l[cc]}
      t_1 & w_{1, 1} & w_{1, 2} \\
      t_2 & w_{2, 1} & w_{2, 2} \\
      t_3 & w_{3, 1} & w_{3, 2} \\
    \end{block}
  \end{blockarray}
  \hspace{40mm}
  \begin{blockarray}{cccc}
    & t_1 & t_2 & t_3 \\
    \begin{block}{l[ccc]}
      d_1 & w_{1, 1} & w_{2, 1} & w_{3, 1} \\
      d_2 & w_{1, 2} & w_{2, 2} & w_{3, 2} \\
    \end{block}
  \end{blockarray}
    \vspace*{-7mm}
\]
\[
  \hspace{8mm}
  \underbrace{\hspace{2mm}\textbf{M}\hspace{35mm}\times\hspace{35mm}\textbf{M}^{T}}_{\Downarrow} \\
\]
\[
  \begin{blockarray}{cccc}
    & t_1 & t_2 & t_3 \\
    \begin{block}{l[ccc]}
      t_1 & w_{1, 1}w_{1, 1} + w_{1, 2}w_{1, 2} & w_{1, 1}w_{2, 1} + w_{1, 2}w_{2, 2} & w_{1, 1}w_{3, 1} + w_{1, 2}w_{3, 2} \\
      t_2 & w_{2, 1}w_{1, 1} + w_{2, 2}w_{1, 2} & w_{2, 1}w_{2, 1} + w_{2, 2}w_{2, 2} & w_{2, 1}w_{3, 1} + w_{2, 2}w_{3, 2} \\
      t_3 & w_{3, 1}w_{1, 1} + w_{3, 2}w_{1, 2} & w_{3, 1}w_{2, 1} + w_{3, 2}w_{2, 2} & w_{3, 1}w_{3, 1} + w_{3, 2}w_{3, 2} \\
    \end{block}
  \end{blockarray}
\]
\end{samepage}
It can be seen that the term-document matrix \textbf{M} has size $3$ x $2$, where rows consists of the terms, and columns consists of the documents. For example, the weight in the first row, and in the first column $w_{1, 1}$ contains the weight of $t_1$ in document $d_1$. The transposed matrix consists of the terms in the columns, and the documents in the rows. By multiplying these $2$ matrices the term-term correlation matrix \textbf{C} is generated. It has size $3$ x $3$, where rows and columns consists of the terms in the vocabulary. For example, the entry in the first row, and in the second column contains the joint co-occurrence of the terms $t_1$ and $t_2$.

The vector space model is just one information retrieval model which takes advantage of term-term correlation. Other models are the set-based model, fuzzy information retrieval models, and language models.

In the vector space model documents, and queries are represented as vectors in an $t$-dimensional space. The size of $t$ is defined by the size of the vocabulary. As a result, each index term represent one dimension in the vector. Ribeiro-Neto and Baeza-Yates ~\cite{ModernInvormationRetrieval1999} define the document representation $d_j$, and the query representation $q$ as:
\begin{align}
  \vec{d_j} & = (w_{1, j}, w_{2, j}, \dots, w_{t, j}) \\
  \vec{q} & = (w_{1, q}, w_{2, q}, \dots, w_{t, q}).
\end{align}
In both representations the vector consists of term weights which describes their content. Each document in the collection is represented by a document vector $\vec{d_j}$. Thereby $w_{i,j}$ is defined as the weight of term $t_i$ in document $d_j$. It has to be non-negative, and non-binary. The term weights of the query $w_{i, q}$ consists the weight of term $t_i$, which occurs in query $q$. It has to be non-negative.

\myfig{diff_vectors}
      {width=0.5\textwidth}
      {\textbf{Example of the similarity between query and documents in the vector space model.} In this example, the vocabulary of the document collection is given by \mbox{V = \{"\textit{appetizer}", "\textit{dessert}"\}}. The $2$ documents $d_1$, $d_2$, and the query $q$ are represented as vectors according their term weights. The similarity of $2$ vectors is given by the cosine of the angle between them.}
      {Example of the similarity between query and documents in the vector space model.}
      {fig:diff_vectors}

The degree of similarity between a document $d_j$, and a query $q$ is calculated by the cosine of the angle between their corresponding vectors (see \Cref{fig:diff_vectors}). Specifically,
\begin{equation}
  \label{vector_space_similarity}
  \begin{split}
    sim(d_j, q) & = \frac{\vec{d_j} \bullet \vec{q}}{|\vec{d_j}| \times |\vec{q}|} \\
    & = \frac{\sum_{i = 1}^t w_{i, j} \times w_{i, q}}{\sqrt{\sum_{i = 1}^t w_{i, j}^2} \times \sqrt{\sum_{i = 1}^t w_{i, q}^2}},
  \end{split}
\end{equation}
where $\vec{d_j} \bullet \vec{q}$ is the internal product of the $2$ vectors. The factor $|\vec{d_j}|$ is the norm of the document vector, and $|\vec{q}|$ is the norm of query vector. These norms define the document length, and the query length. The norm of the query vector does not affect the ranking result since it is the same for all documents in the collection. Singhal et. al ~\cite{SinghalBM96} discuss in their work more advanced document length normalization for vector space models.

\subsubsection{TF-IDF Weighting Scheme}
\label{sec:tfidf}

Term weighting was first discussed by Luhn ~\cite{Luhn_statistical-1957}. The Author observed that terms that occurs more often in a document are important to describe the content of the document. Therefore, these terms can be seen as keywords. As a result, he assumed that the term frequency $f_{i, j}$, where term $t_i$ occurs in document $d_j$, is relative to the term frequency weight $TF_{i, j}$. Hence, a high term frequency leads to a high term frequency weight. This assumption leads to the following formulation of term frequency weights
\begin{align}
  \label{raw_tf}
  \mathit{tf}_{i, j} = f_{i, j}.
\end{align}
In this formula the raw term frequency is used as term weight. However, Salton and Yang ~\cite{FT023} observed in their work that in some cases term frequency weights are an improvement according binary weights. Furthermore, they state inconsistence in their test results when they changed their test collection, and query set.

To improve the results of term frequency weighting, inverse document frequency weights are used additionally. The concept of inverse document frequency was introduced by Spärck Jones ~\cite{jones72astatistical}, and is one of the foundations of term weighting. Therefore, it is used in every modern information retrieval system. The inverse document frequency weight is approximated using Zipf's Law ~\cite{zipf1932selected}
\begin{align}
  \label{idf}
  \mathit{IDF}_i = \log \frac{N}{n_i}.
\end{align}
It is called inverse document frequency as $n_i/N$ is the relative document frequency. Therefore, is $n_i$ the number of documents, where a term $k_i$ occurs in the document collection, and $N$ is the size of the document collection. The inverse document frequency represents the importance of a term regarding the whole document collection. It is small if a term occurs in almost every document, and it is high if the term appears just in a few documents.

\begin{table}[b!]
    \centering
    \begin{tabular}{ l c }
      \toprule
      \textbf{Weighting scheme} & \textbf{TF Weight} \\ \midrule
      \textit{Binary}  & $\{0, 1\}$  \\
      \textit{Raw Frequency} & $f_{i, j}$  \\
      \textit{Log Normalization} & $1 + \log f_{i, j}$  \\
      \textit{Double Normalization $0.5$} & $0.5 + 0.5 \frac{f_{i, j}}{max f_{i, j}}$  \\
      \textit{Double Normalization K} & $K + (1 - K)\frac{f_{i, j}}{max_i f_{i, j}}$  \\
      \bottomrule
    \end{tabular}
  \caption[Variants of TF weight]{\textbf{Variants of TF weight.} There exist $5$ important variants of term frequency weighting. First, \textit{Binary} weight is the simplest form, and only captures if a term occurs in a document. Second, \textit{Raw Frequency} uses the term frequency directly, and can be seen as base for term frequency weighting. Third, \textit{Log Normalization} is an extension of \textit{Raw Frequency}, and uses the logarithm of the (raw) frequency. Forth, \textit{Double Normalization $0.5$} rescales the weights to be in the range between $0.5$ and $1.0$. Therefore, the weight is always normalized. Fifth, the \textit{Double Normalization K} is a generalization of the \textit{Double Normalization $0.5$}, where K can take values in the range between $0.0$ and $1.0$.}
  \label{tbl:tf_variants}
\end{table}

To use term frequency weighting in combination with inverse document frequency weighting the raw term frequency weight (see \Cref{raw_tf}) has to be adopted to the logarithmic term frequency weight:
\begin{equation}
  \mathit{tf}_{i, j} =
  \begin{cases}
    1 + \log f_{i, j}, & \text{if $f_{i,j} > 0$} \\
    0, & \text{otherwise}.
  \end{cases}
\end{equation}
The log term frequency weight is one of the most frequently used term frequency weighing schemes as the inverse document frequency term weight is also a logarithmic function. Therefore, they can be combined directly as defined by Salton and Yang ~\cite{FT023} to the TF-IDF weighing scheme:
\begin{equation}
  w_{i, j} =
  \begin{cases}
    1 + \log f_{i, j} \times \log \frac{N}{n_i}, & \text{if $f_{i,j} > 0$} \\
    0, & \text{otherwise},
  \end{cases}
\end{equation}
where $w_{i, j}$ is the term weight of term $k_i$, which occurs in document $d_j$. On the one hand, this includes the term frequency, which represents the importance of the term within the document. On the other hand, it is composed of the inverse document frequency, which represents the importance of the term within the whole document collection. The combination of these two weights leads to an effective term weighting scheme. Therefore, it is the base for term weighting that is used in almost every modern information retrieval system.

As the TF-IDF weighting scheme is one of the most popular weighting schemes in information retrieval several variants where proposed over time. Ribeiro-Neto and Baeza-Yates ~\cite{ModernInvormationRetrieval1999} describe in their work the most important forms of term frequency weighting (see \Cref{tbl:tf_variants}), and inverse document weighting (see \Cref{tbl:idf_variants}). Furthermore, they discuss the best combinations of them to compile different TF-IDF weighting schemes for document term weighting, as well as query term weighting. 

\begin{table}[b!]
    \centering
    \begin{tabular}{ l c }
      \toprule
      \textbf{Weighting scheme} & \textbf{IDF Weight} \\ \midrule
      \textit{Unary}  & $1$  \\
      \textit{Inverse Frequency} & $\log \frac{N}{n_i}$  \\
      \textit{Inverse Frequency Smooth} & $\log (1 + \frac{N}{n_i})$  \\
      \textit{Inverse Frequency Max} & $\log (1 + \frac{max_i n_i}{n_i})$  \\
      \textit{Probabilistic Inverse Frequency} & $\log (\frac{N - n_i}{n_i})$  \\
      \bottomrule
    \end{tabular}
  \caption[Variants of inverse document frequency weight]{\textbf{Variants of inverse document frequency weight.} There exist $5$ important variants of inverse document frequency weight. First, the \textit{Unary} form is used to ignore the inverse document frequency. Second, the \textit{Inverse Frequency} is the standard variant as it was initial introduced. Third, the \textit{Inverse Frequency Smooth} adds $1$ to the fracture result. This produces a more representative weight for extreme values of $n_i$. Fourth, the \textit{Inverse Frequency Max} uses the term with the largest document frequency instead of the number of documents in the collection. Therefore the computed weights are relative to the term with the highest document frequency. Fifth, the \textit{Probabilistic Inverse Frequency} subtract the document frequency from the number of documents in the collection. It is the basic form for the probabilistic model, as described in the next section}
  \label{tbl:idf_variants}
\end{table}

For example, given a document $d_1$ = \{"\textit{apple}", "\textit{apple}", "\textit{apple}", "\textit{pear}", "\textit{pear}"\}. When searching for "\textit{pear}", the individual variants of term frequency weighting (see \Cref{tbl:tf_variants}) yield different results. First, \textit{Binary} weight captures if the term occurs in the document. Therefore, the weight of $d_1$ is $1$. Second, \textit{Raw Frequency} denotes how often a term occurs in a document. As a result, the weight is $2$. Third, \textit{Log Normalization} uses $1$ for each query term in the document, and adds the logarithmic \textit{Raw Frequency} of the term. Hence, the weight is approximately $1.7$. Fourth, \textit{Double Normalization $0.5$} leverages the constant factor $0.5$, the \textit{Raw Frequency} of a term, and the maximum frequency over all terms to normalize weights in the range between $0.5$ and $1.0$. The result of the document weight is $0.8\overline{3}$. Fifth, \textit{Double Normalization K} is a generalization of the \textit{Double Normalization $0.5$}. For example, if K is $0.1$, the weighting value lies between $0.1$ and $1$.

To illustrate inverse term frequency, in addition to to $d_1$, the document collection also contains $d_2$ = \{"\textit{apple}", "\textit{peach}"\}. When searching for "\textit{apple}" all documents of the collection contain the term. Therefore, the term is not useful to differentiate documents of the collection. As a result, the inverse document frequency weight is the minimum possible outcome for each weighting scheme (see \Cref{tbl:idf_variants}). When searching for "\textit{pear}" $1$ of $2$ documents contain the term. Hence, the individual variants of inverse document frequency weighting lead to different results. First, the \textit{Unary} variant is always $1$. Second, the \textit{Inverse Frequency} variant denotes $N$ as the number of all documents in the collection. Additionally, $n_i$ is the document frequency of the query term. The logarithm of these to values leads approximately to $0.7$. Third, \textit{Inverse Frequency Smooth} also uses $N$ and $n_i$. In contrast to \textit{Inverse Frequency}, 1 is added inside the logarithm, which results of a weight of approximately $1.1$. Fourth, the \textit{Inverse Frequency Max} is similar to the \textit{Inverse Frequency Smooth}, however uses the term with the largest document frequency instead of $N$. In our example that is "\textit{apple}", which occurs in every document. Therefore, the result is also approximately $1.1$. Fifth, the \textit{Probabilistic Inverse Frequency} is similar to the \textit{Inverse Frequency}, however subtracted $N$ by $n_i$ inside the logarithm. This leads to an inverse term frequency weight of $0.0$.

\begin{table}[b]
    \centering
    \begin{tabular}{ c c }
      \toprule
      \textbf{Document term weight} & \textbf{Query term weight} \\ \midrule
      $f_{i, j}$  & $(0.5 + 0.5 \frac{f_{i, q}}{max_i f_{i, q}}) \times \log \frac{N}{n_i}$  \\
      $1 + \log f_{i, j}$ & $\log (1 + \frac{N}{n_i})$  \\
      $(1 + \log f_{i, j} \times \log \frac{N}{n_i})$ & $(1 + \log f_{i, q}) \times \log \frac{N}{n_i}$  \\
      \bottomrule
    \end{tabular}
  \caption[Recommended variants of the TF-IDF weighting scheme]{\textbf{Recommended variants of the TF-IDF weighting scheme.} There exist $3$ recommended variants of the TF-IDF weighting scheme. Given different document collections the best performing form could vary. They combine term frequency weighing variants with inverse frequency weighing variants. Additionally, they distinguish between document term weights, and query term weights.}
  \label{tbl:recommended_tfidf_weights}
\end{table}
There exist several forms of TF-IDF weighting schemes. The best performing variant could vary given different document collections. In \Cref{tbl:recommended_tfidf_weights} are $3$ recommended combinations, which were proposed by Salton ~\cite{salton1971}. The author distinguishes between document term weights, and query term weights. 
\begin{enumerate} 
	\item The first variant combines the \textit{Raw Frequency} of the term frequency weighing variants in \Cref{tbl:tf_variants}, and the \textit{Inverse Frequency} of the inverse frequency weighing variants in \Cref{tbl:idf_variants} for the document term weight. The associated query term weight uses \textit{Double Normalization $0.5$} and also \textit{Inverse Frequency}.
	\item The second variant differs, as document terms consist only of a term frequency weight, and query terms consists only of a inverse frequency weight. It uses the \textit{Log Normalization} as term frequency weight, and the \textit{Inverse Frequency Smooth} as inverse frequency weight.
  \item The third variant is the initial variant, as it was defined by Salton and Yang ~\cite{FT023}. It combines the \textit{Log Normalization}, and the \textit{Inverse Frequency} for document-, and query terms.
\end{enumerate}

The third variant is the most common used form in the vector space model. Ribeiro-Neto and Baeza-Yates ~\cite{ModernInvormationRetrieval1999} describe in their work an extension for this variant to retrieve better results. There the document term weight, and the query term weight should only be used if the term frequency is greater than $0$, otherwise the corresponding weight is $0$. In the web it is common that the query term frequency is $1$. Therefore, it should be reduced to the inverse document frequency. If this is done, the similarity (see \Cref{vector_space_similarity}) captures $TF \times IDF^2$. To bring it back to the $TF \times IDF$ form, the document term weight is reduced to the term frequency weight $TF$. The second variant is based on the same basic principle.

The TF-IDF weighting scheme can also be used as a ranking function with limited knowledge about vectors. Manning et. al ~\cite{manning2008} describe in their work such a TF-IDF ranking function:
\begin{align}
  \begin{split}
    \text{Score}(q, d) & = \sum_{i \in q} \text{tf-idf}_{i, d} \\
    \text{tf-idf}_{i, d} & = f_{i, q} \times \log \frac{N}{n_i},
  \end{split}
\end{align}
where the query q, and a document d are passed into the ranking function. The query and the document can be seen as sets of terms. To calculate the rank, the TF-IDF weights are summed up. For every query term that does not occur in the document the weight is $0$. For the calculation the \textit{Raw Frequency} term frequency weighing, and the \textit{Inverse Frequency} inverse frequency weighing are used (see \Cref{tbl:tf_variants}, and \Cref{tbl:idf_variants}).

The TF-IDF weighting scheme is one of the most popular weighting schemes in information retrieval as it is leveraged, adapted and optimized for a wide array of different use-cases.

\subsection{The Probabilistic Model}
\label{sec:the_probabilistic_model}

The first probabilistic model was introduced by Robertson and Sparck Jones ~\cite{Robertson1976}. In their work they defined the Probabilistic Relevance Model as framework for future models. Given a user query the task is to find the set of documents that contain all relevant documents. This set of documents is called the ideal answer set. 

Properties are defined to receive the ideal answer set. The challenge is to define these properties with index terms. An additional challenge is, that the properties of the ideal answer set are not known at query time. Therefore, they are estimated to generate the initial answer set.

Afterwards, the answer set is improved by user interaction. For example, the user has to choose which documents of the answer set are relevant. With this user decisions the known properties are adapted, and a new answer set is generated. With each iteration of this process, the resulting answer set converges to the ideal answer set. 

Robertson ~\cite{robertson-the-1997} proposed is his work the probability ranking principle, which denotes that documents that are relevant for the user can be influenced by properties outside the system. Therefore, the ideal answer set generated by the system is not necessarily the ideal answer set for the user.

In the probabilistic model documents are represented as vectors in a $t$-dimensional space:
\begin{equation}
  \vec{d_j} = (w_{1, j}, w_{2, j}, \dots, w_{t, j}),
\end{equation}
where the size of $t$ is the size of the vocabulary. As a result, each index term represents one dimension in the vector. Furthermore, the weights of the vector are binary. Hence, $w_{i, j} = 1$ if term $k_i$ occurs in document $d_j$, and $w_{i, j} = 0$ otherwise. The query $q$ is a subset of index terms.

Using the document representation $d_j$, and the query representation $q$, the similarity can be calculated as follows:
\begin{equation}
  sim(d_{j}, q) = \frac{P(R|\vec{d_j}, q)}{P(\overline{R}|\vec{d_j}, q)},
\end{equation}
where $R$ is a set of documents that is initially estimated, or known to be relevant for the user given a query $q$. $\overline{R}$ describes the complement of $R$ (i.e., the set of documents that are not relevant for the user). Furthermore, $P(R|\vec{d_j}, q)$ defines the probability that a document $d_j$ is relevant with respect to query $q$, and $P(\overline{R}|\vec{d_j}, q)$ defines the probability that the document is not relevant given this query. The equation can be approximated, as denoted by Ribeiro-Neto and Baeza-Yates ~\cite{ModernInvormationRetrieval1999}, using:
\begin{equation}
  \label{similarity_porbabilistic_approx}
  sim(d_{j}, q) \sim \sum_{k_i \in q \wedge k_i \in d_j} \log \left(\frac{P(k_i|R, q)}{1 - P(k_i|R, q)}\right) + \log \left(\frac{1 - P(k_i|\overline{R}, q)}{P(k_i|\overline{R}, q)}\right),
\end{equation}
where $P(k_i|R, q)$ is the probability that index term $k_i$ is present in a document selected randomly from the set of relevant documents $R$. Furthermore, $P(k_i|\overline{R}, q)$ is the probability that index term $k_i$ is not present in a document selected randomly from $R$. \Cref{similarity_porbabilistic_approx} can be seen as foundation for ranking in probabilistic models.

The set of relevant documents $R$ is not known initially. Therefore, an additional method is necessary to calculate $P(k_i|R, q)$, and $P(k_i|\overline{R}, q)$ for the initial answer set. One possible solution is a contingency table as proposed by Robertson and Sparck Jones ~\cite{Robertson1976} (see \Cref{tbl:contingency_table_probabilistic_ranking}).

\begin{table}[b]
    \centering
    \begin{tabular}{ c c c c}
      \toprule
      \textbf{Case} & \textbf{Relevant} & \textbf{Non-relevant} & \textbf{Total} \\ \midrule
      Documents containing $k_i$ & $r_i$ & $n_i - r_i$ & $n_i$ \\
      Documents not containing $k_i$ & $R - r_i$ & $N - n_i - (R - r_i)$ & $N - n_i$ \\
      All documents & $R$ & $N - R$ & $N$ \\
      \bottomrule
    \end{tabular}
  \caption[Contingency table for probabilistic ranking]{\textbf{Contingency table for probabilistic ranking.} The contingency table represents the relationships between documents and relevance. $N$ is the number of all documents in the collection, $n_i$ defines the number of documents that contain an index term $k_i$. Furthermore, $R$ is the number of documents that are relevant for a user with respect to a query $q$, $r_i$ defines the number of relevant documents that contain an index term $k_i$.}
  \label{tbl:contingency_table_probabilistic_ranking}
\end{table}

With the contingency table $P(k_i|R, q)$, and $P(k_i|\overline{R}, q)$ can be defined as:
\begin{align}
  P(k_i|R, q) & = \frac{r_i}{R} \\
  P(k_i|\overline{R}, q) & = \frac{n_i - r_i}{N - R},
\end{align}
where the first equation captures the relevant documents that contain term $k_i$ in relation to all relevant documents. The second equation captures the non-relevant documents that contain term $k_i$ in relation to all relevant documents. By combining these two equations with \Cref{similarity_porbabilistic_approx} the similarity can be computed:
\begin{equation}
  sim(d_{j}, q) \sim \sum_{k_i \in q \wedge k_i \in d_j} \log \left(\frac{r_i(N - n_i - R + r_i)}{(R - r_i)(n_i - r_i)}\right).
\end{equation}
To improve numerical stability of the equation, a constant of $0.5$ is added ro $r_i$:
\begin{equation}
  sim(d_{j}, q) \sim \sum_{k_i \in q \wedge k_i \in d_j} \log \left(\frac{(r_i + 0.5)(N - n_i - R + r_i + 0.5)}{(R - r_i + 0.5)(n_i - r_i + 0.5)}\right).
\end{equation}
This is the classic probabilistic ranking equation, called the Robertson-Sparck Jones equation. However, it is still necessary to know the relevant documents, as $R$ and $r_i$ are part of the equation. One solution is to set $R$, and $r_i$ to be $0$:
\begin{equation}
  \label{similarity_porbabilistic_no_relevant}
  sim(d_{j}, q) \sim \sum_{k_i \in q \wedge k_i \in d_j} \log \left(\frac{N - n_i + 0.5}{n_i + 0.5}\right).
\end{equation}
This equation is used to calculate the initial set of relevant documents. Robertson ~\cite{robertson04idf} proposed is his work that the equation has negative terms if $n_i > N/2$. Therefore, he defined an alternative form by removing the subtraction of $n_i$ in the numerator:
\begin{equation}
  sim(d_{j}, q) \sim \sum_{k_i \in q \wedge k_i \in d_j} \log \left(\frac{N + 0.5}{n_i + 0.5}\right).
\end{equation}
As a result, the term of the sum is zero if the index term occurs in each document. Furthermore, it can be determined that the calculation for one term is similar to the calculation of the inverse document frequency (see \Cref{idf}).

In theory, the probabilistic model is optimal, as documents are ordered by the probability of being relevant for the user. In practice the system is affected by external factors. Therefore, finding the ideal answer set is only theoretically possible. An additional disadvantages is that the initial relevant-, and not-relevant sets have to be estimated. Furthermore, term frequency is not used, as document weights are binary. More advanced extensions of the probabilistic model tackle some of the outlined issues. For example, BM$25$, which will be discussed in the text.

\subsubsection{Okapi BM25}
\label{sec:okapi_bm25}

Okapi BM$25$ is the result of several experiments by Robertson et. al ~\cite{RobertsonWHGL92, RobertsonWJHG93, RobertsonWJHG94}. The idea is to transfer the strengths of the vector space model to the probabilistic model. These advantages are inverse document frequency, term frequency, and document length normalization. As foundation the classic probabilistic ranking equation (see \Cref{similarity_porbabilistic_no_relevant}) is used. The equation already covers the usage of the inverse document frequency, and is known as BM$1$.

To bring the term frequency into the probabilistic model the following equation was designed:
\begin{equation}
  \label{bm25_term_frequency_factor}
  \mathcal{F}_{i, j} = S_1 \times \frac{f_{i,j}}{K_1 + f_{i, j}},
\end{equation}
where $f_{i, j}$ is the term frequency of term $k_i$ that occurs in document $d_j$. Furthermore, $K_1$ is a constant that is determined based on the document collection. When $K_1$ is set to $0$ the factor becomes $1$, and the term frequency is not used for ranking. Finally, $S_1$ is used as a scaling factor.   

The next step was to bring document length normalization into the probabilistic model. To that end, the above equation was adjusted:
\begin{equation}
  \label{bm25_term_frequency_factor_extension}
  \mathcal{F}'_{i, j} = S_1 \times \frac{f_{i,j}}{\frac{K_1 \times len(d_j)}{avg\_doclen} + f_{i, j}},
\end{equation} 
where $len(d_j)$ is the number of terms in the document, and $avg\_doclen$ is the average document length over all documents in the collection. Furthermore, a correction factor for the document length, and the query length is introduced:
\begin{equation}
  \mathcal{G}_{i, q} = K_2 \times len(q) \times \frac{avg\_doclen - len(d_j)}{avg\_doclen + len(d_j)},
\end{equation}
where $len(q)$ is the number of terms in the query, and $K_2$ is a constant. To introduce term frequency into queries, an additional equation was designed based on \Cref{bm25_term_frequency_factor}:
\begin{equation}
  \mathcal{F}_{i, q} = S_3 \times \frac{f_{i,q}}{K_3 + f_{i, q}},
\end{equation}
where $f_{i,q}$ is the term frequency of term $k_i$ that occurs in query $q$. $K_3$ is an additional constant, and is determined based on queries. $S_3$ is a scaling factor for the fraction term.

With these $4$ equations BM$15$, and BM$11$ can be defined:
\begin{align}
  sim_{\text{BM$15$}}(d_j, q) & \sim \mathcal{G}_{i, q} + \sum_{k_i \in q \wedge k_i \in d_j} \mathcal{F}_{i, j}  \times \mathcal{F}_{i, q} \times \log \left(\frac{N - n_i + 0.5}{n_i + 0.5}\right) \\
  sim_{\text{BM$11$}}(d_j, q) & \sim \mathcal{G}_{i, q} + \sum_{k_i \in q \wedge k_i \in d_j} \mathcal{F}'_{i, j} \times \mathcal{F}_{i, q} \times \log \left(\frac{N - n_i + 0.5}{n_i + 0.5}\right).
\end{align}
BM$15$ uses $\mathcal{F}_{i, j}$ without the extension of document length normalization, as introduced in BM$11$. Robertson and Walker ~\cite{RobertsonW94} observed in their work that the best value for $K_2 = 0$. As a result, the correction factor $\mathcal{G}_{i, q}$ is eliminated. Furthermore, they suggest $S_1 = (K_1 + 1)$, and $S_3 = (K_3 + 1)$, where  the evaluation results increased between $12$ and $37$ percent (depending on the query length) if $K_3$ was chosen large (e.g., $100$ as in their experiments). Finally, they observed that the query term frequency factor $\mathcal{F}_{i, q}$ can be reduced to $f_{i, q}$, and for small queries set to be $1$. Adding these findings to the BM$15$-, and BM$11$-equation they can be simplified:
\begin{align}
  sim_{\text{BM$15$}}(d_j, q) & \sim \sum_{k_i \in q \wedge k_i \in d_j} \frac{(K_1 + 1) f_{i, j}}{(K_1 + f_{i, j})} \times \log \left(\frac{N - n_i + 0.5}{n_i + 0.5}\right) \\
  sim_{\text{BM$11$}}(d_j, q) & \sim \sum_{k_i \in q \wedge k_i \in d_j} \frac{(K_1 + 1) f_{i, j}}{\frac{K_1 \times len(d_j)}{avg\_doclen} + f_{i, j}} \times \log \left(\frac{N - n_i + 0.5}{n_i + 0.5}\right).
\end{align}
In these equations the correction factor, and the query term frequency factor are removed. Furthermore, $S_1$ is set to be $K_1 + 1$. As the document length normalization is only part of BM$11$ it outperforms the BM$15$.

BM$25$ was designed as a combination of BM$15$, and BM$11$. As a result, their term frequency factors (see \Cref{bm25_term_frequency_factor}, and \Cref{bm25_term_frequency_factor_extension}) was merged to generate the term frequency factor of BM$25$:
\begin{align}
  \mathcal{B}_{i, j} = \frac{(K_1 + 1) f_{i, j}}{K_1 \left[(1 - b) + b \frac{len(d_j)}{avg\_doclen} \right] + f_{i, j}},
\end{align}
where $b$ is a constant that takes values between $0.0$ and $1.0$. On the one hand, for $b = 0$ the equation is reduced to the BM$15$ term frequency factor. On the other hand, for $b = 1$ the equation is reduced to the BM$11$ term frequency factor. When using $\mathcal{B}_{i, j}$ BM$25$ is defined as follows:
\begin{align}
  sim_{\text{BM$25$}}(d_j, q) & \sim \sum_{k_i \in q \wedge k_i \in d_j} \mathcal{B}_{i, j} \times \log \left(\frac{N - n_i + 0.5}{n_i + 0.5}\right).
\end{align}
Robertson et. al ~\cite{RobertsonWJHG94} find in their work that $b = 0.75$, and $K_1 = 1$ is the best choice for most cases. In general, they proposed that $b$ should be close to $1$ to apply the document length normalization. 

BM25 transfer the strengths of the vector space model to the probabilistic model. It requires additional tuning, but achieves better performance than the vector space model for general document collections. Therefore, it has become to a baseline to evaluate new ranking methods.

\subsubsection{Divergence from Randomness}
\label{sec:divergence_from_randomness}

The Divergence from Randomness model was introduced by Amati and Rijsbergen ~\cite{AmatiR02}. It is a probabilistic model that exhibits characteristics of a language model as well. In the model, the term weights are computed by evaluating the divergence between the actual term distribution and the term distribution generated by a random process.

The model is based on $2$ assumptions:
\begin{enumerate}
\item First, terms of a document have different importance when describing the content of it. It assumes that less important words are distributed randomly over the document collection $C$. The probability distribution of a term $k_i$ over the document collection $C$ is given by $P(k_i|C)$. Furthermore, the amount of information of these terms over the whole document collection is defined as $- \log P(K_i|C)$.
\item Second, a complementary term distribution is received by considering only the subset of documents that contain term $k_i$. This subset is called elite set. The underlying probability distribution is defined as $P(k_i|d_j)$. The probability is high if the term $k_i$ occurs often in document $d_j$. Furthermore, if a term occurs rarely in a document, it is important to describe the content of the document. Therefore, the amount of information of these terms being in the elite set is given by $1 - P(k_i|d_j)$.
\end{enumerate}
By combining these $2$ assumptions the term weight is given by:
\begin{equation}
  \label{dfr_term_weight}
  w_{i, j} = (- \log P(k_i | C)) \times (1 - P(k_i | d_j)),
\end{equation}
where $w_{i, j}$ is the term weight of term $k_i$, which occurs in document $d_j$. Furthermore, the ranking function is defined as:
\begin{equation}
  R(d_j, q) = \sum_{k_i \in q} f_{i, q} \times w_{i, j},
\end{equation}
where $f_{i, q}$ is the term frequency of term $k_i$ in query $q$. To approximate the first term of the term weight the following definition are needed:
\begin{align}
  F_i & = \sum_j f_{i, j} \\
  \label{lambda_i}
  \lambda_i & = p \times F_i,
\end{align}
where $F_i$ is the frequency of a term $k_i$ over all documents in the collection. For \Cref{lambda_i}, $p$ is defined to be $p = 1/N$.

The first part of the term weight (see \Cref{dfr_term_weight}) represents the amount of information of a term over the entire document collection. Amati and Rijsbergen ~\cite{AmatiR02} propose in their work $2$ approximations:
\begin{align}
    - \log P(k_i | C) \approx & f_{i, j} \log \Bigl( \frac{f_{i, j}}{\lambda_i} \Bigr) + \Bigl( \lambda_i + \frac{1}{12 f_{i, j} + 1} - f_{i, j}\Bigr) \log e \\
    & + \frac{1}{2} \log(2 \pi f_{i, j}) \nonumber \\
  - \log P(k_i | C) \approx & - \log \left( \frac{1}{1 + \lambda_i} \right) - f_{i, j} \times \log \left( \frac{\lambda_i}{1 + \lambda_i} \right),
\end{align}
where $f_{i, j}$ is the frequency of term $k_i$ in document $d_j$. For the second approximation $p = 1/(1 + \lambda_i)$.

The second part of the term weight (see \Cref{dfr_term_weight}) represents the amount of information of a term with respect to the elite set, which is calculated by:
\begin{align}
  1 - P(k_i | d_j) & = \frac{1}{f_{i, j} + 1} \\
  1 - P(k_i | d_j) & = \frac{F_i + 1}{n_i \times (f_{i, j} + 1)},
\end{align}
where $n_i$ is the number of documents that contain term $k_i$.

The basic form of the Divergence from Randomness model does not take document length normalization into account. Therefore, there exist 2 extensions for the term frequency $f_{i, j}$. Each of them can be used to add document length normalization:
\begin{align}
  f^{\prime}_{i, j} & = f_{i, j} \times \frac{avg\_doclen}{len(d_j)} \\
  f^{\prime}_{i, j} & = f_{i, j} \times \log \left(1 + \frac{avg\_doclen}{len(d_j)} \right),
\end{align}
where $avg\_doclen$ is the average document length over the entire collection, and $len(d_j)$ is the number of terms inside document $d_j$.

With the different possibilities to combine the $2$ terms of the term weight, and the document length normalization various ranking formulas that suites different collections can be produced.

\section{Structured Text Retrieval}
\label{sec:structured_text_Retrieval}

Text documents always contain a certain structure. For example, a scientific paper can be split by its sections, subsections, and paragraphs. Another example would be a book that is divided by pages and columns. Information retrieval models that leverages document structure, in addition to the content, are called structured text retrieval models. There are several stages in which the information retrieval systems can take the advantage of this structural information:
\begin{enumerate}
  \item At index stage, where components of a document are detected, and indexed separately. The relationship between the components is preserved.
  \item At retrieval stage, by allowing components to be retrieved in varying granularity.
  \item As result of the presentation stage, where only relevant components are returned to the user and not the entire document.
  \item At querying stage, where a query language is used that includes structural constrains.
\end{enumerate}
Information retrieval models use structural information either in an explicit or implicit way. More frequently, an explicit structure is used where documents are structured in a defined scheme (e.g., XML scheme). The advantage of this structure is that relationships between the components of the document remain. For example, a nested component knows its parent, and the parent knows its children.

For implicit structure, it is not possible to distinguish between content and structural information. Documents are represented as a sequence of text tokens and mark-up tokens. At query time the sequence of tokens is searched for opening and closing mark-up tokens. Afterwards the structural component is generated with the content between these mark-up tokens.
It exists only at query time and is discarded by the system afterwards.

In an information retrieval system it is possible to use multiple layers. For example, the first layer represents the logical structure of a scientific paper. Therefore, it contains the information about chapters, sections, subsections, and paragraphs. The second layer represents the layout structure. Hence, it contains information about columns and pages. Multiple layers are usually used for explicit structure, as maintaining them for implicit structure is too complex.

Inside one layer, content of a document is always assignable to one component (e.g., text in a subsection). If this is the case, components do not overlap. Across different layers it is possible that content is assigned to multiple components. Therefore, the components overlap. Alink et al. ~\cite{AlinkBBV06} tackle in their work the possibility to mix layers in one query. Therefore, they defined navigation steps to allow changing between layers. 

One of the first models that use structured text retrieval was proposed by Burkowski ~\cite{BURKOWSKI1992333, Burkowski92}. It is based on non-overlapping lists. Therefore, a list for each type of document component is generated. For example, a first list contains all sections, a second list all subsections, and a third list all subsubsections. An inverted index is generated to search for a term. The inverted index stores a mapping from index terms to occurrences. Queries that contain terms and component types are used to search for content within the lists.

Another model was introduced by Baeza-Yates and Navarro ~\cite{GB95, GB97} that is based on proximal nodes. The model uses hierarchical index structures to store structural information. A more targeted search is possible as relationships between components are stored.

\subsection{Ranking Strategies in XML Retrieval}
\label{sec:ranking_strategies_in_xml_retrieval}

When XML was introduced in $1998$ it became to the standard in structured text retrieval. Nowadays XML Retrieval is almost a synonym for Structured Text Retrieval. 

In the XML Retrieval documents are represented in a XML structure. Ranking Algorithms are used (cf. \Cref{sec:unstructured_text_Retrieval}) to calculate ranks for document components. To that end, documents are represented as set of terms, and components are subsets of these terms. In addition, calculations require a ranking strategy, as ranking of structured text differs from the ranking of unstructured text. For example, a single component does not have to contain all query terms, as they can occur in different parts of the document.

The first ranking strategy is known as contextualization. For this strategy, the rank of the component is combined with the rank of its root element, which is the document. Arvola et al. ~\cite{ArvolaJK05} suggest in their work to use the average of these $2$ ranks. In addition, Mass and Mandelbrod ~\cite{MassM04} introduced in their work a scaling factor for each rank.

The second ranking strategy is named propagation. It requires that only leaf elements are indexed. The rank of the document is calculated from bottom-up. First, the rank of the leaf elements is calculated. Afterwards, the rank of the parent is calculated based on its children ranks. This is continued until the root element is reached. The most common propagation mechanisms are based on weighted sums. Geva ~\cite{GS2005} introduced in his work a weighing based on the number of children:
\begin{align}
  \mathit{score}(e, q) = D(m) \times \sum_{e_c} \mathit{score}(e_c, q),
\end{align}
where $e$ is the component, $e_c$ are the child components of $e$, and $m$ is the number of retrieved child components of $e$. Retrieved indicates that $\mathit{score}(e_c, q) > 0$ holds. Furthermore, $D(m) = 0.49$ if $e$ has just one retrieved child ($m = 1$), otherwise $D(m) = 0.99$. Propagation mechanisms are more complex to implement than contextualization mechanisms, but provide good retrieval performance.

The third ranking strategy is called aggregation and is proposed by Chiaramella et al. ~\cite{Yv96}. In their work they describe that the representation of a XML element can be interpreted as an aggregation of its own and its children content representations. Linear interpolation can be used as aggregation strategy. To calculate the probability for leaf elements the following definitions are needed:
\begin{align}
  \label{prob_query_lan_model}
  P(q, M_e) & = \prod_{k_i \in q} P(k_i|M_e, \lambda) \\
  P(k_i|M_e, \lambda) & = \lambda P(k_i|e) + (1 - \lambda) P(k_i|C),
\end{align}
where $P(k_i|e)$ is the probability of query term $k_i$ in component $e$, and yields the element models term frequency. $P(k_i|C)$ is the probability of query term $k_i$ in the collection $C$, and represents the element models inverse document frequency (see \Cref{sec:the_vector_space_model} for different approaches to compute term frequency, and inverse document frequency). The constant $\lambda$ is used as smoothing parameter. \Cref{prob_query_lan_model} represents the probability of a query $q$ being generated by a components language model $M_e$. Afterwards, the aggregation strategy based on linear interpolation is defined as:
\begin{align}
  P(k_i|M_e) = \sum_{e_j} w_j P(k_i|M_{e_{j}}),
\end{align}
where $\sum_{e_j} w_j = 1$. The rank of the leaf elements is generated by \Cref{prob_query_lan_model}. For example, a section $s_1$ contains $3$ paragraphs $p_1$, $p_2$, and $p_3$. Furthermore, the probabilities when searching for "\textit{apple}" are defined as $P("apple"|M_{p_1}) = 0.26$, $P("apple"|M_{p_2}) = 0.49$, and $P("apple"|M_{p_3}) = 0.15$. When "\textit{apple}" is passed as query into the aggregation function then $P("apple"|M_{s_1}) = 1/3 \times (0.26 + 0.49 + 0.15) = 0.3$. Where $w_j = 1/3$ for all components as they contribute equally.

The fourth strategy is named merging. It requires a selective index strategy as described by Mass and Mandelbrod ~\cite{MassM04}, where a separate index is created for each component type (similar to the model based on non-overlapping lists). Afterwards, the components are ranked for each type. As a result, ranked lists are created for each type. To merge the list normalization is required, as components of different types have different lengths (e.g., the length of a section, and the length of a paragraph). Hence, the normalization of components is defined as follows:
\begin{align}
  \text{max}\left( \frac{\mathit{score}(e, q)}{\mathit{score}(q, q)}, 1 \right),
\end{align}
where $\mathit{score}(q, q)$ is taken for normalization. There, any element that is identical to the query obtains the full score of $1$. Afterwards, the ranked lists of components can be merged according their normalized score.

Manning et al. ~\cite{manning2008} propose in their work a method to include zone scores. Their approach assigns a weight between $0.0$ and $1.0$ to a document $d$ with respect to a query $q$. This is done by linear combinations of zone scores, where each zone of the document contributes to a boolean value:
\begin{align}
  \sum_{i = 1}^{l}g_i s_i,
\end{align}
where $l$ is the number of zones, and $g_i$ are the zone scores. Each zone score is between $0.0$ and $1.0$, and $\sum_{i = 1}^l = 1$. The parameter $s_i$ is the boolean score that represents a match between the query and the $i$th zone. The weighted zone scoring is also known as Ranked Boolean Retrieval. 

There exist many more strategies to rank structured text. Some of them require special indexing strategies. The best ranking strategy always strongly depends on the used document collection.

%\section{Document Preprocessing}
%\label{sec:document_preprocessing}

%~\cite{ModernInvormationRetrieval1999} page 223
%Describe Text Similarities and common techniques. ~\cite{ModernInvormationRetrieval1999} page 222

%\subsection{Stop Words}
%\label{subsec:stop_words}

%What are stopwords, and describe common techniques. In ~\cite{Vijayarani2015} and references

%\subsection{Stemming}
%\label{subsec:stemming}

%Describe stemming and common stemming techniques. In ~\cite{Vijayarani2015} and references

%\section{Text Similarities}
%\label{sec:text_similarities}

%Describe Text Similarities and common techniques. ~\cite{ModernInvormationRetrieval1999} page 222

\section{Evaluation of Ranking Algorithms}
\label{sec:evaluation_of_ranking_algorithms}

The effectivity of an information retrieval system (IRS) is lean on the underlying ranking function. Therefore, it is important to measure the performance of these ranking functions to make them comparable.

\myfig{precision_recall}
      {width=0.50\textwidth}
      {\textbf{Document collection split according their relevance.} During the evaluation of an ranking algorithm the document collection is split into $4$ subsets. First, the \textit{true positive} documents are retrieved as relevant for the user, and are relevant. Second, the \textit{false positive} documents are retrieved as relevant, but are not relevant. Third, the \textit{true negatives} documents were correctly received as not relevant. Forth, the \textit{false negatives} documents were wrongly received as not relevant. The \textit{true positive} set, and \textit{true negative} set should be as large as possible as it is directly related to the effectivity of the ranking algorithm.}
      {Document collection split according their relevance.}
      {fig:precision_recall}

Evaluation of a IRS is based on the set of relevant documents provided by the system. To evaluate a generated set without any ranking (unordered) $2$ information retrieval basic measures known as \textit{Precision} and \textit{Recall} are used. \textit{Precision} is a measurement of retrieved documents (see \Cref{fig:precision_recall} for sets in the document collection split according their relevance) that are relevant for the user:
\begin{align}
  \label{precision}
  \text{Precision} = & \frac{\text{\# relevant items retrieved}}{\text{\# retrieved items}} \nonumber \\
    = & P(\text{relevant} | \text{retrieved}) \nonumber \\
    = & \frac{\text{true positives}}{\text{true positives} + \text{false positives}}.
\end{align}
\textit{Recall} represents the fraction of relevant documents that are received:
\begin{align}
  \text{Recall} = & \frac{\text{\# relevant items retrieved}}{\text{\# relevant items}} \nonumber \\
    = & P(\text{retrieved} | \text{relevant}) \nonumber \\
    = & \frac{\text{true positives}}{\text{true positives} + \text{false negatives}}.
\end{align}
There is a trade-off between the two measures. Therefore, when having a \textit{Recall} of $1$ it is possible to have a low \textit{Precision}. This happens as \textit{Recall} always increasing until all relevant documents are retrieved, but new received documents can be \textit{false positives}. Hence, the \textit{Precision} decreases, and \textit{Recall} stays the same.

There exist many different metrics to evaluate generated sets with ranking (ordered). Mean Average Precision is one of the most commonly used evaluation techniques. It is defined as the average of all precision values after a new relevant document is observed:
\begin{align}
  \label{map_of_a_single_query}
  \mathit{MAP}_i = \frac{1}{|R_i|}\sum_{k = 1}^{|R_i|} P(R_i[k]),
\end{align}
where $R_i$ is the set of relevant documents with respect to query $q_i$. $R_i[k]$ represents the reference of the $k$th document in $R_i$, and $P(R_i[k])$ is the \textit{Precision} of the document (see \Cref{precision}). If $R_i[k]$ belongs to a \textit{false positive} document $P(R_i[k]) = 0$ (for an example see \Cref{fig:map}). Furthermore, the Mean Average Precision over a set of queries is defined as:
\begin{align}
  \mathit{MAP} = \frac{1}{N_q}\sum_{i = 1}^{|N_q|} \mathit{MAP}_i,
\end{align}
where $N_q$ is the total number of queries. Mean Average Precision is widely used as it is simple, easy to implement, versatile, and stable.

\myfig{map}
      {width=1.00\textwidth}
      {\textbf{Example for Mean Average Precision of a single query.} The Precision values are calculated according the retrieved set of relevant documents. Documents with a green hook are \textit{true positives} (TP), and documents with a red cross are \textit{false positives} (FP). The precision values down the FP are ignored, as they are not counted. In the example $R_1 =$ \{TP, FP, TP, TP, FP, TP\}, and therefore $|R_1| = 6$. Inserting these values into the Mean Average Precision formula of a single query (see \Cref{map_of_a_single_query}) results with $\mathit{MAP}_1 = 1/6 \times (1 + (2/3) + (3/4) + (4/6)) = 0,513\overline{8}$.}
      {Example for Mean Average Precision of a single query}
      {fig:map}

\section{IMRaD Structure}
\label{sec:imrad_structure}

\begin{enumerate}
  \item general about IMRaD in scientific writing: ~\cite{robert1989}
  \item chapter distribution analysis: ~\cite{bertin2013}
  \item other interesting articles: ~\cite{Sollaci-The-2004}
\end{enumerate}

section in progress... %todo
