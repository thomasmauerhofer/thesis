\chapter{Related Work}
\label{cha:related_work}

\section{Information Retrieval Models}
\label{sec:information_retrieval_models}

Creating an information retrieval system is a complex process that has to be planned accordingly. To reach this goal models are used as a base, where the whole system is sketched. The model generation consists of two tasks. First, design a framework which represents the documents and the user queries. Second, create a ranking function, which generates a numeric rank for each document based on a query. Afterwards these ranks are used by the system to sort the documents.

One of the most common retrieval approaches is retrieval based on index terms. In this context an index term is a keyword, which appears in the document collection of the framework. This approach can be implemented efficiently as query words can be used as index terms with limited transformations. For example a user is interested in cooking, and searches for "Austrian dishes". The query words "Austrian", and "dishes" can directly used to search through the document collection since they do not need any transformation.

In general, information retrieval models consists of four parts. Ribeiro-Neto and Baeza-Yates ~\cite{ModernInvormationRetrieval1999} define them as a quadruple $[\textbf{D}, \textbf{Q}, \mathcal{F}, \mathcal{R}(q_i, d_j)]$, where:

\begin{enumerate}
  \item \textbf{D} is a set composed of logical views of documents in a collection.
  \item \textbf{Q} is a set composed of logical views of the user information needs. Such representations are called queries.
  \item $\mathcal{F}$ is a framework for modeling document representations, queries, and their relationships.
  \item $\mathcal{R}(q_i, d_j)$ is a raking function that associates a real number with a query representation $q_i \in \textbf{Q}$ and a document representation $d_j \in \textbf{D}$. The ranking function generates an order over all documents \textbf{D} with respect to a query $q_i$.
\end{enumerate}

Hence, the model is used to define the framework $\mathcal{F}$ and the ranking function $\mathcal{R}(q_i, d_j)$. For example, for textual documents the document representation is a set of all terms within the document. To keep the collection smaller without losing any information stop words should be removed in a preprocessing step. The set of index terms within a document collection is called vocabulary. According to our document representation the query representation is a set of all terms within the query. There can also be an additional preprocessing step in the query creation. An example for such an preprocessing step would be synonyms which are added to the query set.

After the design of the framework, a ranking function is created. It should be constructed in a way that it fits to the requirements of the user. This means for a given query, the ranking function determines a numeric rank to each document in the collection, which represents the relevance for the user. For example, the ranking function counts how many query terms appear in the term set of a document.

Another example is to use term frequency as ranking function. Term frequency itself denotes how often a term occurs in a document. To be able to use it, the document representation is adapted from a set with all terms to a bag of words. In a bag of words each term is represented as a pair of term and term frequency. The ranking function sums the frequencies over all query terms. To be able to compare the documents using the term frequency, the ranks are normalized.

Information retrieval is used in several fields where the underlying models have to fulfill different requirements. Therefore, they are separated into text-based models, link-based models, and multimedia objects-based models. Furthermore, text-based models can be categorized in unstructured, and semi-structured text models. Unstructured text models are used for text documents where the content is represented as sequence of words. Semi-structured text models contain structure such as title, sections, paragraphs, in addition to unstructured text.

The web is rapidly growing, and as a consequence has a huge number of web pages (i.e. documents). Therefore, additional information has to be leveraged as well. This means that the content of documents, and furthermore the links between those documents are take into account. Models which use those additional link information are called Link-based models where PageRank ~\cite{brin1998anatomy} and Hyperlink-Induced Topic Search ~\cite{kleinberg1999authoritative} are important parts of the models.

Information retrieval for multimedia objects differs according their underlying data from the first $2$ types. For example, when thinking on a image it can be seen as a matrix of color values. Detecting similarities between images requires the calculation of more complex features, such as shapes. The representation of the query has to be adapted as well. The user can use words, or use images to define a query. One of the simplest forms of multimedia-based retrieval is image retrieval. Audio and video retrieval are more complicated since there is also a time value which have to be taken into account.

\section{Unstructured Text Retrieval}
\label{sec:unstructured_text_Retrieval}

In unstructured text retrieval, documents can be seen as sequence of words. The $3$ classical models are boolean-, vector-, and probabilistic model. First, in the boolean model, documents and queries are represented as sets. Terms are stitched together with boolean operators to formulize user queries. Second, in the vector model, documents and queries are represented as a vector in a t-dimensional space. The size of t is defined by the number of words in the vocabulary of the collection. Third, in the probabilistic model, documents and queries are represented based on probability theory. Specifically by estimating the probability of a term appearing in a relevant document. Gudivada et al. ~\cite{gudivada1997} advice in their work to denote boolean models as set theoretic, vector models as algebraic, and probabilistic models as probabilistic.

\subsection{The Boolean Model}
\label{sec:the_boolean_model}

The boolean model is a well-known information retrieval model in the area of unstructured text retrieval. It was proposed as a paradigm for accessing large-scale systems since the $1950$s ~\cite{Melucci2009}. The model uses boolean operators and set theory to find relevant documents.

\myfig{boolean_model}
      {width=0.65\textwidth}
      {\textbf{Example query in the boolean model with $3$ terms.} For the boolean model documents in the collection are represented as sets of terms. In this example the vocabulary of the document collection is given by V = \{$t_1$, $t_2$, $t_3$\}. Furthermore, documents can be separated according to the terms they are containing. Given a query $q=t_1 \wedge (t_2 \vee \neg t_3)$ all documents which satisfy this query are marked with an green hook. This means that they are relevant for the user. All other documents which does not satisfy the query are marked with a red cross.}
      {Example query in the boolean model with $3$ terms.}
      {fig:boolean_model}

The classic boolean model can only decide if a document is relevant for the user, or not. It does not provide a rank, which is used to sort the documents. Salton et al. introduce in their work ~\cite{Salton-Extended-1983} an extension where documents are sorted according their relevance.

Index terms are combined with the $3$ boolean operators NOT($\neg$), AND($\wedge$), OR($\vee$) to formulize user queries. The disjunctive normal form of the query shows which areas of the sets are relevant. For example, for query $q=t_1 \wedge (t_2 \vee \neg t_3)$, and vocabulary V = \{$t_1$, $t_2$, $t_3$\}, $q_{DNF}$ is:
\begin{equation}
q_{DNF} = (t_1 \wedge t_2 \wedge t_3) \vee (t_1 \wedge t_2 \wedge \neg t_3) \vee (t_1 \wedge \neg t_2 \wedge \neg t_3)
\end{equation}
This representation of the query highlight that $3$ areas are relevant for the user. First, all $3$ query terms occur. Second, the first and the second term occur, but not the third. Finally, the first term occurs, but not the second and the third. \Cref{fig:boolean_model} displays the example query represented in a Venn diagram, where the $3$ areas can be seen in a graphical representation.

The boolean model works also if not all terms of the vocabulary are part of the user query. Considering a vocabulary V = \{$t_1$, $t_2$, $t_3$, $t_4$\}, and the previous example query, the disjunctive normal form is:
\begin{equation}
  \begin{aligned}
    q_{DNF} = &(t_1 \wedge t_2 \wedge t_3 \wedge \neg t_4) \vee (t_1 \wedge t_2 \wedge t_3 \wedge t_4) \vee \\
              &(t_1 \wedge t_2 \wedge \neg t_3 \wedge \neg t_4) \vee (t_1 \wedge t_2 \wedge \neg t_3 \wedge t_4) \vee \\
              &(t_1 \wedge \neg t_2 \wedge \neg t_3 \wedge \neg t_4) \vee (t_1 \wedge \neg t_2 \wedge \neg t_3 \wedge t_4)
  \end{aligned}
\end{equation}
The last term (i.e., $t_4$), which is not part of the query, is also considered in the disjunctive normal form. It is added once as present, and once as absent to the other $3$ terms.

The main advantages of the boolean model are the clean formalism, and its simplicity ~\cite{ModernInvormationRetrieval1999}. These advantages comes with the usage of binary operators, and binary index term weighing. One of the main disadvantages is the exact matching of documents. This means that a document can only be relevant, or not relevant to the user, without any ranking. As a result, users receive too few or too many documents.

\subsection{The Vector Space Model}
\label{sec:the_vector_space_model}

The vector space model was introduced by Salton et al.~\cite{salton75vsm}. In the model documents and queries are represented as vectors in an t-dimensional space. The size of t is defined by the number of words in the vocabulary of the collection.

The vector space model is more sophisticated than the boolean model, since it contains partial matching. Partial matching means, that a degree of similarity between user queries and the documents in the system are calculated. To accomplish this, non-binary weights are used in combination with index terms.

The idea of non-binary weights is based on the assumption that some index terms are more important than others to describe the content of a document. The calculation of such term weights is a challenging task, since they have to reflect the subjective expectation of a user. As a result terms that appear in a few documents have a higher weight than terms that occur in many documents. For a example a collection consists of $3$ documents, given by $D_1 =$ \{"cooking", "appetizer"\}, $D_2 =$ \{"cooking", "main", "dish"\}, $D_3 =$ \{"cooking", "dessert"\}. A user is interested to create a dessert. Therefore he searches for "cooking dessert". The first query word "cooking" is part of each document. This means it is not very useful to define the users requirements. The second query word "dessert" is only part of one document. %mehr aussagekraft, und dadurch höher gewichtet...



%term weighing

Ribeiro-Neto and Baeza-Yates ~\cite{ModernInvormationRetrieval1999} define the document representation $d_j$, and the query representation $q$ as:
\begin{align}
  \vec{d_j} & = (w_{1, j}, w_{2, j}, \dots, w_{t, j}) \\
  \vec{q} & = (w_{1, q}, w_{2, q}, \dots, w_{t, q})
\end{align}
aaaa bbbb ccccc
% Formeln genau beschreiben. Beispiel mit 3 Wörtern.. one basic example to understand with tf .. überschwingen auf tf-idf....
% Sortierung der documente...

\subsubsection{Term Frequency-Inverse Document Frequency Model}
\label{sec:tfidf}

% also Tf-idf Model in the same paper defined ~\cite{salton75vsm}
% concept of inverse document frequency was introduced by jones72astatistical

Describe basics like term frequency, document frequency...

\begin{equation}
  \begin{split}
    \text{idf}_t & = log \frac{N}{df_t} \\
    \text{tf-idf}_{t, d} & = \text{tf}_{t, d} \cdot \text{idf}_t \\
    \text{Score}(q, d) & = \sum_{t \in q}\text{tf-idf}_{t, d}
  \end{split}
\end{equation}

\subsection{The Probabilistic Model}
\label{sec:the_probabilistic_model}

General about probabilistic model.

\subsubsection{Okapi BM25}
\label{sec:okapi_bm25}
as described in ~\cite{manning2008} page 214 and in ~\cite{ModernInvormationRetrieval1999} page 105
\begin{equation}
  \begin{split}
    \text{idf}_t & = log \frac{N - \text{df}_t + 0.5}{\text{df}_t + 0.5} \\
    \text{bm25}_{t, d} & = \text{idf}_t \cdot \frac{\text{tf}_{t, d} \cdot (k_1 + 1)}{\text{tf}_{t, d} + k_1 \cdot \bigl(1 - b \cdot \frac{|G|}{\text{avgdl}}\bigr)}  \\
    \text{Score}(q, d) & = \sum_{t \in q}\text{bm25}_{t, d}
  \end{split}
\end{equation}


\subsubsection{Divergence from Randomness}
\label{sec:divergence_from_randomness}

as described in ~\cite{ModernInvormationRetrieval1999} page 113
\begin{equation}
  w_{i, j} = (- \log P(k_i | C)) \cdot (1 - P(k_i | d_j))
\end{equation}
\begin{equation}
  \text{Score}(d_j, q) = \sum_{k_i \in q} f_{i, q} \cdot w_{i, j}
\end{equation}
\begin{equation}
  F_i = \sum_j f_{i, j}
\end{equation}
\begin{equation}
  P(k_i | C) = \binom{F_i}{f_{i, j}}p^{f_{i, j}} \cdot (1 - p)^{F_i - f_{i, j}}
\end{equation}
\begin{equation}
  \lambda_i = p \cdot F_i
\end{equation}
\begin{equation}
  P(k_i | C) = \frac{e^{-\lambda_i}\lambda_i^{f_{i, j}}}{f_{i, j}!}
\end{equation}
\begin{equation}
  \begin{split}
    - \log P(k_i | C) & = -\log\Biggl(\frac{e^{-\lambda_i}\lambda_i^{f_{i, j}}}{f_{i, j}!}\Biggr) \\
    & \approx -f_{i,j} \log \lambda_i + \lambda_i \log e + log(f_{i,j}!) \\
    & \approx f_{i, j} \log \Bigl( \frac{f_{i, j}}{\lambda_i} \Bigr) + \Bigl( \lambda_i + \frac{1}{12 f_{i, j} + 1} - f_{i, j}\Bigr) \log e + \frac{1}{2} \log(2 \pi f_{i, j})
  \end{split}
\end{equation}
\begin{equation}
  1 - P(k_i | d_j) = \frac{1}{f_{i, j} + 1}
\end{equation}
\begin{equation}
  f^{\prime}_{i, j} = f_{i, j} \cdot \frac{avgdl}{len(d_j)}
\end{equation}

\section{Structured Text Retrieval}
\label{sec:structured_text_Retrieval}

Write about structuring, early text retrieval, xml retrieval

\subsubsection{Ranked Boolean Retrieval}
\label{sec:ranked_boolean_retrieval}

Described in ~\cite{manning2008} page 103

\begin{equation}
  \sum_{i = 1}^{l}g_i s_i
\end{equation}

\section{Document Preprocessing}
\label{sec:document_preprocessing}

~\cite{ModernInvormationRetrieval1999} page 223

\subsection{Stop Words}
\label{subsec:stop_words}

What are stopwords, and describe common techniques. In ~\cite{Vijayarani2015} and references

\subsection{Stemming}
\label{subsec:stemming}

Describe stemming and common stemming techniques. In ~\cite{Vijayarani2015} and references

\section{Text Similarities}
\label{sec:text_similarities}

Describe Text Similarities and common techniques. ~\cite{ModernInvormationRetrieval1999} page 222

\section{IMRaD Structure}
\label{sec:imrad_structure}

general about IMRaD in scientific writing: ~\cite{robert1989}, chapter distribution analysis: ~\cite{bertin2013} important: ~\cite{Sollaci-The-2004}

\section{Evaluation of Ranking Algorithms}
\label{sec:evaluation_of_ranking_algorithms}

as described in ~\cite{manning2008} page 147 and in ~\cite{ModernInvormationRetrieval1999} page 140

\myfig{precision_recall}
      {width=0.50\textwidth}
      {Precision and Recall}
      {Precision and Recall}
      {fig:precision_recall}

\begin{equation}
  P = \frac{\text{\# relevant items retrieved}}{\text{\# retrieved items}} = \frac{TP}{TP + FP} = P(\text{relevant} | \text{retrieved})
\end{equation}

\myfig{map}
      {width=1.00\textwidth}
      {Example for the precision of a search result}
      {Example for the precision of a search result}
      {fig:map}

\begin{equation}
  \text{MAP}(Q) = \frac{1}{|Q|}\sum_{j = 1}^{|Q|} \frac{1}{m_j}\sum_{k = 1}^{m_j}\text{Precision}(R_{jk})
\end{equation}
