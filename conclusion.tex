\chapter{Conclusion}
\label{cha:conclusion}

%In the final chapter we 

\section{Summary}
\label{sec:summary}

We started our work with an motivation section, where we discuss the fast growth of the internet and the resulting importance of search engines. Search engines help to reduce the time required to find a piece of information, and minimize the number of information sources that need to be searched. We focus on scientific literature search where search engines help to find scientific articles.

An advantage of scientific articles is that they share a common structure to increase the readability. This structure is known is IMRaD (Introduction, Method, Results and Discussion). In our work we tackle the problem whether it is possible to improve the search result quality when searching for scientific works by leveraging IMRaD structure information. Specifically,
\begin{enumerate}[label=(\alph*)]
  \item Does the search result improve for explicit search using queries?
  \item Does the search result improve for implicit search using complete scientific papers?
  \item Does the search result improve if only a single chapter of the scientific paper is used for searching?
\end{enumerate}
In the related work section we describe the definition of an information retrieval model. Afterwards, we discuss the $3$ classical models for unstructured text retrieval. First, the boolean model where documents and queries are represented as sets. Terms are combined with boolean operators to formulate queries. Second, the vector model where documents and queries are represented as a vector in a t-dimensional space. Third, the probabilistic model where documents and queries are represented based on probability theory. Specifically, by estimating the probability of a term appearing in a relevant document.

Additionally, we describe extensions of the vector-, and the probabilistic model. First, the TF-IDF model which is based on the vector space model, and is one of the most popular weighting schemes in information retrieval. Second, the BM$25$ model which is based on the probabilistic model. It is the result of several experiments by Robertson et. al ~\cite{RobertsonWHGL92, RobertsonWJHG93, RobertsonWJHG94}. Third, the Divergence from Randomness model was introduced by Amati and Rijsbergen ~\cite{AmatiR02} and is a probabilistic model that exhibits characteristics of a language model as well.

Next, we discuss techniques of structured text retrieval. We focus specifically on $5$ ranking strategies known as contextualization, propagation, aggregation, merging, and zone scores. The model based on zone scores is proposed by Manning et al. ~\cite{manning2008}, and is also known as Ranked Boolean Retrieval.

In the last part of the related work we focus on the IMRaD structure in scientific articles. Sollaci and Pereira ~\cite{Sollaci-The-2004} describe in their work that the IMRaD structure began to be adopted in the 1940s, and became the standard format for scientific articles in the 1970s. Furthermore, we discuss IMRaD structure distributions as proposed by Bertin et al. ~\cite{bertin2013}, and how IMRaD structure can be leveraged in information retrieval systems.

In the methods section we started with the description of our dataset. Our dataset is composed of 821 scientific articles. We added additional information such as IMRaD mappings, and links between the articles based on citations. Furthermore, we defined our database schema with respect to the article structure and analyze the citation network.

Afterwards, we describe our introduced system and the underlying model. We design our system to compare various common ranking algorithms. Hence, the ranking algorithms require to be easily interchangeable. In addition, our model is designed to work with unstructured as well as structured data. This is reflected by the query language.

In the results and discussion section we describe a measure for the performance of ranking algorithms, which is required to compare our proposed ranking algorithms.

Finally, we list our study results, and interpret them. We divide our interpretation into $3$ parts, which is based on our research questions:
\begin{enumerate}[label=(\alph*)]
  \item First, we compare unstructured text retrieval with structured text retrieval where we simulate explicitly formulated queries with the usage of a generated test set. We find that it is not necessary to have the overhead of IMRaD chapter features when only a few keywords are used to search for scientific articles. This happens as the keywords define the content that should occur anywhere in the articles. Additional constraints that restrict where terms can appear are rather obstructive, as they tend to prevent the retrieval of relevant articles.
  \item Second, we discuss implicit search were we use entire scientific articles to search for other articles. When articles are used to search they can be seen as large and precise queries. IMRaD chapter features are leveraged to define these queries. We find that these are helpful as they introduce constraints that describe the expected content of articles.
  \item Third, we focus on structured text retrieval and the influence of single chapters on the search result. Searching with all IMRaD chapter of a document performs marginally better than searching with single IMRaD chapters. We obtain the best accuracy when we use the Background section to search in Introduction sections. One interesting point we found out is that queries for single chapters are one fifth of queries of implicit search, but have almost the same performance.
\end{enumerate}

\section{Overarching results}
\label{sec:overarching_results}

In this chapter we briefly summarize the results we obtained in \Cref{cha:results_discussion}. Afterward, we discuss these results against each other to present them in a larger context.

For our first experiments, we compare unstructured text retrieval with structured text retrieval where we simulate explicitly formulated queries with the usage of a generated test set. The accuracies of the $3$ best performing algorithms are between $0.1921$ and $0.2199$ for unstructured text retrieval, and between $0.1015$ and $0.1642$ for IMRaD structured text retrieval. We only take the best $3$ results as others have less significance for the best performing results. Based on the results we find that unstructured text retrieval outperforms IMRaD structured text retrieval. This happens as terms in a short query define content that should occur anywhere in the articles. Additional constraints that restrict where terms can appear are rather obstructive as they tend to prevent the retrieval of relevant articles.

For our second experiments, we compare unstructured text retrieval with structured text retrieval according entire documents. The accuracies of the $3$ best performing algorithms are between $0.0554$ and $0.1186$ for unstructured text retrieval, and between $0.0882$ and $0.1613$ for IMRaD structured text retrieval. Based on the results we find that IMRaD structured text retrieval outperforms unstructured text retrieval. This happens as documents can be seen as large and precise queries when searching for other documents. IMRaD chapter features are leveraged to define these queries. These are helpful as they introduce constraints that describe the expected content of articles.

The comparison of the accuracies in the $2$ experiments may lead to the assumption that searches with queries result with more relevant papers than searches with documents. This conclusion is misleading as the $2$ experiments cover different requirements of a user. On one hand, when a short query is used to search for other articles a user expects a lot of documents due to unknown expectations. On the other hand, when a user searches with the usage of a document then other documents that are related to it are expected.

The accuracies of \textit{Term Frequency}, \textit{TF-IDF}, and \textit{BM$25$} are similarly high for the first and the second experiment when using IMRaD structured text retrieval. Only \textit{Ranked Boolean Retrieval} lost a lot of its accuracy. We assume that this is due to the fact that the \textit{Ranked Boolean Retrieval} already provides a structure for the ranking, and the $2$ structures influence each other negatively. All $4$ algorithms show a similar decrease in accuracy for unstructured text retrieval. \textit{Divergence from Randomness} always performs bad, and therefore the results are hard to interpret.

For our third experiments we focus on structured text retrieval and the influence of single chapters on the search result. We used \textit{TF-IDF} and \textit{BM$25$} as ranking algorithms for the experiment. The highest accuracy was obtained when the Background section is used to search in Introduction sections, where \textit{TF-IDF} obtains an accuracy of $0.1454$ and \textit{BM$25$} obtains an accuracy of $0.0909$. 

In comparison with our first experiment the accuracies are lower, but also they reflect different requirements. Furthermore, in comparison to our second experiment the performance is $1.59\%$ worse. Therefore, searching with all IMRaD chapter of a document performs marginally better than searching with single IMRaD chapters.

During the development of our system, we focus on $2$ applications for information retrieval system in the field of scientific literature research. We find that the usage of IMRaD structure features depends on the use-case. The first application is based on breadth-first search and covers the initial search process. During this initial search a user wants to gets a first overview of a topic, and obtain many articles based on a set of given keywords. Therefore, the system has to provide these articles in response to simple structured queries. We discussed for our first experiments that additional constraints that restrict where terms can appear are rather obstructive as they tend to prevent the retrieval of relevant articles. IMRaD structure features are not necessary when a system is used for the initial search.

The second application is based on depth-first search and covers the specific search of literature. More precise queries are necessary when a user is finished with the initial search, and specifically wants to find additional literature based on the articles obtained in the first step. Therefore, the system has to leverage additional functionality to handle more complex queries. In our second experiment we observe that IMRaD structure features are helpful as they introduce constraints that describe the expected content of articles. As a result, IMRaD structure features improve the results for the specific literature search.

\section{Future Work}
\label{sec:future_work}

The implemented information retrieval system, and the generated dataset provide various opportunities for possible extensions and experiments. The following examples should give ideas of what are possible paths for future research.

For IMRaD structured text retrieval we calculate the rank based on the mean over all IMRaD chapters. As discussed in \Cref{sec:ranking_strategies_in_xml_retrieval} there exist other ranking strategy as well. They can be added as extensions to our system. Afterwards, different combinations of ranking algorithms and ranking strategies can be evaluated. 

Another possible improvement would be a parameter search for BM$25$. Robertson et. al ~\cite{RobertsonWJHG94} propose in their work a general configuration that suites many cases. We find that BM$25$ always performs mediocre with this configuration. With an extensive parameter search the performance may increase, and BM$25$ could catch up with the best performing algorithms.

We use a dataset that consists of $821$ scientific articles. One improvement would be to add additional articles. However, we assume for our query evaluation that citations describe the content of referenced articles. Afterward, queries were auto-generated based on this assumption. These auto-generated queries require to be verified as the assumption may not hold for all of them. All queries where the assumption does not hold needs to be removed from the test set. Another possibility is to create document collections from journals of the Public Library of Science (PLOS). These journals consist of thousands of articles, which increases the expressiveness of experiments.

An additional extension for our system would be to calculate similarities based on article clusters. The idea is that the user provides a set of articles, which are clustered with the articles in the document collection. These clusters are generated based on different properties (e.g., based on the Introduction of the articles). Afterwards a ranked list is generated based on the distances in the cluster.
