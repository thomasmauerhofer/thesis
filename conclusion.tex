\chapter{Conclusion}
\label{cha:conclusion}

%In the final chapter we 

\section{Summary}
\label{sec:summary}

We started our work with an motivation section, where we discuss the fast growth of the internet and the resulting importance of search engines. Search engines help to reduce the time required to find a piece of information, and minimize the number of information sources that need to be searched. We focus on scientific literature search where search engines help to find scientific articles.

An advantage of scientific articles is that they share a common structure to increase the readability. This structure is known is IMRaD (Introduction, Method, Results and Discussion). In our work we tackle the problem whether it is possible to improve the search result quality when searching for scientific works by leveraging IMRaD structure information. Specifically,
\begin{enumerate}[label=(\alph*)]
  \item Does the search result improve for explicit search using queries?
  \item Does the search result improve for implicit search using complete scientific papers?
  \item Does the search result improve if only a single chapter of the scientific paper is used for searching?
\end{enumerate}
In the related work section we describe the definition of an information retrieval model. Afterwards, we discuss the $3$ classical models for unstructured text retrieval. First, the boolean model where documents and queries are represented as sets. Terms are combined with boolean operators to formulate queries. Second, the vector model where documents and queries are represented as a vector in a t-dimensional space. Third, the probabilistic model where documents and queries are represented based on probability theory. Specifically, by estimating the probability of a term appearing in a relevant document.

Additionally, we describe extensions of the vector-, and the probabilistic model. First, the TF-IDF model which is based on the vector space model, and is one of the most popular weighting schemes in information retrieval. Second, the BM$25$ model which is based on the probabilistic model. It is the result of several experiments by Robertson et. al ~\cite{RobertsonWHGL92, RobertsonWJHG93, RobertsonWJHG94}. Third, the Divergence from Randomness model was introduced by Amati and Rijsbergen ~\cite{AmatiR02} and is a probabilistic model that exhibits characteristics of a language model as well.

Next, we discuss techniques of structured text retrieval. We focus specifically on $5$ ranking strategies known as contextualization, propagation, aggregation, merging, and zone scores. The model based on zone scores is proposed by Manning et al. ~\cite{manning2008}, and is also known as Ranked Boolean Retrieval.

In the last part of the related work we focus on the IMRaD structure in scientific articles. Sollaci and Pereira ~\cite{Sollaci-The-2004} describe in their work that the IMRaD structure began to be adopted in the 1940s, and became the standard format for scientific articles in the 1970s. Furthermore, we discuss IMRaD structure distributions as proposed by Bertin et al. ~\cite{bertin2013}, and how IMRaD structure can be leveraged in information retrieval systems.

In the methods section we started with the description of our dataset. Our dataset is composed of 821 scientific articles. We added additional information such as IMRaD mappings, and links between the articles based on citations. Furthermore, we defined our database schema with respect to the article structure and analyze the citation network.

Afterwards, we describe our introduced system and the underlying model. We design our system to compare various common ranking algorithms. Hence, the ranking algorithms require to be easily interchangeable. In addition, our model is designed to work with unstructured as well as structured data. This is reflected by the query language.

In the results and discussion section we describe a measure for the performance of ranking algorithms, which is required to compare our proposed ranking algorithms.

Finally, we list our study results, and interpret them. We divide our interpretation into $3$ parts, which is based on our research questions:
\begin{enumerate}[label=(\alph*)]
  \item First, we compare unstructured text retrieval with structured text retrieval regarding queries. We find that it is not necessary to have the overhead of IMRaD chapter features when only a few keywords are used to search for scientific articles. This happens as the keywords define the content that should occur anywhere in the articles. Additional constraints that restrict where terms can appear are rather obstructive, as they tend to prevent the retrieval of relevant articles.
  \item Second, we discuss implicit search were we use entire scientific articles to search for other articles. When articles are used to search they can be seen as large and precise queries. IMRaD chapter features are leveraged to define these queries. We find that these are helpful as they introduce constraints that describe the expected content of articles.
  \item Third, we focus on structured text retrieval and the influence of single chapters on the search result. Searching with all IMRaD chapter of a document performs marginally better than searching with single IMRaD chapters. We obtain the best accuracy when we use the Background section to search in Introduction sections. One interesting point we found out is that queries for single chapters are one fifth of queries of implicit search, but have almost the same performance.
\end{enumerate}

\section{Overarching results}
\label{sec:overarching_results}

In this chapter we briefly summarize the results we obtained in \Cref{cha:results_discussion}. Afterward, we discuss these results against each other to present them in a larger context.

In our first experiments we compare unstructured text retrieval with structured text retrieval according queries. The results of the $3$ best performing algorithms are between $0.1921\%$ and $0.2199\%$ for unstructured text retrieval, and between $0.1015\%$ and $0.1642\%$ for IMRaD structured text retrieval. We only take the best $3$ results as others have less meaning to the best performing results. Based on the results unstructured text retrieval outperforms IMRaD structured text retrieval. This happens as terms in a short query define content that should occur anywhere in the articles. Additional constraints that restrict where terms can appear are rather obstructive as they tend to prevent the retrieval of relevant articles.

In our second experiments we compare unstructured text retrieval with structured text retrieval according entire documents. The results of the $3$ best performing algorithms are between $0.0554\%$ and $0.1186\%$ for unstructured text retrieval, and between $0.0882\%$ and $0.1613\%$ for IMRaD structured text retrieval. Based on the results IMRaD structured text retrieval outperforms unstructured text retrieval. This happens as documents can be seen as large and precise queries when searching for other documents. IMRaD chapter features are leveraged to define these queries. These are helpful as they introduce constraints that describe the expected content of articles.

The comparison of the accuracies in the $2$ experiments leads to the assumption that searches with queries lead to more relevant papers than searches with documents. This assumption is misleading as the $2$ experiments covers different requirements of an user. On one hand, when a short query is used to search for other articles a user expects a lot of documents, as he does not know exactly what he searches for. On the other hand, when the user searches with the usage of a document he expects other documents that are related to it.

When using IMRaD structured text retrieval the accuracies of \textit{Term Frequency}, \textit{TF-IDF}, and \textit{BM$25$} are similar with query and document. Only \textit{Ranked Boolean Retrieval} lost a lot of his accuracy. We assume that this is happening as the \textit{Ranked Boolean Retrieval} already provides a structure for the ranking, and the $2$ structures hinder themselves. All $4$ algorithms lost a similar amount of accuracy for unstructured text retrieval. \textit{Divergence from Randomness} always performs bad, and therefore the results are hard to interpret.

In our third experiments we focus on structured text retrieval and the influence of single chapters on the search result. We used \textit{TF-IDF} and \textit{BM$25$} as ranking algorithms for the experiment. The highest accuracy was obtained when the Background section is used to search in Introduction sections, where \textit{TF-IDF} obtains an accuracy of $0.1454$ and \textit{BM$25$} obtains an accuracy of $0.0909$. 

In comparison with the first experiment the accuracies are lower, but also they reflect different requirements. Furthermore, in comparison to the second experiment the performance is $1.59\%$ worse. Therefore, searching with all IMRaD chapter of a document performs a bit better than searching with single IMRaD chapters.

\section{Future Work}
\label{sec:future_work}

The implemented information retrieval system, and the generated dataset provide various opportunities for possible extensions and experiments. The following examples should give ideas of what are possible paths for future research.

For IMRaD structured text retrieval we calculate the rank based on the mean over all IMRaD chapters. As discussed in \Cref{sec:ranking_strategies_in_xml_retrieval} there exist other ranking strategy as well. They can be added as extensions to our system. Afterwards, different combinations of ranking algorithms and ranking strategies can be evaluated. 

Another possible improvement would be a parameter search for BM$25$. Robertson et. al ~\cite{RobertsonWJHG94} propose in their work a general configuration that suites many cases. We find that BM$25$ always performs mediocre with this configuration. With an extensive parameter search the performance may increase, and BM$25$ could catch up with the best performing algorithms.

We use a dataset that consists of $821$ scientific articles. One improvement would be to add additional articles. However, we assume for our query evaluation that citations describe the content of referenced articles. Afterward, queries were auto-generated based on this assumption. These auto-generated queries require to be verified as the assumption may not hold for all of them. All queries where the assumption does not hold needs to be removed from the test set. Another possibility is to create document collections from journals of the Public Library of Science (PLOS). These journals consist of thousands of articles, which increases the expressiveness of experiments.

An additional extension for our system would be to calculate similarities based on article clusters. The idea is that the user provides a set of articles, which are clustered with the articles in the document collection. These clusters are generated based on different properties (e.g., based on the Introduction of the articles). Afterwards a ranked list is generated based on the distances in the cluster.
